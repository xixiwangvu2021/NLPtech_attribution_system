{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a95dae0",
   "metadata": {},
   "source": [
    "# Atribution Relations Extractions Model: \n",
    "## Conditional Random Field (CRF) Approach\n",
    "###  CRF classifier trained with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072d4a1",
   "metadata": {},
   "source": [
    "In this notebook, we train and evaluate wto CRF classifiers using 100 dimentional pre-trained GloVe word embeddings as features.\n",
    "\n",
    "The first classifier is trained with only PolNeAR corpus. The second one is trained with both PolNeAr and PARC3 corpus. \n",
    "Note: Nested attributions in PARC3 is not taken into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eaf003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with imports:\n",
    "\n",
    "import sklearn_crfsuite# the model\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from gensim.models import KeyedVectors # to load pre-trained word embeddings\n",
    "import numpy as np # to create 0 vectors for the words which are not in the vocabulary\n",
    "import pandas as pd # to load input&output files for evaluation\n",
    "import csv # to read the data files for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c69c6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-20 18:40:21,069 - glove2word2vec - INFO - running C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py --input glove.6B.100d.txt --output glove.6B.100d.w2vformat.txt\n",
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py:125: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  num_lines, num_dims = glove2word2vec(args.input, args.output)\n",
      "2021-06-20 18:40:21,111 - keyedvectors - INFO - loading projection weights from glove.6B.100d.txt\n",
      "2021-06-20 18:41:00,775 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (400000, 100) matrix of type float32 from glove.6B.100d.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-06-20T18:41:00.775074', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n",
      "2021-06-20 18:41:00,806 - glove2word2vec - INFO - converting 400000 vectors from glove.6B.100d.txt to glove.6B.100d.w2vformat.txt\n",
      "2021-06-20 18:41:01,181 - keyedvectors - INFO - storing 400000x100 projection weights into glove.6B.100d.w2vformat.txt\n",
      "2021-06-20 18:41:31,553 - glove2word2vec - INFO - Converted model with 400000 vectors and 100 dimensions\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word embeddings\n",
    "glove_dimensions = 100\n",
    "!python -m gensim.scripts.glove2word2vec --input  glove.6B.100d.txt --output glove.6B.100d.w2vformat.txt\n",
    "model = KeyedVectors.load_word2vec_format(\"glove.6B.100d.w2vformat.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b897d6",
   "metadata": {},
   "source": [
    "### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba623dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sents_from_conll(inputfile):\n",
    "    '''Read the data from tsv file, return sentences as tokens with corresponding labels.'''\n",
    "    \n",
    "    rows = csv.reader(open(inputfile, encoding=\"utf-8\"), delimiter='\\t')\n",
    "    sents = []\n",
    "    current_sent = []\n",
    "    for row in rows:\n",
    "        current_sent.append(tuple(row))  \n",
    "        #After each sentence there is a special token: Sent_end. Its label is O. It was added in the preprocessing step.\n",
    "        if row[0] == \"Sent_end\":\n",
    "            sents.append(current_sent)\n",
    "            current_sent = []\n",
    "    return sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a15a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = extract_sents_from_conll(\"Toy_data_train.tsv\")\n",
    "\n",
    "#print(sents[0])\n",
    "#print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5c42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2tokens(sent):\n",
    "    '''Take the sentence as token-label pair, return only tokens'''\n",
    "\n",
    "    return [token for token, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24147d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test =  sent2tokens(sents[0])\n",
    "\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6e9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):    \n",
    "    '''Take the sentence as token-label pair, return only labels'''\n",
    "\n",
    "    return [label for token, label  in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2fac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2 = sent2labels(sents[0])\n",
    "\n",
    "#print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e17e1",
   "metadata": {},
   "source": [
    "It is time to extract the features: \n",
    "\n",
    "IMPORTANT: Crfsuite does not support array features, like word embeddings. Instead, we pass every vector component as a feature.\n",
    "\n",
    "https://stackoverflow.com/questions/58736548/how-to-use-word-embedding-as-features-for-crf-sklearn-crfsuite-model-training\n",
    "https://github.com/scrapinghub/python-crfsuite/issues/39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36555f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding function \n",
    "def get_features(token):\n",
    "    '''Get token, return word vector'''\n",
    "    \n",
    "    token=token.lower()\n",
    "    try:\n",
    "         vector=model[token]\n",
    "    except:\n",
    "        # if the word is not in vocabulary,\n",
    "        # returns zeros array\n",
    "        vector=np.zeros(100,)\n",
    "\n",
    "    return vector   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b90d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector = get_features(\"are\")\n",
    "#print(len(vector))\n",
    "#print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53fa515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2features(sent, i):\n",
    "    '''Get tokens in the sentence, add bias, token and word embeddings as features and return all as a feature dictionary.'''\n",
    "    \n",
    "    token = sent[i][0]\n",
    "    wordembdding=get_features(token)   ## word embedding vector \n",
    "    wordembdding=np.array(wordembdding) ## vectors \n",
    "    \n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'token': token.lower()\n",
    "    }\n",
    "    \n",
    "    for iv,value in enumerate(wordembdding):\n",
    "        features['v{}'.format(iv)]=value\n",
    "\n",
    "    if i == 0:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    elif i == len(sent) -1:\n",
    "        features['EOS'] = True\n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10974a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features= token2features(sents[0], i=0)\n",
    "\n",
    "#print(features)\n",
    "#print(type(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21130c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    '''Get sentence as an input, add the features and return as a list of dictionaries.'''\n",
    "    return [token2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f3b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test3 =sent2features(sents[0])\n",
    "\n",
    "#print(sents[0])\n",
    "#print(type(test3))\n",
    "#print(test3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5843afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crf_model(X_train, y_train):\n",
    "    '''Compile and fit the model'''\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    return crf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d26163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crf_model(trainingfile):\n",
    "    \n",
    "    '''Perform the training with the data, return the classifier'''\n",
    "\n",
    "    train_sents = extract_sents_from_conll(trainingfile)\n",
    "    X_train = [sent2features(s) for s in train_sents]\n",
    "    y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "    crf = train_crf_model(X_train, y_train)\n",
    "    \n",
    "    return crf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dba1cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crf_model(crf, evaluationfile):\n",
    "    \n",
    "    '''Get and prepare the validation sentences, run the classifier and return predictions'''\n",
    "\n",
    "    test_sents = extract_sents_from_conll(evaluationfile)\n",
    "    X_test = [sent2features(s) for s in test_sents]\n",
    "    y_test = [sent2labels(s) for s in test_sents]\n",
    "    y_pred = crf.predict(X_test)\n",
    "    \n",
    "    return y_pred, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22f5ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_evaluation(eval_data, pred_labels, outputfile):\n",
    "    \n",
    "    '''Write the predicitons to a new file along with tokens'''\n",
    "\n",
    "    outfile = open(outputfile, 'w', encoding=\"utf-8\")\n",
    "    \n",
    "    for evalsents, predsents in zip(eval_data, pred_labels):\n",
    "        for data, pred in zip(evalsents, predsents):\n",
    "            token = str(data.get('token'))\n",
    "            outfile.write(token + \"\\t\" + pred + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d811201",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb67ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate_crf_model(trainingfile, evaluationfile, outputfile):\n",
    "\n",
    "    '''Perform the full training at once'''\n",
    "    crf = create_crf_model(trainingfile)\n",
    "    labels = list(crf.classes_)\n",
    "    labels.remove('O')\n",
    "    labels.remove('AR_label')\n",
    "    labels\n",
    "    y_pred, X_test, y_test = run_crf_model(crf, evaluationfile)\n",
    "    write_out_evaluation(X_test, y_pred, outputfile)\n",
    "    print('The predictions are written on the output file.')\n",
    "    print(metrics.flat_classification_report(y_test, y_pred, labels=labels, digits=4))\n",
    "    print('Accuracy score for sequence items')\n",
    "    print(metrics.flat_accuracy_score(y_test, y_pred))\n",
    "    print('Precision score for sequence items')\n",
    "    print(metrics.flat_precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall score for sequence items')\n",
    "    print(metrics.flat_recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1 score score for sequence items')\n",
    "    print(metrics.flat_f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bca993",
   "metadata": {},
   "source": [
    "## Toy example to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13387e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_trainingfile = \"Toy_data_train.tsv\"\n",
    "toy_evaluationfile = \"Toy_data_eval.tsv\"\n",
    "toy_outputfile = \"toy_output_CRF_Embeddings.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bee3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_and_evaluate_crf_model(toy_trainingfile, toy_evaluationfile, toy_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f5bdc",
   "metadata": {},
   "source": [
    "## Set the variables and run the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a6ecd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "polnear_trainingfile = \"polnear_withBIO_train.tsv\"\n",
    "polnear_evaluationfile = \"polnear_withBIO_dev.tsv\"\n",
    "polnear_outputfile = \"polnear_output_CRF_Embeddings.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51a38b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=['AR_label', 'O', 'B-SOURCE', 'B-CUE', 'B-CONTENT', 'I-CONTENT', 'I-CUE', 'I-SOURCE', ''] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AR_label     0.0000    0.0000    0.0000         1\n",
      "           O     0.6470    0.7393    0.6901     32482\n",
      "    B-SOURCE     0.7376    0.4718    0.5755      1948\n",
      "       B-CUE     0.7978    0.5279    0.6353      2190\n",
      "   B-CONTENT     0.5946    0.3999    0.4782      2193\n",
      "   I-CONTENT     0.7332    0.7250    0.7291     36881\n",
      "       I-CUE     0.4102    0.2085    0.2765      1808\n",
      "    I-SOURCE     0.6128    0.4371    0.5103      4070\n",
      "                 0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.6848    0.6848    0.6848     81573\n",
      "   macro avg     0.5037    0.3899    0.4328     81573\n",
      "weighted avg     0.6838    0.6848    0.6797     81573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training takes around 1 hr\n",
    "run_and_evaluate_crf_model(polnear_trainingfile, polnear_evaluationfile, polnear_outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b31d4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training takes around 3.5 hrs\n",
    "\n",
    "parc3_trainingfile = \"parc3_withBIO_train.tsv\"\n",
    "parc3_evaluationfile = \"parc3_withBIO_dev.tsv\"\n",
    "parc3_outputfile = \"parc3_output_CRF_Embeddings.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c095933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictions are written on the output file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=['B-CONTENT', 'I-CONTENT', 'B-SOURCE', 'B-CUE', 'I-SOURCE', 'I-CUE'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT     0.6250    0.2849    0.3914       544\n",
      "   I-CONTENT     0.6249    0.3893    0.4797      8888\n",
      "    B-SOURCE     0.8277    0.3855    0.5260       511\n",
      "       B-CUE     0.8664    0.4011    0.5483       566\n",
      "    I-SOURCE     0.6898    0.3713    0.4828      1605\n",
      "       I-CUE     0.5000    0.0870    0.1481       184\n",
      "\n",
      "   micro avg     0.6477    0.3782    0.4775     12298\n",
      "   macro avg     0.6890    0.3198    0.4294     12298\n",
      "weighted avg     0.6510    0.3782    0.4763     12298\n",
      "\n",
      "Accuracy score for sequence items\n",
      "0.7080599812558576\n",
      "Precision score for sequence items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filiz\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6977904875340245\n",
      "Recall score for sequence items\n",
      "0.7080599812558576\n",
      "F1 score score for sequence items\n",
      "0.6834464135971214\n"
     ]
    }
   ],
   "source": [
    "run_and_evaluate_crf_model(parc3_trainingfile, parc3_evaluationfile, parc3_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb40c5c",
   "metadata": {},
   "source": [
    "End of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170233a9",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-use-conll-2002-data-to-build-a-ner-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032d89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
