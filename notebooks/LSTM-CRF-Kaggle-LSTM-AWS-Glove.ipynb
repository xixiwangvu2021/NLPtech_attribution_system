{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "https://www.kaggle.com/williamroe/bi-lstm-with-crf-for-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow-cpu 2.3.0\n",
      "Uninstalling tensorflow-cpu-2.3.0:\n",
      "  Successfully uninstalled tensorflow-cpu-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: tensorflow 2.3.0\n",
      "Uninstalling tensorflow-2.3.0:\n",
      "  Successfully uninstalled tensorflow-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: tensorflow-addons 0.13.0\n",
      "Uninstalling tensorflow-addons-0.13.0:\n",
      "  Successfully uninstalled tensorflow-addons-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping keras-crf as it is not installed.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping tf2crf as it is not installed.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping keras-contrib as it is not installed.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: Keras 2.1.6\n",
      "Uninstalling Keras-2.1.6:\n",
      "  Successfully uninstalled Keras-2.1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping s3fs as it is not installed.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip uninstall --yes tensorflow-cpu\n",
    "# %pip uninstall --yes tensorflow\n",
    "# %pip uninstall --yes tensorflow_addons\n",
    "# %pip uninstall --yes keras-crf\n",
    "# %pip uninstall --yes tf2crf\n",
    "# %pip uninstall --yes keras-contrib  # Successfully uninstalled keras-contrib-2.0.8\n",
    "# %pip uninstall --yes keras  # Successfully uninstalled Keras-2.2.1\n",
    "# %pip uninstall --yes s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.1.6\n",
      "  Using cached Keras-2.1.6-py2.py3-none-any.whl (339 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from keras==2.1.6) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from keras==2.1.6) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from keras==2.1.6) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from keras==2.1.6) (1.15.0)\n",
      "Requirement already satisfied: h5py in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from keras==2.1.6) (2.10.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras==2.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-cpu==2.3.0\n",
      "  Using cached tensorflow_cpu-2.3.0-cp38-cp38-macosx_10_11_x86_64.whl (165.2 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (0.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (0.35.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.34.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (2.5.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu==2.3.0) (3.15.8)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (1.31.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-cpu==2.3.0) (3.1.1)\n",
      "Installing collected packages: tensorflow-cpu\n",
      "Successfully installed tensorflow-cpu-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-cpu==2.3.0\n",
    "\n",
    "# keras-applications 1.0.4 requires keras>=2.1.6, which is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.3.0\n",
      "  Using cached tensorflow-2.3.0-cp38-cp38-macosx_10_11_x86_64.whl (165.2 MB)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.15.8)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.5.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: scipy==1.4.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.34.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.35.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.31.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow==2.4.0  # 2.5.0  # 2.2.0\n",
    "%pip install tensorflow==2.3.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons==0.13.0\n",
      "  Using cached tensorflow_addons-0.13.0-cp38-cp38-macosx_10_13_x86_64.whl (511 kB)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_addons==0.13.0) (2.12.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow_addons==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install keras-crf==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "# # %pip install keras-contrib==0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2crf==0.1.32\n",
      "  Using cached tf2crf-0.1.32-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tf2crf==0.1.32) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-addons>=0.8.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tf2crf==0.1.32) (0.13.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.34.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.18.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.12.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (3.15.8)\n",
      "Requirement already satisfied: scipy==1.4.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (1.4.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (2.5.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (0.35.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow>=2.1.0->tf2crf==0.1.32) (3.3.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-addons>=0.8.2->tf2crf==0.1.32) (2.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (50.3.1.post20201107)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (1.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (4.7.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->tf2crf==0.1.32) (0.4.8)\n",
      "Installing collected packages: tf2crf\n",
      "Successfully installed tf2crf-0.1.32\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/xuxingya/tf2crf\n",
    "%pip install tf2crf==0.1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env SM_FRAMEWORK=tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall --yes tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import s3fs\n",
    "\n",
    "what_corpus = 'data_total'  # 'polnear', 'parc30'\n",
    "what_type_files = 'train'\n",
    "what_type_test_file = 'test'\n",
    "what_type_val_file = 'dev'\n",
    "\n",
    "# filepath = 's3://sagemaker-studio-528576943967-ssf9zkrg3os/polnear-conll/prepared/'\n",
    "filepath = '../' + what_corpus + '-conll/prepared/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = what_corpus + '_preprocessed_' + what_type_files + '_noBIO.csv'\n",
    "df_train = pd.read_csv(filepath + train_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Unnamed: 0.1, filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1794014\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cuban</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>would</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>consider</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>future</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>House</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>bid</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Cuban</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>billionaire</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>owner</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.1                                           filename  \\\n",
       "0              0  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "1              1  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "2              2  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "3              3  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "4              4  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "5              5  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "6              6  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "7              7  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "8              8  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "9              9  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "10            10  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "11            11  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "12            12  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "13            13  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "14            14  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "15            15  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "16            16  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "17            17  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "18            18  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "19            19  politico_2016-05-22_mark-cuban-i-d-consider-a-...   \n",
       "\n",
       "    sentence_idx         word      tag  \n",
       "0              1         Mark   SOURCE  \n",
       "1              1        Cuban   SOURCE  \n",
       "2              1            :      CUE  \n",
       "3              1            I  CONTENT  \n",
       "4              1        would  CONTENT  \n",
       "5              1     consider  CONTENT  \n",
       "6              1            a  CONTENT  \n",
       "7              1       future  CONTENT  \n",
       "8              1        White  CONTENT  \n",
       "9              1        House  CONTENT  \n",
       "10             1          bid  CONTENT  \n",
       "11             1            .        O  \n",
       "12             2         Mark   SOURCE  \n",
       "13             2        Cuban   SOURCE  \n",
       "14             2            ,   SOURCE  \n",
       "15             2          the   SOURCE  \n",
       "16             2  billionaire   SOURCE  \n",
       "17             2        owner   SOURCE  \n",
       "18             2           of   SOURCE  \n",
       "19             2          the   SOURCE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train.index))\n",
    "del df_train['Unnamed: 0']\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = what_corpus + '_preprocessed_' + what_type_test_file + '_noBIO.csv'\n",
    "df_test = pd.read_csv(filepath + test_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130042\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>drops</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Into</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>get</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>black</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>turnout</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>--</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>increasingly</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.1                                           filename  \\\n",
       "0              0  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "1              1  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "2              2  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "3              3  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "4              4  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "5              5  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "6              6  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "7              7  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "8              8  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "9              9  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "10            10  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "11            11  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "12            12  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "13            13  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "14            14  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "15            15  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "16            16  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "17            17  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "18            18  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "19            19  huff-post_2016-11-05_hillary-clinton-drops-int...   \n",
       "\n",
       "    sentence_idx          word      tag  \n",
       "0              1       Hillary        O  \n",
       "1              1       Clinton        O  \n",
       "2              1         drops        O  \n",
       "3              1          Into        O  \n",
       "4              1       Detroit        O  \n",
       "5              1            as        O  \n",
       "6              1     Democrats   SOURCE  \n",
       "7              1           get      CUE  \n",
       "8              1       nervous      CUE  \n",
       "9              1         about      CUE  \n",
       "10             1         black  CONTENT  \n",
       "11             1       turnout  CONTENT  \n",
       "12             1             .        O  \n",
       "13             2       DETROIT        O  \n",
       "14             2            --        O  \n",
       "15             2          with        O  \n",
       "16             2     Democrats   SOURCE  \n",
       "17             2  increasingly      CUE  \n",
       "18             2       nervous      CUE  \n",
       "19             2         about      CUE  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_test.index))\n",
    "del df_test['Unnamed: 0']\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_filename = what_corpus + '_preprocessed_' + what_type_val_file + '_noBIO.csv'\n",
    "df_val = pd.read_csv(filepath + val_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gold</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Star</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>mom</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Corners</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Obama</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>refusal</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>use</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>word</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>`</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>islamic</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>'</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>President</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Barack</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Obama</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.1                                           filename  \\\n",
       "0              0  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "1              1  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "2              2  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "3              3  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "4              4  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "5              5  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "6              6  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "7              7  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "8              8  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "9              9  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "10            10  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "11            11  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "12            12  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "13            13  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "14            14  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "15            15  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "16            16  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "17            17  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "18            18  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "19            19  west-journal_2016-09-29_gold-star-mom-corners-...   \n",
       "\n",
       "    sentence_idx       word      tag  \n",
       "0              1       Gold        O  \n",
       "1              1       Star        O  \n",
       "2              1        mom        O  \n",
       "3              1    Corners        O  \n",
       "4              1      Obama        O  \n",
       "5              1         on        O  \n",
       "6              1         he   SOURCE  \n",
       "7              1    refusal      CUE  \n",
       "8              1         to  CONTENT  \n",
       "9              1        use  CONTENT  \n",
       "10             1        the  CONTENT  \n",
       "11             1       word  CONTENT  \n",
       "12             1          `  CONTENT  \n",
       "13             1    islamic  CONTENT  \n",
       "14             1  terrorist  CONTENT  \n",
       "15             1          .  CONTENT  \n",
       "16             1          '  CONTENT  \n",
       "17             2  President   SOURCE  \n",
       "18             2     Barack   SOURCE  \n",
       "19             2      Obama   SOURCE  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_val.index))\n",
    "del df_val['Unnamed: 0']\n",
    "df_val.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "#         agg_func = lambda s: ' '.join(s[\"word\"].values.tolist())\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby([\"filename\", \"sentence_idx\"]).apply(agg_func)\n",
    "#         self.grouped = self.dataset.groupby([\"sentence_idx\"]).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_train = SentenceGetter(df_train)\n",
    "getter_test = SentenceGetter(df_test)\n",
    "getter_val = SentenceGetter(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = getter_train.sentences\n",
    "sentences_test = getter_test.sentences\n",
    "sentences_val = getter_val.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75048\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_train))\n",
    "# sentences_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38179\n",
      "19089.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('in', 'O'),\n",
       "  ('we', 'O'),\n",
       "  ('last', 'O'),\n",
       "  ('epistle', 'O'),\n",
       "  ('from', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('bottomless', 'O'),\n",
       "  ('pit', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('we', 'SOURCE'),\n",
       "  ('mock', 'CUE'),\n",
       "  ('the', 'CONTENT'),\n",
       "  ('presidential', 'CONTENT'),\n",
       "  ('ambition', 'CONTENT'),\n",
       "  ('of', 'CONTENT'),\n",
       "  ('three', 'CONTENT'),\n",
       "  ('republican', 'CONTENT'),\n",
       "  ('hopeful', 'CONTENT'),\n",
       "  (':', 'CONTENT'),\n",
       "  ('Jeb', 'CONTENT'),\n",
       "  ('!', 'CONTENT')],\n",
       " [(',', 'CONTENT'),\n",
       "  ('Ben', 'CONTENT'),\n",
       "  ('Carson', 'CONTENT'),\n",
       "  (',', 'CONTENT'),\n",
       "  ('and', 'CONTENT'),\n",
       "  ('Donald', 'CONTENT'),\n",
       "  ('Trump', 'CONTENT'),\n",
       "  ('.', 'O')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train_only_labeled = []\n",
    "for sentence_train in sentences_train:\n",
    "    is_labeled = False\n",
    "    for sentence_train_word in sentence_train:\n",
    "        label = sentence_train_word[1] # Label is the second of the tuple\n",
    "        if label != 'O':\n",
    "            is_labeled = True\n",
    "            break\n",
    "    if is_labeled:\n",
    "        sentences_train_only_labeled.append(sentence_train)\n",
    "        \n",
    "sentences_train = sentences_train_only_labeled\n",
    "print(len(sentences_train))\n",
    "print(len(sentences_train)/2)\n",
    "sentences_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_train = sentences_train[0:15171]  # parc: 22352]\n",
    "# print(len(sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 'CONTENT'), ('Ben', 'CONTENT'), ('Carson', 'CONTENT'), (',', 'CONTENT'), ('and', 'CONTENT'), ('Donald', 'CONTENT'), ('Trump', 'CONTENT'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5438\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length train: 141\n",
      "Maximum sequence length test: 94\n",
      "Maximum sequence length val: 116\n"
     ]
    }
   ],
   "source": [
    "maxlen = max([len(s) for s in sentences_train])\n",
    "print ('Maximum sequence length train:', maxlen)\n",
    "maxlen_test = max([len(s) for s in sentences_test])\n",
    "print ('Maximum sequence length test:', maxlen_test)\n",
    "maxlen_val = max([len(s) for s in sentences_val])\n",
    "print ('Maximum sequence length val:', maxlen_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_train = ['', '[UNK]']\n",
    "# words_train.extend(list(set(df_train[\"word\"].values)))\n",
    "# words_train.append(\"ENDPAD\")\n",
    "# words_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', Ben Carson , and Donald Trump .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_train = [' '.join([w[0] for w in s]) for s in sentences_train]\n",
    "sentences_texts_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Washington -lrb- AFP -rrb- -- the Washington Post on Thursday become the latest US newspaper to emphatically endorse Hillary Clinton for the White House , say it be sway as much by she competence as by the alarming specter of a Donald Trump presidency .'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_test = [' '.join([w[0] for w in s]) for s in sentences_test]\n",
    "sentences_texts_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the New York Post report that Hillary Clinton 's team avoid take she to the emergency room follow she medical scare on Sunday in order to `` keep the detail of she medical treatment under wrap . ''\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_val = [' '.join([w[0] for w in s]) for s in sentences_val]\n",
    "sentences_texts_val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=50000, output_sequence_length=maxlen)\n",
    "# vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = Dataset.from_tensor_slices(df_train[\"word\"].values).batch(128)\n",
    "text_ds = Dataset.from_tensor_slices(sentences_texts_train).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'be', 'to']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "# output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 65, 17, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"be\", \"get\", \"on\", \"the\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2021-06-17 10:26:53,691 - glove2word2vec - INFO - running /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/gensim/scripts/glove2word2vec.py --input glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
      "2021-06-17 10:26:53,824 - glove2word2vec - INFO - converting 400000 vectors from glove.6B.50d.txt to glove.6B.50d.w2vformat.txt\n",
      "2021-06-17 10:26:54,531 - glove2word2vec - INFO - Converted model with 400000 vectors and 50 dimensions\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# word_vecs = KeyedVectors.load_word2vec_format(\"glove.txt\") \n",
    "# https://www.kaggle.com/watts2/glove6b50dtxt\n",
    "glove_dimensions = 50\n",
    "!python -m gensim.scripts.glove2word2vec --input  glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
    "word_vecs = KeyedVectors.load_word2vec_format(\"glove.6B.50d.w2vformat.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test word_vecs\n",
    "word_vecs.word_vec('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(word_vecs.word_vec('the')).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 23504 words (5667 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = glove_dimensions\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "# for i, word in enumerate(words_train):\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     print(i)\n",
    "    try:\n",
    "        embedding_vector = word_vecs.word_vec(word)\n",
    "#     if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "#     else:\n",
    "    except KeyError:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_test = list(set(df_test[\"word\"].values))\n",
    "# words_test.append(\"ENDPAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = len(words_train)\n",
    "# n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'CONTENT', 'SOURCE', 'CUE']\n"
     ]
    }
   ],
   "source": [
    "from math import nan\n",
    "\n",
    "tags = []\n",
    "for index, tag in enumerate(set(df_train[\"tag\"].values)):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags.append('unk')\n",
    "    else:\n",
    "        tags.append(tag)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "# word2idx_train = {w: i for i, w in enumerate(words_train)}\n",
    "# word2idx_test = {w: i for i, w in enumerate(words_test)}\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx_train['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'CONTENT': 1, 'SOURCE': 2, 'CUE': 3}\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUE'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O', 1: 'CONTENT', 2: 'SOURCE', 3: 'CUE'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer(np.array([[s] for s in sentences_texts_train])).numpy()\n",
    "X_test = vectorizer(np.array([[s] for s in sentences_texts_test])).numpy()\n",
    "X_val = vectorizer(np.array([[s] for s in sentences_texts_val])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = [[word2idx_train[w[0]] for w in s] for s in sentences_train]\n",
    "# X_test = [[word2idx_test[w[0]] for w in s] for s in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 1530  268  844  662   30    2  862    4 6428   18  166    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38179, 141)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\",value=n_words - 1)\n",
    "# print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\",value=n_words - 1)\n",
    "# print(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_train = [[tag2idx[w[1]] for w in s] for s in sentences_train]\n",
    "# print('sentences_train[25]')\n",
    "# print(sentences_train[25])\n",
    "# print('y_idx_train[25]')\n",
    "# print(y_idx_train[25])\n",
    "# print('sentences_train[10]')\n",
    "# print(sentences_train[10])\n",
    "# print('y_idx_train[10]')\n",
    "# print(y_idx_train[10])\n",
    "# print(len(y_idx_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_test = [[tag2idx[w[1]] for w in s] for s in sentences_test]\n",
    "# print('sentences_test[25]')\n",
    "# print(sentences_test[25])\n",
    "# print('y_idx_test[25]')\n",
    "# print(y_idx_test[25])\n",
    "# print('sentences_test[10]')\n",
    "# print(sentences_test[10])\n",
    "# print('y_idx_test[10]')\n",
    "# print(y_idx_test[10])\n",
    "# print(len(y_idx_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_val = [[tag2idx[w[1]] for w in s] for s in sentences_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "y_train = pad_sequences(maxlen=maxlen, sequences=y_idx_train, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_train[10])\n",
    "print(len(y_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "y_test = pad_sequences(maxlen=maxlen, sequences=y_idx_test, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_test[10])\n",
    "print(len(y_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "y_val = pad_sequences(maxlen=maxlen, sequences=y_idx_val, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_val[10])\n",
    "print(len(y_val[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
    "print(y_train[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n",
    "print(y_test[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_val = [to_categorical(i, num_classes=n_tags) for i in y_val]\n",
    "print(y_val[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# X_train, y_train = X, y\n",
    "# X_test, y_test = X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "47\n",
      "[0. 1. 0. 0.]\n",
      "[0. 0. 1. 0.]\n",
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10][0])\n",
    "print(X_test[10][0])\n",
    "print(X_val[10][0])\n",
    "print(y_train[10][0])\n",
    "print(y_test[10][0])\n",
    "print(y_val[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Input, InputLayer, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Model, Sequential  # , Input\n",
    "import tensorflow.keras as k\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(k.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.utils as generic_utils\n",
    "# from keras_contrib.layers.crf import CRF\n",
    "# AttributeError: module 'tensorflow.compat.v2' has no attribute '__internal__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_size = embedding_dim  # glove_dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xuxingya/tf2crf\n",
    "\n",
    "More inspiration:\n",
    "https://github.com/Akshayc1/named-entity-recognition/blob/master/NER%20using%20Bidirectional%20LSTM%20-%20CRF%20.ipynb\n",
    "\n",
    "https://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kaggle example code \n",
    "\n",
    "# inputs = Input(shape=(None,), dtype=\"int64\")  \n",
    "# # inputs = Input(shape=(maxlen,))\n",
    "# # https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n",
    "# # https://keras.io/examples/nlp/pretrained_word_embeddings/ --> pretrained embeddings\n",
    "# # outputs = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(inputs)\n",
    "# outputs = Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,)(inputs)\n",
    "# outputs = Bidirectional(LSTM(units=word_embedding_size, \n",
    "#                              return_sequences=True, \n",
    "#                              dropout=0.5,  # 0.2, 0.5, \n",
    "#                              recurrent_dropout=0.5,  # 0.2, 0.5, \n",
    "#                              kernel_initializer=k.initializers.he_normal()))(outputs)\n",
    "# # https://github.com/xuxingya/tf2crf: Add internal kernel like CRF in keras_contrib, so now there is no need to stack a Dense layer before the CRF layer.\n",
    "# outputs = TimeDistributed(Dense(n_tags, activation=\"relu\"))(outputs)  # previously softmax output layer\n",
    "# # outputs = TimeDistributed(Dense(n_tags, activation=tensorflow.keras.activations.softmax))(outputs)  # previously softmax output layer\n",
    "\n",
    "# # crf = CRF(n_tags)  # CRF layer\n",
    "# # outputs = crf(outputs)  # output\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "\n",
    "# # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "# adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# # https://stackoverflow.com/questions/61742556/valueerror-shapes-none-1-and-none-2-are-incompatible\n",
    "# # model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # categorical_accuracy?\n",
    "# # model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "# # model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])# Saving the best only\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfcrf version\n",
    "# from tf2crf import CRF\n",
    "# from tf2crf import ModelWithCRFLoss\n",
    "\n",
    "# inputs = Input(shape=(None,), dtype=\"int64\")  \n",
    "# # inputs = Input(shape=(maxlen,))\n",
    "# # https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n",
    "# # https://keras.io/examples/nlp/pretrained_word_embeddings/ --> pretrained embeddings\n",
    "# # outputs = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(inputs)\n",
    "# outputs = Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,)(inputs)\n",
    "# outputs = Bidirectional(LSTM(units=word_embedding_size, \n",
    "#                              return_sequences=True, \n",
    "#                              dropout=0.5,  # 0.2, 0.5, \n",
    "#                              recurrent_dropout=0.5,  # 0.2, 0.5, \n",
    "#                              kernel_initializer=k.initializers.he_normal()))(outputs)\n",
    "# # https://github.com/xuxingya/tf2crf: Add internal kernel like CRF in keras_contrib, so now there is no need to stack a Dense layer before the CRF layer.\n",
    "# # outputs = TimeDistributed(Dense(n_tags, activation=\"relu\"))(outputs)  # previously softmax output layer\n",
    "# # outputs = TimeDistributed(Dense(n_tags, activation=tensorflow.keras.activations.softmax))(outputs)  # previously softmax output layer\n",
    "\n",
    "# crf = CRF(n_tags)  # CRF layer\n",
    "# outputs = crf(outputs)  # output\n",
    "\n",
    "# # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "# adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# base_model = Model(inputs, outputs)\n",
    "# model = ModelWithCRFLoss(base_model, sparse_target=False)  # True)\n",
    "\n",
    "# model.compile(optimizer='adam')\n",
    "# # model.compile(optimizer=adam)\n",
    "# # https://stackoverflow.com/questions/61742556/valueerror-shapes-none-1-and-none-2-are-incompatible\n",
    "# # model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # categorical_accuracy?\n",
    "# # model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "# # model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])  # Saving the best only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 50)          1458650   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 100)         40400     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 4)           404       \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, None, 4)           16        \n",
      "=================================================================\n",
      "Total params: 1,499,470\n",
      "Trainable params: 40,820\n",
      "Non-trainable params: 1,458,650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential TF addons CRF\n",
    "# https://github.com/tensorflow/addons/issues/1769\n",
    "from crf import CRF\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(None,), dtype=\"int64\"))\n",
    "model.add(Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,))\n",
    "model.add(Bidirectional(LSTM(units=word_embedding_size, \n",
    "                             return_sequences=True, \n",
    "                             dropout=0.5,  # 0.2, 0.5, \n",
    "                             recurrent_dropout=0.5,  # 0.2, 0.5, \n",
    "                             kernel_initializer=k.initializers.he_normal())))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "crf = CRF(n_tags, sparse_target=False)\n",
    "model.add(crf)\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # categorical_accuracy?\n",
    "# model.compile(optimizer=adam, loss=crf.loss, metrics=[crf.accuracy])\n",
    "model.summary()\n",
    "\n",
    "# /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/text/crf.py:540: \n",
    "# UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
    "\n",
    "# /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:376: \n",
    "# UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476 return py_builtins.overload_of(f)(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model TF addons CRF\n",
    "# # https://github.com/tensorflow/addons/issues/1769\n",
    "# from crf import CRF\n",
    "\n",
    "# inputs = Input(shape=(None,), dtype=\"int64\")  \n",
    "# # inputs = Input(shape=(maxlen,))\n",
    "# # https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n",
    "# # https://keras.io/examples/nlp/pretrained_word_embeddings/ --> pretrained embeddings\n",
    "# # outputs = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(inputs)\n",
    "# outputs = Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,)(inputs)\n",
    "# outputs = Bidirectional(LSTM(units=word_embedding_size, \n",
    "#                              return_sequences=True, \n",
    "#                              dropout=0.5,  # 0.2, 0.5, \n",
    "#                              recurrent_dropout=0.5,  # 0.2, 0.5, \n",
    "#                              kernel_initializer=k.initializers.he_normal()))(outputs)\n",
    "# # https://github.com/xuxingya/tf2crf: Add internal kernel like CRF in keras_contrib, so now there is no need to stack a Dense layer before the CRF layer.\n",
    "# outputs = TimeDistributed(Dense(n_tags, activation=\"relu\"))(outputs)  # previously softmax output layer\n",
    "# # outputs = TimeDistributed(Dense(n_tags, activation=tensorflow.keras.activations.softmax))(outputs)  # previously softmax output layer\n",
    "\n",
    "# crf = CRF(n_tags, sparse_target=False)\n",
    "# outputs = crf(outputs)  # output\n",
    "# model = Model(inputs, outputs)\n",
    "\n",
    "# # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "# adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "\n",
    "# # https://stackoverflow.com/questions/61742556/valueerror-shapes-none-1-and-none-2-are-incompatible\n",
    "# # model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # metrics=[\"categorical_accuracy\"]?\n",
    "# # model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "# # model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])# Saving the best only\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the best only\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
    "filepath = 'tmp/checkpoint'\n",
    "checkpoint_acc = ModelCheckpoint(filepath + '/acc', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint_loss = ModelCheckpoint(filepath + '/loss', monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint_acc, checkpoint_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate generalization metrics\n",
    "# i = len(X_test) - 1 \n",
    "# score = model.evaluate(np.array([X_test[:i]]), y_test, verbose=0)\n",
    "# print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['crf_1/transitions:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['crf_1/transitions:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['crf_1/transitions:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['crf_1/transitions:0'] when minimizing the loss.\n",
      "120/120 [==============================] - ETA: 0s - loss: nan - accuracy: 0.8490\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84141, saving model to tmp/checkpoint/acc\n",
      "WARNING:tensorflow:From /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: tmp/checkpoint/acc/assets\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 2.55624, saving model to tmp/checkpoint/loss\n",
      "INFO:tensorflow:Assets written to: tmp/checkpoint/loss/assets\n",
      "120/120 [==============================] - 67s 556ms/step - loss: nan - accuracy: 0.8490 - val_loss: 2.5562 - val_accuracy: 0.8414\n",
      "Epoch 2/2\n",
      "120/120 [==============================] - ETA: 0s - loss: nan - accuracy: 0.8508\n",
      "Epoch 00002: val_accuracy did not improve from 0.84141\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.55624\n",
      "120/120 [==============================] - 40s 330ms/step - loss: nan - accuracy: 0.8508 - val_loss: 2.5562 - val_accuracy: 0.8414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x176feccd0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X_train, np.array(y_train), batch_size=256, epochs=1, validation_split=0.1, verbose=1, callbacks=callbacks_list)\n",
    "model.fit(X_train, np.array(y_train), batch_size=256, epochs=2, validation_split=0.2, verbose=1, callbacks=callbacks_list)\n",
    "# model.fit(X_train, np.array(y_train), batch_size=256, epochs=50, validation_data=(X_val, y_val), verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = {}\n",
    "TN = {}\n",
    "FP = {}\n",
    "FN = {}\n",
    "for tag in tag2idx.keys():\n",
    "    TP[tag] = 0\n",
    "    TN[tag] = 0    \n",
    "    FP[tag] = 0    \n",
    "    FN[tag] = 0    \n",
    "\n",
    "def accumulate_score_by_tag(gt, pred):\n",
    "    \"\"\"\n",
    "    For each tag keep stats\n",
    "    \"\"\"\n",
    "    if gt == pred:\n",
    "        TP[gt] += 1\n",
    "    elif gt != 'O' and pred == 'O':\n",
    "        FN[gt] +=1\n",
    "    elif gt == 'O' and pred != 'O':\n",
    "        FP[gt] += 1\n",
    "    else:\n",
    "        TN[gt] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  291   306   137 ...     0     0     0]\n",
      " [  291    96  5136 ...     0     0     0]\n",
      " [  115    32    12 ...     0     0     0]\n",
      " ...\n",
      " [14941 14160     9 ...     0     0     0]\n",
      " [    2     1   214 ...     0     0     0]\n",
      " [   56     3    76 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2  # len(X_test) - 1  # Last one \n",
    "# p = model.predict(np.array([X_test[i]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "# print(p.shape)\n",
    "# gt = np.argmax(y_test[i], axis=-1)\n",
    "# print(gt)\n",
    "# print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(p)\n",
    "# for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
    "#     if words_test[w] == 'ENDPAD':\n",
    "#         break\n",
    "#     print(\"{:14}: ({:5}): {}\".format(words_test[w],idx2tag[gt[idx]],tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5438, 141, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[100][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p, axis=axis)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.91      1.00      0.95    696169\n",
      "     CONTENT       0.00      0.00      0.00     56040\n",
      "      SOURCE       0.00      0.00      0.00      9511\n",
      "         CUE       0.00      0.00      0.00      5038\n",
      "\n",
      "    accuracy                           0.91    766758\n",
      "   macro avg       0.23      0.25      0.24    766758\n",
      "weighted avg       0.82      0.91      0.86    766758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=axis).ravel(), np.argmax(p, axis=axis).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values())))\n",
    "\n",
    "# Warning might be caused by small test set compared to train set?\n",
    "# /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: \n",
    "# Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "#   _warn_prf(average, modifier, msg_start, len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(X_test):\n",
    "    y_hat = np.argmax(p[i], axis=-1)\n",
    "    gt = np.argmax(y_test[i], axis=-1)\n",
    "    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
    "        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag:O\n",
      "\t TN:         0\tFP:         0\n",
      "\t FN:         0\tTP:    696169\n",
      "tag:CONTENT\n",
      "\t TN:         0\tFP:         0\n",
      "\t FN:     56040\tTP:         0\n",
      "tag:SOURCE\n",
      "\t TN:         0\tFP:         0\n",
      "\t FN:      9511\tTP:         0\n",
      "tag:CUE\n",
      "\t TN:         0\tFP:         0\n",
      "\t FN:      5038\tTP:         0\n"
     ]
    }
   ],
   "source": [
    "for tag in tag2idx.keys():\n",
    "    print(f'tag:{tag}')    \n",
    "    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
    "    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_py37_venv",
   "language": "python",
   "name": "jupyter_py37_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
