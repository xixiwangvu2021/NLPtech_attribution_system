{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "https://www.kaggle.com/williamroe/bi-lstm-with-crf-for-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.0\n",
      "  Downloading tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.7 MB 3.7 kB/s  eta 0:00:01     |███████████████▍                | 189.1 MB 42.9 MB/s eta 0:00:05\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (0.2.0)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 50.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (1.19.5)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 323 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (3.7.4.3)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 66.0 MB/s eta 0:00:01     |█████████▎                      | 1.7 MB 66.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (3.15.2)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (1.15.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.31.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 60.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.25.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (49.6.0.post20210108)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 63.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.4.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=f632ec3457383e2a86daa59bacd01113d7fb7d3cc13a7a8ca8294daac519b2f4\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.3.3 google-auth-1.31.0 google-auth-oauthlib-0.4.4 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==2.4.0  # 2.5.0  # 2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow_addons==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install keras-crf==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow-cpu==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "# # %pip install keras-contrib==0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/xuxingya/tf2crf\n",
    "# %pip install tf2crf==0.1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env SM_FRAMEWORK=tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall --yes tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.4.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (0.8.7)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.20.76)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from fsspec>=0.6.0->s3fs) (3.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->fsspec>=0.6.0->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->fsspec>=0.6.0->s3fs) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import s3fs\n",
    "\n",
    "what_corpus = 'polnear'\n",
    "what_type_files = 'train'\n",
    "what_type_test_file = 'test'\n",
    "what_type_val_file = 'dev'\n",
    "\n",
    "# filepath = 's3://sagemaker-studio-528576943967-ssf9zkrg3os/polnear-conll/prepared/'\n",
    "filepath = '../' + what_corpus + '-conll/prepared/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = what_corpus + '_preprocessed_' + what_type_files + '_noBIO.csv'\n",
    "df_train = pd.read_csv(filepath + train_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cuban</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>would</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>consider</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>future</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>House</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>bid</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Cuban</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>billionaire</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>owner</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "1   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "2   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "3   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "4   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "5   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "6   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "7   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "8   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "9   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "10  politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "11  politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "12  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "13  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "14  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "15  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "16  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "17  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "18  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "19  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "\n",
       "           word      tag  \n",
       "0          Mark   SOURCE  \n",
       "1         Cuban   SOURCE  \n",
       "2             :      CUE  \n",
       "3             I  CONTENT  \n",
       "4         would  CONTENT  \n",
       "5      consider  CONTENT  \n",
       "6             a  CONTENT  \n",
       "7        future  CONTENT  \n",
       "8         White  CONTENT  \n",
       "9         House  CONTENT  \n",
       "10          bid  CONTENT  \n",
       "11            .        O  \n",
       "12         Mark   SOURCE  \n",
       "13        Cuban   SOURCE  \n",
       "14            ,   SOURCE  \n",
       "15          the   SOURCE  \n",
       "16  billionaire   SOURCE  \n",
       "17        owner   SOURCE  \n",
       "18           of   SOURCE  \n",
       "19          the   SOURCE  "
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train.index))\n",
    "del df_train['Unnamed: 0']\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = what_corpus + '_preprocessed_' + what_type_test_file + '_noBIO.csv'\n",
    "df_test = pd.read_csv(filepath + test_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73370\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>drops</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Into</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>get</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>black</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>turnout</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>--</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>increasingly</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "1   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "2   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "3   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "4   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "5   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "6   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "7   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "8   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "9   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "10  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "11  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "12  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "13  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "14  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "15  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "16  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "17  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "18  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "19  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "\n",
       "            word      tag  \n",
       "0        Hillary        O  \n",
       "1        Clinton        O  \n",
       "2          drops        O  \n",
       "3           Into        O  \n",
       "4        Detroit        O  \n",
       "5             as        O  \n",
       "6      Democrats   SOURCE  \n",
       "7            get      CUE  \n",
       "8        nervous      CUE  \n",
       "9          about      CUE  \n",
       "10         black  CONTENT  \n",
       "11       turnout  CONTENT  \n",
       "12             .        O  \n",
       "13       DETROIT        O  \n",
       "14            --        O  \n",
       "15          with        O  \n",
       "16     Democrats   SOURCE  \n",
       "17  increasingly      CUE  \n",
       "18       nervous      CUE  \n",
       "19         about      CUE  "
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_test.index))\n",
    "del df_test['Unnamed: 0']\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_filename = what_corpus + '_preprocessed_' + what_type_val_file + '_noBIO.csv'\n",
    "df_val = pd.read_csv(filepath + val_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gold</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Star</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>mom</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Corners</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Obama</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>refusal</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>use</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>word</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>`</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>islamic</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>'</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>President</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Barack</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Obama</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "1   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "2   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "3   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "4   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "5   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "6   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "7   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "8   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "9   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "10  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "11  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "12  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "13  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "14  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "15  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "16  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "17  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "18  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "19  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "\n",
       "         word      tag  \n",
       "0        Gold        O  \n",
       "1        Star        O  \n",
       "2         mom        O  \n",
       "3     Corners        O  \n",
       "4       Obama        O  \n",
       "5          on        O  \n",
       "6          he   SOURCE  \n",
       "7     refusal      CUE  \n",
       "8          to  CONTENT  \n",
       "9         use  CONTENT  \n",
       "10        the  CONTENT  \n",
       "11       word  CONTENT  \n",
       "12          `  CONTENT  \n",
       "13    islamic  CONTENT  \n",
       "14  terrorist  CONTENT  \n",
       "15          .  CONTENT  \n",
       "16          '  CONTENT  \n",
       "17  President   SOURCE  \n",
       "18     Barack   SOURCE  \n",
       "19      Obama   SOURCE  "
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_val.index))\n",
    "del df_val['Unnamed: 0']\n",
    "df_val.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "#         agg_func = lambda s: ' '.join(s[\"word\"].values.tolist())\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby([\"filename\", \"sentence_idx\"]).apply(agg_func)\n",
    "#         self.grouped = self.dataset.groupby([\"sentence_idx\"]).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_train = SentenceGetter(df_train)\n",
    "getter_test = SentenceGetter(df_test)\n",
    "getter_val = SentenceGetter(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = getter_train.sentences\n",
    "sentences_test = getter_test.sentences\n",
    "sentences_val = getter_val.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30343\n",
      "15171.5\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_train))\n",
    "print(len(sentences_train)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_train = sentences_train[0:15171]  # parc: 22352]\n",
    "# print(len(sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 'O'), ('we', 'O'), ('last', 'O'), ('epistle', 'O'), ('from', 'O'), ('the', 'O'), ('bottomless', 'O'), ('pit', 'O'), (',', 'O'), ('we', 'SOURCE'), ('mock', 'CUE'), ('the', 'CONTENT'), ('presidential', 'CONTENT'), ('ambition', 'CONTENT'), ('of', 'CONTENT'), ('three', 'CONTENT'), ('republican', 'CONTENT'), ('hopeful', 'CONTENT'), (':', 'CONTENT'), ('Jeb', 'CONTENT'), ('!', 'CONTENT')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3022\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length train: 220\n",
      "Maximum sequence length test: 94\n",
      "Maximum sequence length val: 95\n"
     ]
    }
   ],
   "source": [
    "maxlen = max([len(s) for s in sentences_train])\n",
    "print ('Maximum sequence length train:', maxlen)\n",
    "maxlen_test = max([len(s) for s in sentences_test])\n",
    "print ('Maximum sequence length test:', maxlen_test)\n",
    "maxlen_val = max([len(s) for s in sentences_val])\n",
    "print ('Maximum sequence length val:', maxlen_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_train = ['', '[UNK]']\n",
    "# words_train.extend(list(set(df_train[\"word\"].values)))\n",
    "# words_train.append(\"ENDPAD\")\n",
    "# words_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in we last epistle from the bottomless pit , we mock the presidential ambition of three republican hopeful : Jeb !'"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_train = [' '.join([w[0] for w in s]) for s in sentences_train]\n",
    "sentences_texts_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Washington -lrb- AFP -rrb- -- the Washington Post on Thursday become the latest US newspaper to emphatically endorse Hillary Clinton for the White House , say it be sway as much by she competence as by the alarming specter of a Donald Trump presidency .'"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_test = [' '.join([w[0] for w in s]) for s in sentences_test]\n",
    "sentences_texts_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the New York Post report that Hillary Clinton 's team avoid take she to the emergency room follow she medical scare on Sunday in order to `` keep the detail of she medical treatment under wrap . ''\""
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_val = [' '.join([w[0] for w in s]) for s in sentences_val]\n",
    "sentences_texts_val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/pretrained_word_embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=50000, output_sequence_length=maxlen)\n",
    "# vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = Dataset.from_tensor_slices(df_train[\"word\"].values).batch(128)\n",
    "text_ds = Dataset.from_tensor_slices(sentences_texts_train).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'be', 'to']"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "# output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 71, 15, 2]"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"be\", \"get\", \"on\", \"the\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2021-06-16 19:55:38,335 - glove2word2vec - INFO - running /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/gensim/scripts/glove2word2vec.py --input glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
      "2021-06-16 19:55:38,506 - glove2word2vec - INFO - converting 400000 vectors from glove.6B.50d.txt to glove.6B.50d.w2vformat.txt\n",
      "2021-06-16 19:55:38,913 - glove2word2vec - INFO - Converted model with 400000 vectors and 50 dimensions\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# word_vecs = KeyedVectors.load_word2vec_format(\"glove.txt\") \n",
    "# https://www.kaggle.com/watts2/glove6b50dtxt\n",
    "glove_dimensions = 50\n",
    "!python -m gensim.scripts.glove2word2vec --input  glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
    "word_vecs = KeyedVectors.load_word2vec_format(\"glove.6B.50d.w2vformat.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test word_vecs\n",
    "word_vecs.word_vec('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(word_vecs.word_vec('the')).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18285 words (3714 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = glove_dimensions\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "# for i, word in enumerate(words_train):\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     print(i)\n",
    "    try:\n",
    "        embedding_vector = word_vecs.word_vec(word)\n",
    "#     if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "#     else:\n",
    "    except KeyError:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_test = list(set(df_test[\"word\"].values))\n",
    "# words_test.append(\"ENDPAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = len(words_train)\n",
    "# n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONTENT', 'SOURCE', 'O', 'CUE']\n"
     ]
    }
   ],
   "source": [
    "from math import nan\n",
    "\n",
    "tags = []\n",
    "for index, tag in enumerate(set(df_train[\"tag\"].values)):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags.append('unk')\n",
    "    else:\n",
    "        tags.append(tag)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "# word2idx_train = {w: i for i, w in enumerate(words_train)}\n",
    "# word2idx_test = {w: i for i, w in enumerate(words_test)}\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx_train['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CONTENT': 0, 'SOURCE': 1, 'O': 2, 'CUE': 3}\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUE'"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'CONTENT', 1: 'SOURCE', 2: 'O', 3: 'CUE'}"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer(np.array([[s] for s in sentences_texts_train])).numpy()\n",
    "X_test = vectorizer(np.array([[s] for s in sentences_texts_test])).numpy()\n",
    "X_val = vectorizer(np.array([[s] for s in sentences_texts_val])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# X_train = [[word2idx_train[w[0]] for w in s] for s in sentences_train]\n",
    "# X_test = [[word2idx_test[w[0]] for w in s] for s in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1055     9   755    10    71     5   406 18235  7390    15   439   227\n",
      "   451     3     5  5392  3335    14     9    37 18916     7    86   101\n",
      "     9    70  4031     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30343, 220)"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\",value=n_words - 1)\n",
    "# print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\",value=n_words - 1)\n",
    "# print(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_train = [[tag2idx[w[1]] for w in s] for s in sentences_train]\n",
    "# print('sentences_train[25]')\n",
    "# print(sentences_train[25])\n",
    "# print('y_idx_train[25]')\n",
    "# print(y_idx_train[25])\n",
    "# print('sentences_train[10]')\n",
    "# print(sentences_train[10])\n",
    "# print('y_idx_train[10]')\n",
    "# print(y_idx_train[10])\n",
    "# print(len(y_idx_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_test = [[tag2idx[w[1]] for w in s] for s in sentences_test]\n",
    "# print('sentences_test[25]')\n",
    "# print(sentences_test[25])\n",
    "# print('y_idx_test[25]')\n",
    "# print(y_idx_test[25])\n",
    "# print('sentences_test[10]')\n",
    "# print(sentences_test[10])\n",
    "# print('y_idx_test[10]')\n",
    "# print(y_idx_test[10])\n",
    "# print(len(y_idx_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_val = [[tag2idx[w[1]] for w in s] for s in sentences_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "y_train = pad_sequences(maxlen=maxlen, sequences=y_idx_train, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_train[10])\n",
    "print(len(y_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "y_test = pad_sequences(maxlen=maxlen, sequences=y_idx_test, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_test[10])\n",
    "print(len(y_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "y_val = pad_sequences(maxlen=maxlen, sequences=y_idx_val, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_val[10])\n",
    "print(len(y_val[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
    "print(y_train[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n",
    "print(y_test[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_val = [to_categorical(i, num_classes=n_tags) for i in y_val]\n",
    "print(y_val[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# X_train, y_train = X, y\n",
    "# X_test, y_test = X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n",
      "2\n",
      "47\n",
      "[0. 0. 1. 0.]\n",
      "[0. 1. 0. 0.]\n",
      "[1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10][0])\n",
    "print(X_test[10][0])\n",
    "print(X_val[10][0])\n",
    "print(y_train[10][0])\n",
    "print(y_test[10][0])\n",
    "print(y_val[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential  # , Input\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import tensorflow.keras as k\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(k.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.utils as generic_utils\n",
    "# from keras_contrib.layers.crf import CRF\n",
    "# AttributeError: module 'tensorflow.compat.v2' has no attribute '__internal__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_size = embedding_dim  # glove_dimensions  # 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xuxingya/tf2crf\n",
    "\n",
    "More inspiration:\n",
    "https://github.com/Akshayc1/named-entity-recognition/blob/master/NER%20using%20Bidirectional%20LSTM%20-%20CRF%20.ipynb\n",
    "https://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle example code \n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "inputs = Input(shape=(None,), dtype=\"int64\")  \n",
    "# inputs = Input(shape=(maxlen,))\n",
    "# https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/ --> pretrained embeddings\n",
    "# outputs = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(inputs)\n",
    "outputs = Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,)(inputs)\n",
    "outputs = Bidirectional(LSTM(units=word_embedding_size, \n",
    "                             return_sequences=True, \n",
    "                             dropout=0.5,  # 0.2, 0.5, \n",
    "                             recurrent_dropout=0.5,  # 0.2, 0.5, \n",
    "                             kernel_initializer=k.initializers.he_normal()))(outputs)\n",
    "# https://github.com/xuxingya/tf2crf: Add internal kernel like CRF in keras_contrib, so now there is no need to stack a Dense layer before the CRF layer.\n",
    "# outputs = Dense(n_tags, activation=\"relu\")(outputs)  # previously softmax output layer\n",
    "outputs = TimeDistributed(Dense(n_tags, activation=\"relu\"))(outputs)  # previously softmax output layer\n",
    "# outputs = TimeDistributed(Dense(n_tags, activation=tensorflow.keras.activations.softmax))(outputs)  # previously softmax output layer\n",
    "\n",
    "# crf = CRF(n_tags)  # CRF layer\n",
    "# out = crf(outputs)  # output\n",
    "# model = Model(input, out)\n",
    "\n",
    "# adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle version (removed CRF code)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# https://stackoverflow.com/questions/61742556/valueerror-shapes-none-1-and-none-2-are-incompatible\n",
    "# model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # categorical_accuracy?\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "# model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])# Saving the best only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the best only\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
    "filepath = 'tmp/checkpoint'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfcrf version\n",
    "# model.fit(x=X_train, y=np.array(y_train), epochs=1, batch_size=2)\n",
    "# model.save('tests/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate generalization metrics\n",
    "# i = len(X_test) - 1 \n",
    "# score = model.evaluate(np.array([X_test[:i]]), y_test, verbose=0)\n",
    "# print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 4/95 [>.............................] - ETA: 50s - loss: nan - accuracy: 0.0852"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-815-79a690856ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model.fit(X_train, np.array(y_train), batch_size=256, epochs=1, validation_split=0.1, verbose=1, callbacks=callbacks_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# validation_data=(X_val, y_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, np.array(y_train), batch_size=256, epochs=1, validation_split=0.1, verbose=1, callbacks=callbacks_list)\n",
    "# model.fit(X_train, np.array(y_train), batch_size=256, epochs=50, validation_split=0.2, verbose=1, callbacks=callbacks_list)\n",
    "model.fit(X_train, np.array(y_train), batch_size=256, epochs=50, validation_data=(X_val, y_val), verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = {}\n",
    "TN = {}\n",
    "FP = {}\n",
    "FN = {}\n",
    "for tag in tag2idx.keys():\n",
    "    TP[tag] = 0\n",
    "    TN[tag] = 0    \n",
    "    FP[tag] = 0    \n",
    "    FN[tag] = 0    \n",
    "\n",
    "def accumulate_score_by_tag(gt, pred):\n",
    "    \"\"\"\n",
    "    For each tag keep stats\n",
    "    \"\"\"\n",
    "    if gt == pred:\n",
    "        TP[gt] += 1\n",
    "    elif gt != 'O' and pred == 'O':\n",
    "        FN[gt] +=1\n",
    "    elif gt == 'O' and pred != 'O':\n",
    "        FP[gt] += 1\n",
    "    else:\n",
    "        TN[gt] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2  # len(X_test) - 1  # Last one \n",
    "# p = model.predict(np.array([X_test[i]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "# print(p.shape)\n",
    "# gt = np.argmax(y_test[i], axis=-1)\n",
    "# print(gt)\n",
    "# print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(p)\n",
    "# for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
    "#     if words_test[w] == 'ENDPAD':\n",
    "#         break\n",
    "#     print(\"{:14}: ({:5}): {}\".format(words_test[w],idx2tag[gt[idx]],tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(p, axis=axis)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=axis).ravel(), np.argmax(p, axis=axis).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(X_test):\n",
    "    y_hat = np.argmax(p[i], axis=-1)\n",
    "    gt = np.argmax(y_test[i], axis=-1)\n",
    "    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
    "        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in tag2idx.keys():\n",
    "    print(f'tag:{tag}')    \n",
    "    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
    "    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_py37_venv",
   "language": "python",
   "name": "jupyter_py37_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
