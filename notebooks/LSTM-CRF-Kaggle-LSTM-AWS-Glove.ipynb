{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "https://www.kaggle.com/williamroe/bi-lstm-with-crf-for-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.4.0  # 2.5.0  # 2.2.0\n",
    "%pip install tensorflow==2.5.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow_addons==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install keras-crf==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow-cpu==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "# # %pip install keras-contrib==0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/xuxingya/tf2crf\n",
    "# %pip install tf2crf==0.1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env SM_FRAMEWORK=tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall --yes tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import s3fs\n",
    "\n",
    "what_corpus = 'polnear'\n",
    "what_type_files = 'train'\n",
    "what_type_test_file = 'test'\n",
    "what_type_val_file = 'dev'\n",
    "\n",
    "# filepath = 's3://sagemaker-studio-528576943967-ssf9zkrg3os/polnear-conll/prepared/'\n",
    "filepath = '../' + what_corpus + '-conll/prepared/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = what_corpus + '_preprocessed_' + what_type_files + '_noBIO.csv'\n",
    "df_train = pd.read_csv(filepath + train_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cuban</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>would</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>consider</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>future</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>House</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>bid</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Cuban</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>billionaire</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>owner</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>politico_2016-05-22_mark-cuban-i-d-consider-a-...</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "1   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "2   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "3   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "4   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "5   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "6   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "7   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "8   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "9   politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "10  politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "11  politico_2016-05-22_mark-cuban-i-d-consider-a-...             1   \n",
       "12  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "13  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "14  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "15  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "16  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "17  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "18  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "19  politico_2016-05-22_mark-cuban-i-d-consider-a-...             2   \n",
       "\n",
       "           word      tag  \n",
       "0          Mark   SOURCE  \n",
       "1         Cuban   SOURCE  \n",
       "2             :      CUE  \n",
       "3             I  CONTENT  \n",
       "4         would  CONTENT  \n",
       "5      consider  CONTENT  \n",
       "6             a  CONTENT  \n",
       "7        future  CONTENT  \n",
       "8         White  CONTENT  \n",
       "9         House  CONTENT  \n",
       "10          bid  CONTENT  \n",
       "11            .        O  \n",
       "12         Mark   SOURCE  \n",
       "13        Cuban   SOURCE  \n",
       "14            ,   SOURCE  \n",
       "15          the   SOURCE  \n",
       "16  billionaire   SOURCE  \n",
       "17        owner   SOURCE  \n",
       "18           of   SOURCE  \n",
       "19          the   SOURCE  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train.index))\n",
    "del df_train['Unnamed: 0']\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = what_corpus + '_preprocessed_' + what_type_test_file + '_noBIO.csv'\n",
    "df_test = pd.read_csv(filepath + test_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73370\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>drops</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Into</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>get</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>black</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>turnout</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>--</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>increasingly</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "1   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "2   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "3   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "4   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "5   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "6   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "7   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "8   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "9   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "10  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "11  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "12  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "13  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "14  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "15  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "16  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "17  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "18  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "19  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "\n",
       "            word      tag  \n",
       "0        Hillary        O  \n",
       "1        Clinton        O  \n",
       "2          drops        O  \n",
       "3           Into        O  \n",
       "4        Detroit        O  \n",
       "5             as        O  \n",
       "6      Democrats   SOURCE  \n",
       "7            get      CUE  \n",
       "8        nervous      CUE  \n",
       "9          about      CUE  \n",
       "10         black  CONTENT  \n",
       "11       turnout  CONTENT  \n",
       "12             .        O  \n",
       "13       DETROIT        O  \n",
       "14            --        O  \n",
       "15          with        O  \n",
       "16     Democrats   SOURCE  \n",
       "17  increasingly      CUE  \n",
       "18       nervous      CUE  \n",
       "19         about      CUE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_test.index))\n",
    "del df_test['Unnamed: 0']\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_filename = what_corpus + '_preprocessed_' + what_type_val_file + '_noBIO.csv'\n",
    "df_val = pd.read_csv(filepath + val_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gold</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Star</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>mom</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Corners</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Obama</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>refusal</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>use</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>word</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>`</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>islamic</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>'</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>President</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Barack</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Obama</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "1   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "2   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "3   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "4   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "5   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "6   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "7   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "8   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "9   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "10  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "11  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "12  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "13  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "14  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "15  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "16  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "17  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "18  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "19  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "\n",
       "         word      tag  \n",
       "0        Gold        O  \n",
       "1        Star        O  \n",
       "2         mom        O  \n",
       "3     Corners        O  \n",
       "4       Obama        O  \n",
       "5          on        O  \n",
       "6          he   SOURCE  \n",
       "7     refusal      CUE  \n",
       "8          to  CONTENT  \n",
       "9         use  CONTENT  \n",
       "10        the  CONTENT  \n",
       "11       word  CONTENT  \n",
       "12          `  CONTENT  \n",
       "13    islamic  CONTENT  \n",
       "14  terrorist  CONTENT  \n",
       "15          .  CONTENT  \n",
       "16          '  CONTENT  \n",
       "17  President   SOURCE  \n",
       "18     Barack   SOURCE  \n",
       "19      Obama   SOURCE  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_val.index))\n",
    "del df_val['Unnamed: 0']\n",
    "df_val.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "#         agg_func = lambda s: ' '.join(s[\"word\"].values.tolist())\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby([\"filename\", \"sentence_idx\"]).apply(agg_func)\n",
    "#         self.grouped = self.dataset.groupby([\"sentence_idx\"]).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_train = SentenceGetter(df_train)\n",
    "getter_test = SentenceGetter(df_test)\n",
    "getter_val = SentenceGetter(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = getter_train.sentences\n",
    "sentences_test = getter_test.sentences\n",
    "sentences_val = getter_val.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30343\n",
      "15171.5\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_train))\n",
    "print(len(sentences_train)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_train = sentences_train[0:15171]  # parc: 22352]\n",
    "# print(len(sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 'O'), ('we', 'O'), ('last', 'O'), ('epistle', 'O'), ('from', 'O'), ('the', 'O'), ('bottomless', 'O'), ('pit', 'O'), (',', 'O'), ('we', 'SOURCE'), ('mock', 'CUE'), ('the', 'CONTENT'), ('presidential', 'CONTENT'), ('ambition', 'CONTENT'), ('of', 'CONTENT'), ('three', 'CONTENT'), ('republican', 'CONTENT'), ('hopeful', 'CONTENT'), (':', 'CONTENT'), ('Jeb', 'CONTENT'), ('!', 'CONTENT')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3022\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length train: 220\n",
      "Maximum sequence length test: 94\n",
      "Maximum sequence length val: 95\n"
     ]
    }
   ],
   "source": [
    "maxlen = max([len(s) for s in sentences_train])\n",
    "print ('Maximum sequence length train:', maxlen)\n",
    "maxlen_test = max([len(s) for s in sentences_test])\n",
    "print ('Maximum sequence length test:', maxlen_test)\n",
    "maxlen_val = max([len(s) for s in sentences_val])\n",
    "print ('Maximum sequence length val:', maxlen_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_train = ['', '[UNK]']\n",
    "# words_train.extend(list(set(df_train[\"word\"].values)))\n",
    "# words_train.append(\"ENDPAD\")\n",
    "# words_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in we last epistle from the bottomless pit , we mock the presidential ambition of three republican hopeful : Jeb !'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_train = [' '.join([w[0] for w in s]) for s in sentences_train]\n",
    "sentences_texts_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Washington -lrb- AFP -rrb- -- the Washington Post on Thursday become the latest US newspaper to emphatically endorse Hillary Clinton for the White House , say it be sway as much by she competence as by the alarming specter of a Donald Trump presidency .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_test = [' '.join([w[0] for w in s]) for s in sentences_test]\n",
    "sentences_texts_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the New York Post report that Hillary Clinton 's team avoid take she to the emergency room follow she medical scare on Sunday in order to `` keep the detail of she medical treatment under wrap . ''\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_val = [' '.join([w[0] for w in s]) for s in sentences_val]\n",
    "sentences_texts_val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/pretrained_word_embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=50000, output_sequence_length=maxlen)\n",
    "# vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = Dataset.from_tensor_slices(df_train[\"word\"].values).batch(128)\n",
    "text_ds = Dataset.from_tensor_slices(sentences_texts_train).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'be', 'to']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "# output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 71, 15, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"be\", \"get\", \"on\", \"the\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2021-06-16 23:03:03,695 - glove2word2vec - INFO - running /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/gensim/scripts/glove2word2vec.py --input glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
      "2021-06-16 23:03:03,835 - glove2word2vec - INFO - converting 400000 vectors from glove.6B.50d.txt to glove.6B.50d.w2vformat.txt\n",
      "2021-06-16 23:03:04,251 - glove2word2vec - INFO - Converted model with 400000 vectors and 50 dimensions\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# word_vecs = KeyedVectors.load_word2vec_format(\"glove.txt\") \n",
    "# https://www.kaggle.com/watts2/glove6b50dtxt\n",
    "glove_dimensions = 50\n",
    "!python -m gensim.scripts.glove2word2vec --input  glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
    "word_vecs = KeyedVectors.load_word2vec_format(\"glove.6B.50d.w2vformat.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test word_vecs\n",
    "word_vecs.word_vec('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(word_vecs.word_vec('the')).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18285 words (3714 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = glove_dimensions\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "# for i, word in enumerate(words_train):\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     print(i)\n",
    "    try:\n",
    "        embedding_vector = word_vecs.word_vec(word)\n",
    "#     if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "#     else:\n",
    "    except KeyError:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_test = list(set(df_test[\"word\"].values))\n",
    "# words_test.append(\"ENDPAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = len(words_train)\n",
    "# n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUE', 'SOURCE', 'O', 'CONTENT']\n"
     ]
    }
   ],
   "source": [
    "from math import nan\n",
    "\n",
    "tags = []\n",
    "for index, tag in enumerate(set(df_train[\"tag\"].values)):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags.append('unk')\n",
    "    else:\n",
    "        tags.append(tag)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "# word2idx_train = {w: i for i, w in enumerate(words_train)}\n",
    "# word2idx_test = {w: i for i, w in enumerate(words_test)}\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx_train['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CUE': 0, 'SOURCE': 1, 'O': 2, 'CONTENT': 3}\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONTENT'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'CUE', 1: 'SOURCE', 2: 'O', 3: 'CONTENT'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer(np.array([[s] for s in sentences_texts_train])).numpy()\n",
    "X_test = vectorizer(np.array([[s] for s in sentences_texts_test])).numpy()\n",
    "X_val = vectorizer(np.array([[s] for s in sentences_texts_val])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = [[word2idx_train[w[0]] for w in s] for s in sentences_train]\n",
    "# X_test = [[word2idx_test[w[0]] for w in s] for s in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1055     9   755    10    71     5   406 18235  7390    15   439   227\n",
      "   451     3     5  5392  3335    14     9    37 18916     7    86   101\n",
      "     9    70  4031     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30343, 220)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\",value=n_words - 1)\n",
    "# print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\",value=n_words - 1)\n",
    "# print(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_train = [[tag2idx[w[1]] for w in s] for s in sentences_train]\n",
    "# print('sentences_train[25]')\n",
    "# print(sentences_train[25])\n",
    "# print('y_idx_train[25]')\n",
    "# print(y_idx_train[25])\n",
    "# print('sentences_train[10]')\n",
    "# print(sentences_train[10])\n",
    "# print('y_idx_train[10]')\n",
    "# print(y_idx_train[10])\n",
    "# print(len(y_idx_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_test = [[tag2idx[w[1]] for w in s] for s in sentences_test]\n",
    "# print('sentences_test[25]')\n",
    "# print(sentences_test[25])\n",
    "# print('y_idx_test[25]')\n",
    "# print(y_idx_test[25])\n",
    "# print('sentences_test[10]')\n",
    "# print(sentences_test[10])\n",
    "# print('y_idx_test[10]')\n",
    "# print(y_idx_test[10])\n",
    "# print(len(y_idx_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_val = [[tag2idx[w[1]] for w in s] for s in sentences_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "y_train = pad_sequences(maxlen=maxlen, sequences=y_idx_train, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_train[10])\n",
    "print(len(y_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "y_test = pad_sequences(maxlen=maxlen, sequences=y_idx_test, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_test[10])\n",
    "print(len(y_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "y_val = pad_sequences(maxlen=maxlen, sequences=y_idx_val, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_val[10])\n",
    "print(len(y_val[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
    "print(y_train[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n",
    "print(y_test[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_val = [to_categorical(i, num_classes=n_tags) for i in y_val]\n",
    "print(y_val[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# X_train, y_train = X, y\n",
    "# X_test, y_test = X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n",
      "2\n",
      "47\n",
      "[0. 0. 1. 0.]\n",
      "[0. 1. 0. 0.]\n",
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10][0])\n",
    "print(X_test[10][0])\n",
    "print(X_val[10][0])\n",
    "print(y_train[10][0])\n",
    "print(y_test[10][0])\n",
    "print(y_val[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Input, InputLayer, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Model, Sequential  # , Input\n",
    "import tensorflow.keras as k\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(k.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.utils as generic_utils\n",
    "# from keras_contrib.layers.crf import CRF\n",
    "# AttributeError: module 'tensorflow.compat.v2' has no attribute '__internal__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_size = embedding_dim  # glove_dimensions  # 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xuxingya/tf2crf\n",
    "\n",
    "More inspiration:\n",
    "https://github.com/Akshayc1/named-entity-recognition/blob/master/NER%20using%20Bidirectional%20LSTM%20-%20CRF%20.ipynb\n",
    "https://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kaggle example code \n",
    "\n",
    "# inputs = Input(shape=(None,), dtype=\"int64\")  \n",
    "# # inputs = Input(shape=(maxlen,))\n",
    "# # https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n",
    "# # https://keras.io/examples/nlp/pretrained_word_embeddings/ --> pretrained embeddings\n",
    "# # outputs = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(inputs)\n",
    "# outputs = Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,)(inputs)\n",
    "# outputs = Bidirectional(LSTM(units=word_embedding_size, \n",
    "#                              return_sequences=True, \n",
    "#                              dropout=0.5,  # 0.2, 0.5, \n",
    "#                              recurrent_dropout=0.5,  # 0.2, 0.5, \n",
    "#                              kernel_initializer=k.initializers.he_normal()))(outputs)\n",
    "# # https://github.com/xuxingya/tf2crf: Add internal kernel like CRF in keras_contrib, so now there is no need to stack a Dense layer before the CRF layer.\n",
    "# # outputs = Dense(n_tags, activation=\"relu\")(outputs)  # previously softmax output layer\n",
    "# outputs = TimeDistributed(Dense(n_tags, activation=\"relu\"))(outputs)  # previously softmax output layer\n",
    "# # outputs = TimeDistributed(Dense(n_tags, activation=tensorflow.keras.activations.softmax))(outputs)  # previously softmax output layer\n",
    "\n",
    "# # crf = CRF(n_tags)  # CRF layer\n",
    "# # out = crf(outputs)  # output\n",
    "# # model = Model(input, out)\n",
    "\n",
    "# # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "# adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "\n",
    "# # https://stackoverflow.com/questions/61742556/valueerror-shapes-none-1-and-none-2-are-incompatible\n",
    "# # model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # categorical_accuracy?\n",
    "# # model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "# # model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])# Saving the best only\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          1100050   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 100)         40400     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 4)           404       \n",
      "_________________________________________________________________\n",
      "crf (CRF)                    (None, None, 4)           16        \n",
      "=================================================================\n",
      "Total params: 1,140,870\n",
      "Trainable params: 40,820\n",
      "Non-trainable params: 1,100,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/tensorflow/addons/issues/1769\n",
    "from crf import CRF\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(None,), dtype=\"int64\"))\n",
    "model.add(Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,))\n",
    "model.add(Bidirectional(LSTM(units=word_embedding_size, \n",
    "                             return_sequences=True, \n",
    "                             dropout=0.5,  # 0.2, 0.5, \n",
    "                             recurrent_dropout=0.5,  # 0.2, 0.5, \n",
    "                             kernel_initializer=k.initializers.he_normal())))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "crf = CRF(n_tags, sparse_target=False)\n",
    "model.add(crf)\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])  # categorical_accuracy?\n",
    "# model.compile(optimizer=adam, loss=crf.loss, metrics=[crf.accuracy])\n",
    "model.summary()\n",
    "\n",
    "# /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/text/crf.py:540: \n",
    "# UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the best only\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
    "filepath = 'tmp/checkpoint'\n",
    "checkpoint_acc = ModelCheckpoint(filepath + '/acc', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint_loss = ModelCheckpoint(filepath + '/loss', monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint_acc, checkpoint_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate generalization metrics\n",
    "# i = len(X_test) - 1 \n",
    "# score = model.evaluate(np.array([X_test[:i]]), y_test, verbose=0)\n",
    "# print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['crf/transitions:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['crf/transitions:0'] when minimizing the loss.\n",
      "95/95 [==============================] - 54s 535ms/step - loss: nan - accuracy: 0.0072 - val_loss: 16.0239 - val_accuracy: 0.0058\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00585, saving model to tmp/checkpoint/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/text/crf.py:545: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  warnings.warn(\n",
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/text/crf.py:545: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp/checkpoint/acc/assets\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 16.02386, saving model to tmp/checkpoint/loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/text/crf.py:545: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  warnings.warn(\n",
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow_addons/text/crf.py:545: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp/checkpoint/loss/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x192859580>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X_train, np.array(y_train), batch_size=256, epochs=1, validation_split=0.1, verbose=1, callbacks=callbacks_list)\n",
    "model.fit(X_train, np.array(y_train), batch_size=256, epochs=1, validation_split=0.2, verbose=1, callbacks=callbacks_list)\n",
    "# model.fit(X_train, np.array(y_train), batch_size=256, epochs=50, validation_data=(X_val, y_val), verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          1100050   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 100)         40400     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 4)           404       \n",
      "_________________________________________________________________\n",
      "crf (CRF)                    (None, None, 4)           16        \n",
      "=================================================================\n",
      "Total params: 1,140,870\n",
      "Trainable params: 40,820\n",
      "Non-trainable params: 1,100,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = {}\n",
    "TN = {}\n",
    "FP = {}\n",
    "FN = {}\n",
    "for tag in tag2idx.keys():\n",
    "    TP[tag] = 0\n",
    "    TN[tag] = 0    \n",
    "    FP[tag] = 0    \n",
    "    FN[tag] = 0    \n",
    "\n",
    "def accumulate_score_by_tag(gt, pred):\n",
    "    \"\"\"\n",
    "    For each tag keep stats\n",
    "    \"\"\"\n",
    "    if gt == pred:\n",
    "        TP[gt] += 1\n",
    "    elif gt != 'O' and pred == 'O':\n",
    "        FN[gt] +=1\n",
    "    elif gt == 'O' and pred != 'O':\n",
    "        FP[gt] += 1\n",
    "    else:\n",
    "        TN[gt] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 195  264  132 ...    0    0    0]\n",
      " [ 195   84 3831 ...    0    0    0]\n",
      " [  67   21   11 ...    0    0    0]\n",
      " ...\n",
      " [  20    3  848 ...    0    0    0]\n",
      " [  27   36  212 ...    0    0    0]\n",
      " [   7   24  177 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2  # len(X_test) - 1  # Last one \n",
    "# p = model.predict(np.array([X_test[i]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "# print(p.shape)\n",
    "# gt = np.argmax(y_test[i], axis=-1)\n",
    "# print(gt)\n",
    "# print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(p)\n",
    "# for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
    "#     if words_test[w] == 'ENDPAD':\n",
    "#         break\n",
    "#     print(\"{:14}: ({:5}): {}\".format(words_test[w],idx2tag[gt[idx]],tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[100][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(p, axis=axis)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=axis).ravel(), np.argmax(p, axis=axis).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(X_test):\n",
    "    y_hat = np.argmax(p[i], axis=-1)\n",
    "    gt = np.argmax(y_test[i], axis=-1)\n",
    "    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
    "        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in tag2idx.keys():\n",
    "    print(f'tag:{tag}')    \n",
    "    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
    "    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_py37_venv",
   "language": "python",
   "name": "jupyter_py37_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
