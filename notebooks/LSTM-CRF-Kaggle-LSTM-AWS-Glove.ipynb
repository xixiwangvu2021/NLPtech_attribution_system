{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "https://www.kaggle.com/williamroe/bi-lstm-with-crf-for-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.0\n",
      "  Downloading tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.7 MB 3.7 kB/s  eta 0:00:01     |███████████████▍                | 189.1 MB 42.9 MB/s eta 0:00:05\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (0.2.0)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 50.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (1.19.5)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 323 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (3.7.4.3)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 66.0 MB/s eta 0:00:01     |█████████▎                      | 1.7 MB 66.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (3.15.2)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.4.0) (1.15.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.31.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 60.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.25.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (49.6.0.post20210108)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 63.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.4.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=f632ec3457383e2a86daa59bacd01113d7fb7d3cc13a7a8ca8294daac519b2f4\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.3.3 google-auth-1.31.0 google-auth-oauthlib-0.4.4 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==2.4.0  # 2.5.0  # 2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow_addons==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install keras-crf==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow-cpu==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "# # %pip install keras-contrib==0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/xuxingya/tf2crf\n",
    "# %pip install tf2crf==0.1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env SM_FRAMEWORK=tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall --yes tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.4.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (0.8.7)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.20.76)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from fsspec>=0.6.0->s3fs) (3.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->fsspec>=0.6.0->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->fsspec>=0.6.0->s3fs) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import s3fs\n",
    "\n",
    "what_corpus = 'polnear'\n",
    "what_type_files = 'dev'\n",
    "what_type_test_file = 'test'\n",
    "\n",
    "# filepath = 's3://sagemaker-studio-528576943967-ssf9zkrg3os/polnear-conll/prepared/'\n",
    "filepath = '../' + what_corpus + '-conll/prepared/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = what_corpus + '_preprocessed_' + what_type_files + '_noBIO.csv'\n",
    "df_train = pd.read_csv(filepath + train_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gold</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Star</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>mom</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Corners</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Obama</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>refusal</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>use</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>word</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>`</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>islamic</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>1</td>\n",
       "      <td>'</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>President</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Barack</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west-journal_2016-09-29_gold-star-mom-corners-...</td>\n",
       "      <td>2</td>\n",
       "      <td>Obama</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "1   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "2   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "3   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "4   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "5   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "6   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "7   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "8   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "9   west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "10  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "11  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "12  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "13  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "14  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "15  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "16  west-journal_2016-09-29_gold-star-mom-corners-...             1   \n",
       "17  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "18  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "19  west-journal_2016-09-29_gold-star-mom-corners-...             2   \n",
       "\n",
       "         word      tag  \n",
       "0        Gold        O  \n",
       "1        Star        O  \n",
       "2         mom        O  \n",
       "3     Corners        O  \n",
       "4       Obama        O  \n",
       "5          on        O  \n",
       "6          he   SOURCE  \n",
       "7     refusal      CUE  \n",
       "8          to  CONTENT  \n",
       "9         use  CONTENT  \n",
       "10        the  CONTENT  \n",
       "11       word  CONTENT  \n",
       "12          `  CONTENT  \n",
       "13    islamic  CONTENT  \n",
       "14  terrorist  CONTENT  \n",
       "15          .  CONTENT  \n",
       "16          '  CONTENT  \n",
       "17  President   SOURCE  \n",
       "18     Barack   SOURCE  \n",
       "19      Obama   SOURCE  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_train.index))\n",
    "del df_train['Unnamed: 0']\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = what_corpus + '_preprocessed_' + what_type_test_file + '_noBIO.csv'\n",
    "df_test = pd.read_csv(filepath + test_filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73370\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>drops</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Into</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>get</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>black</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>turnout</td>\n",
       "      <td>CONTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>DETROIT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>--</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>Democrats</td>\n",
       "      <td>SOURCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>increasingly</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>nervous</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>huff-post_2016-11-05_hillary-clinton-drops-int...</td>\n",
       "      <td>2</td>\n",
       "      <td>about</td>\n",
       "      <td>CUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  sentence_idx  \\\n",
       "0   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "1   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "2   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "3   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "4   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "5   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "6   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "7   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "8   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "9   huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "10  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "11  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "12  huff-post_2016-11-05_hillary-clinton-drops-int...             1   \n",
       "13  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "14  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "15  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "16  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "17  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "18  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "19  huff-post_2016-11-05_hillary-clinton-drops-int...             2   \n",
       "\n",
       "            word      tag  \n",
       "0        Hillary        O  \n",
       "1        Clinton        O  \n",
       "2          drops        O  \n",
       "3           Into        O  \n",
       "4        Detroit        O  \n",
       "5             as        O  \n",
       "6      Democrats   SOURCE  \n",
       "7            get      CUE  \n",
       "8        nervous      CUE  \n",
       "9          about      CUE  \n",
       "10         black  CONTENT  \n",
       "11       turnout  CONTENT  \n",
       "12             .        O  \n",
       "13       DETROIT        O  \n",
       "14            --        O  \n",
       "15          with        O  \n",
       "16     Democrats   SOURCE  \n",
       "17  increasingly      CUE  \n",
       "18       nervous      CUE  \n",
       "19         about      CUE  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_test.index))\n",
    "del df_test['Unnamed: 0']\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "#         agg_func = lambda s: ' '.join(s[\"word\"].values.tolist())\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby([\"filename\", \"sentence_idx\"]).apply(agg_func)\n",
    "#         self.grouped = self.dataset.groupby([\"sentence_idx\"]).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_train = SentenceGetter(df_train)\n",
    "getter_test = SentenceGetter(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train= getter_train.sentences\n",
    "sentences_test = getter_test.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3308\n",
      "1654.0\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_train))\n",
    "print(len(sentences_train)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3308\n"
     ]
    }
   ],
   "source": [
    "sentences_train = sentences_train[0:15171]  # parc: 22352]\n",
    "print(len(sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'SOURCE'), ('New', 'SOURCE'), ('York', 'SOURCE'), ('Post', 'SOURCE'), ('report', 'CUE'), ('that', 'CONTENT'), ('Hillary', 'CONTENT'), ('Clinton', 'CONTENT'), (\"'s\", 'CONTENT'), ('team', 'CONTENT'), ('avoid', 'CONTENT'), ('take', 'CONTENT'), ('she', 'CONTENT'), ('to', 'CONTENT'), ('the', 'CONTENT'), ('emergency', 'CONTENT'), ('room', 'CONTENT'), ('follow', 'CONTENT'), ('she', 'CONTENT'), ('medical', 'CONTENT'), ('scare', 'CONTENT'), ('on', 'CONTENT'), ('Sunday', 'CONTENT'), ('in', 'CONTENT'), ('order', 'CONTENT'), ('to', 'CONTENT'), ('``', 'CONTENT'), ('keep', 'CONTENT'), ('the', 'CONTENT'), ('detail', 'CONTENT'), ('of', 'CONTENT'), ('she', 'CONTENT'), ('medical', 'CONTENT'), ('treatment', 'CONTENT'), ('under', 'CONTENT'), ('wrap', 'CONTENT'), ('.', 'CONTENT'), (\"''\", 'CONTENT')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3022\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 95\n"
     ]
    }
   ],
   "source": [
    "maxlen = max([len(s) for s in sentences_train])\n",
    "print ('Maximum sequence length:', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how long sentences are so that we can pad them\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(s) for s in sentences_train], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'inner', 'witch', 'prayer']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_train = ['', '[UNK]']\n",
    "words_train.extend(list(set(df_train[\"word\"].values)))\n",
    "words_train.append(\"ENDPAD\")\n",
    "words_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the New York Post report that Hillary Clinton 's team avoid take she to the emergency room follow she medical scare on Sunday in order to `` keep the detail of she medical treatment under wrap . ''\""
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_train = [' '.join([w[0] for w in s]) for s in sentences_train]\n",
    "sentences_texts_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Washington -lrb- AFP -rrb- -- the Washington Post on Thursday become the latest US newspaper to emphatically endorse Hillary Clinton for the White House , say it be sway as much by she competence as by the alarming specter of a Donald Trump presidency .'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_texts_test = [' '.join([w[0] for w in s]) for s in sentences_test]\n",
    "sentences_texts_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/pretrained_word_embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = Dataset.from_tensor_slices(df_train[\"word\"].values).batch(128)\n",
    "text_ds = Dataset.from_tensor_slices(sentences_texts_train).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'be', 'to']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "# output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 79, 16, 2]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"be\", \"get\", \"on\", \"the\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2021-06-16 17:11:14,525 - glove2word2vec - INFO - running /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/gensim/scripts/glove2word2vec.py --input glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
      "2021-06-16 17:11:14,671 - glove2word2vec - INFO - converting 400000 vectors from glove.6B.50d.txt to glove.6B.50d.w2vformat.txt\n",
      "2021-06-16 17:11:15,085 - glove2word2vec - INFO - Converted model with 400000 vectors and 50 dimensions\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# word_vecs = KeyedVectors.load_word2vec_format(\"glove.txt\") \n",
    "# https://www.kaggle.com/watts2/glove6b50dtxt\n",
    "glove_dimensions = 50\n",
    "!python -m gensim.scripts.glove2word2vec --input  glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt\n",
    "word_vecs = KeyedVectors.load_word2vec_format(\"glove.6B.50d.w2vformat.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs.word_vec('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(word_vecs.word_vec('the')).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 6464 words (459 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = glove_dimensions\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "# for i, word in enumerate(words_train):\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     print(i)\n",
    "    try:\n",
    "        embedding_vector = word_vecs.word_vec(word)\n",
    "#     if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "#     else:\n",
    "    except KeyError:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test = list(set(df_test[\"word\"].values))\n",
    "words_test.append(\"ENDPAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7441"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = len(words_train)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, sentence_idx, word, tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONTENT', 'SOURCE', 'O', 'CUE']\n"
     ]
    }
   ],
   "source": [
    "from math import nan\n",
    "\n",
    "tags = []\n",
    "for index, tag in enumerate(set(df_train[\"tag\"].values)):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags.append('unk')\n",
    "    else:\n",
    "        tags.append(tag)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "word2idx_train = {w: i for i, w in enumerate(words_train)}\n",
    "word2idx_test = {w: i for i, w in enumerate(words_test)}\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2218"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_train['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CONTENT': 0, 'SOURCE': 1, 'O': 2, 'CUE': 3}\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUE'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'CONTENT', 1: 'SOURCE', 2: 'O', 3: 'CUE'}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer(np.array([[s] for s in sentences_texts_train])).numpy()\n",
    "X_test = vectorizer(np.array([[s] for s in sentences_texts_test])).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# X_train = [[word2idx_train[w[0]] for w in s] for s in sentences_train]\n",
    "# X_test = [[word2idx_test[w[0]] for w in s] for s in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  49    6    2  195   32    3 2728   37    3 1026   88  144    2  222\n",
      "  101   11  737   29  270   40  379    4    3  737   18   12 1436    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3308, 200)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\",value=n_words - 1)\n",
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\",value=n_words - 1)\n",
    "print(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_train = [[tag2idx[w[1]] for w in s] for s in sentences_train]\n",
    "# print('sentences_train[25]')\n",
    "# print(sentences_train[25])\n",
    "# print('y_idx_train[25]')\n",
    "# print(y_idx_train[25])\n",
    "# print('sentences_train[10]')\n",
    "# print(sentences_train[10])\n",
    "# print('y_idx_train[10]')\n",
    "# print(y_idx_train[10])\n",
    "# print(len(y_idx_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_test = [[tag2idx[w[1]] for w in s] for s in sentences_test]\n",
    "# print('sentences_test[25]')\n",
    "# print(sentences_test[25])\n",
    "# print('y_idx_test[25]')\n",
    "# print(y_idx_test[25])\n",
    "# print('sentences_test[10]')\n",
    "# print(sentences_test[10])\n",
    "# print('y_idx_test[10]')\n",
    "# print(y_idx_test[10])\n",
    "# print(len(y_idx_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "y_train = pad_sequences(maxlen=maxlen, sequences=y_idx_train, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_train[10])\n",
    "print(len(y_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "y_test = pad_sequences(maxlen=maxlen, sequences=y_idx_test, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print(y_test[10])\n",
    "print(len(y_test[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
    "print(y_train[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n",
    "print(y_test[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# X_train, y_train = X, y\n",
    "# X_test, y_test = X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "[1. 0. 0. 0.]\n",
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[10][0])\n",
    "print(X_test[10][0])\n",
    "print(y_train[10][0])\n",
    "print(y_test[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential  # , Input\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import tensorflow.keras as k\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(k.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.utils as generic_utils\n",
    "# from keras_contrib.layers.crf import CRF\n",
    "# AttributeError: module 'tensorflow.compat.v2' has no attribute '__internal__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_size = embedding_dim  # glove_dimensions  # 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xuxingya/tf2crf\n",
    "\n",
    "More inspiration:\n",
    "https://github.com/Akshayc1/named-entity-recognition/blob/master/NER%20using%20Bidirectional%20LSTM%20-%20CRF%20.ipynb\n",
    "https://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle example code \n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# inputs = Input(shape=(None,), dtype=\"int64\")  \n",
    "inputs = Input(shape=(maxlen,))\n",
    "# https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/ --> pretrained embeddings\n",
    "# outputs = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(inputs)\n",
    "outputs = Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False,)(inputs)\n",
    "outputs = Bidirectional(LSTM(units=word_embedding_size, \n",
    "                             return_sequences=True, \n",
    "                             dropout=0.2,  #0.5, \n",
    "                             recurrent_dropout=0.2,  # 0.5, \n",
    "                             kernel_initializer=k.initializers.he_normal()))(outputs)\n",
    "# https://github.com/xuxingya/tf2crf: Add internal kernel like CRF in keras_contrib, so now there is no need to stack a Dense layer before the CRF layer.\n",
    "# outputs = Dense(n_tags, activation=\"relu\")(outputs)  # previously softmax output layer\n",
    "outputs = TimeDistributed(Dense(n_tags, activation=\"relu\"))(outputs)  # previously softmax output layer\n",
    "# outputs = TimeDistributed(Dense(n_tags, activation=tensorflow.keras.activations.softmax))(outputs)  # previously softmax output layer\n",
    "\n",
    "# crf = CRF(n_tags)  # CRF layer\n",
    "# out = crf(outputs)  # output\n",
    "# model = Model(input, out)\n",
    "\n",
    "# adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "adam = k.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle version (removed CRF code)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# https://stackoverflow.com/questions/61742556/valueerror-shapes-none-1-and-none-2-are-incompatible\n",
    "# model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "# model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])# Saving the best only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the best only\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
    "filepath = 'tmp/checkpoint'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfcrf version\n",
    "# model.fit(x=X_train, y=np.array(y_train), epochs=1, batch_size=2)\n",
    "# model.save('tests/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate generalization metrics\n",
    "# i = len(X_test) - 1 \n",
    "# score = model.evaluate(np.array([X_test[:i]]), y_test, verbose=0)\n",
    "# print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "94/94 [==============================] - 11s 84ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.17465, saving model to tmp/checkpoint\n",
      "INFO:tensorflow:Assets written to: tmp/checkpoint/assets\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - 8s 80ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.17465\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - 8s 82ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.17465\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - 8s 87ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.17465\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - 8s 88ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.17465\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - 9s 94ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.17465\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - 9s 95ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.17465\n",
      "Epoch 8/20\n",
      "94/94 [==============================] - 8s 89ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.17465\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - 8s 90ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.17465\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - 8s 88ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.17465\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - 8s 88ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.17465\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - 9s 93ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.17465\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - 8s 88ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.17465\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - 8s 86ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.17465\n",
      "Epoch 15/20\n",
      "94/94 [==============================] - 8s 86ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.17465\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - 8s 85ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.17465\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - 9s 91ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.17465\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - 8s 87ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.17465\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - 9s 90ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.17465\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - 8s 89ms/step - loss: nan - accuracy: 0.1188 - val_loss: nan - val_accuracy: 0.1747\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.17465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1891f41c0>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X_train, np.array(y_train), batch_size=256, epochs=1, validation_split=0.1, verbose=1, callbacks=callbacks_list)\n",
    "model.fit(X_train, np.array(y_train), batch_size=32, epochs=20, validation_split=0.1, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 95)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 95, 50)            346250    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 95, 100)           40400     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 95, 4)             404       \n",
      "=================================================================\n",
      "Total params: 387,054\n",
      "Trainable params: 40,804\n",
      "Non-trainable params: 346,250\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = {}\n",
    "TN = {}\n",
    "FP = {}\n",
    "FN = {}\n",
    "for tag in tag2idx.keys():\n",
    "    TP[tag] = 0\n",
    "    TN[tag] = 0    \n",
    "    FP[tag] = 0    \n",
    "    FN[tag] = 0    \n",
    "\n",
    "def accumulate_score_by_tag(gt, pred):\n",
    "    \"\"\"\n",
    "    For each tag keep stats\n",
    "    \"\"\"\n",
    "    if gt == pred:\n",
    "        TP[gt] += 1\n",
    "    elif gt != 'O' and pred == 'O':\n",
    "        FN[gt] +=1\n",
    "    elif gt == 'O' and pred != 'O':\n",
    "        FP[gt] += 1\n",
    "    else:\n",
    "        TN[gt] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2  # len(X_test) - 1  # Last one \n",
    "# p = model.predict(np.array([X_test[i]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "# print(p.shape)\n",
    "# gt = np.argmax(y_test[i], axis=-1)\n",
    "# print(gt)\n",
    "# print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(p)\n",
    "# for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
    "#     if words_test[w] == 'ENDPAD':\n",
    "#         break\n",
    "#     print(\"{:14}: ({:5}): {}\".format(words_test[w],idx2tag[gt[idx]],tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3022, 95, 4)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p, axis=axis)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     CONTENT       0.13      1.00      0.23     36620\n",
      "      SOURCE       0.00      0.00      0.00      5695\n",
      "           O       0.00      0.00      0.00    241018\n",
      "         CUE       0.00      0.00      0.00      3757\n",
      "\n",
      "    accuracy                           0.13    287090\n",
      "   macro avg       0.03      0.25      0.06    287090\n",
      "weighted avg       0.02      0.13      0.03    287090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=axis).ravel(), np.argmax(p, axis=axis).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(X_test):\n",
    "    y_hat = np.argmax(p[i], axis=-1)\n",
    "    gt = np.argmax(y_test[i], axis=-1)\n",
    "    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
    "        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag:CONTENT\n",
      "\t TN:         0\tFP:         0\n",
      "\t FN:         0\tTP:     36620\n",
      "tag:SOURCE\n",
      "\t TN:      5695\tFP:         0\n",
      "\t FN:         0\tTP:         0\n",
      "tag:O\n",
      "\t TN:         0\tFP:    241018\n",
      "\t FN:         0\tTP:         0\n",
      "tag:CUE\n",
      "\t TN:      3757\tFP:         0\n",
      "\t FN:         0\tTP:         0\n"
     ]
    }
   ],
   "source": [
    "for tag in tag2idx.keys():\n",
    "    print(f'tag:{tag}')    \n",
    "    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
    "    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_py37_venv",
   "language": "python",
   "name": "jupyter_py37_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
