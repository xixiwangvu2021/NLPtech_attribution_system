{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install, import and read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import s3fs\n",
    "\n",
    "what_corpus = 'data_total'  # 'polnear', 'parc30'\n",
    "what_type_files = 'test'\n",
    "\n",
    "# filepath = 's3://sagemaker-studio-528576943967-ssf9zkrg3os/polnear-conll/prepared/'\n",
    "filepath = '../' + what_corpus + '-conll/prepared/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = what_corpus + '_preprocessed_' + what_type_files + '_noBIO.csv'\n",
    "dataset = pd.read_csv(filepath + filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over testfiles, loop over tokens, first look for CUE sequences and number them. Then find the closest SOURCE and CONTENTS sequences and give them the same number.\n",
    "# If we loop over the file in dict format, we can change the dict easily during the loop.\n",
    "# For finding SOURCE and CONTENT do an additional loop traveling the dict backwards and forwards at the same time with loop_index and -loop_index\n",
    "\n",
    "# Additions\n",
    "# Not just take the closest SOURCE and CONTENT, but also check if they are closer to another CUE.\n",
    "# And at the end check and number the SOURCE and CONTENT labels without CUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilesGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_files = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "#         agg_func = lambda f: ' '.join(f[\"word\"].values.tolist())\n",
    "        agg_func = lambda f: [(w, t, f) for w, t, f in zip(f[\"word\"].values.tolist(),\n",
    "                                                           f[\"tag\"].values.tolist(),\n",
    "                                                           f[\"filename\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby([\"filename\"]).apply(agg_func)\n",
    "        self.files = [f for f in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            f = self.grouped[\"Files: {}\".format(self.n_tokens)]\n",
    "            self.n_files += 1\n",
    "            return f\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_getter = FilesGetter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "files = files_getter.files\n",
    "print(type(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "print(len(files))\n",
    "# files[0:1]\n",
    "# # [[('Washington', 'SOURCE'),\n",
    "# #   ('Post', 'SOURCE'),\n",
    "# #   ('back', 'CUE'),\n",
    "# #   ('Hillary', 'CONTENT'),\n",
    "# #   ('Clinton', 'CONTENT'),\n",
    "# #   ('for', 'CONTENT'),\n",
    "# #   ('president', 'CONTENT'),\n",
    "# #   ('.', 'O'),\n",
    "# #   ('Washington', 'O'),\n",
    "# #   ('-lrb-', 'O'),\n",
    "# #   ('AFP', 'O'),\n",
    "# #   ('-rrb-', 'O'),\n",
    "# #   ('--', 'O'),\n",
    "# #   ('the', 'O'),\n",
    "# #   ('Washington', 'O'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testlist = [1, 2, 3, 4, 5, 6]\n",
    "# start_index = 2\n",
    "# print(testlist[start_index])\n",
    "# testlist.reverse()\n",
    "# start_index = len(testlist) - start_index - 1\n",
    "# print(start_index)\n",
    "# print(testlist[start_index])\n",
    "\n",
    "# for file_index, file in enumerate(file_temp[start_index:]):\n",
    "#     print(file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index_labels = 0  # = 2  # For BIO labels\n",
    "\n",
    "def number_continuous_labels(file_temp, start_idx, label_nr, label_type, reverse=False):\n",
    "    if reverse:\n",
    "        file_temp.reverse()\n",
    "        start_idx = (len(file_temp) - 1) - start_idx\n",
    "        \n",
    "    for file_index, file in enumerate(file_temp):\n",
    "#         if file_index >= start_idx - 1:\n",
    "        if file_index > start_idx:\n",
    "#             print(file_index)\n",
    "            label = file[1]\n",
    "            if label == label_type:\n",
    "                token = file[0]\n",
    "                file_temp[file_index] = (token, label + '-' + str(label_nr))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Reverse back once we're done\n",
    "    if reverse:\n",
    "        file_temp.reverse()\n",
    "        \n",
    "    return file_temp\n",
    "\n",
    "# Find the CONTENT or SOURCE label for a CUE and add the number to the label\n",
    "def number_label(token_idx, file_temp, label_nr, label_type):\n",
    "    label_search_index = 1\n",
    "    label_found_index = None\n",
    "    \n",
    "#     print(label_type)\n",
    "#     print(file_temp)\n",
    "    \n",
    "    file_temp_len = len(file_temp)\n",
    "    backwards_search_index = token_idx - label_search_index\n",
    "    forwards_search_index = token_idx + label_search_index\n",
    "\n",
    "#     print(file_temp_len)\n",
    "#     print(backwards_search_index)\n",
    "#     print(forwards_search_index)\n",
    "#     print(len(file_temp[0:backwards_search_index]) > 0 or len(file_temp[forwards_search_index:file_temp_len]) > 0)\n",
    "\n",
    "#     while len(file_temp[0:backwards_search_index]) >= 0 or len(file_temp[forwards_search_index:file_temp_len]) > 0:\n",
    "    while backwards_search_index >= 0 or forwards_search_index < file_temp_len:\n",
    "#         print(label_search_index)\n",
    "\n",
    "        # Backwards loop\n",
    "        if backwards_search_index >= 0:\n",
    "            file_temp_item_backwards = file_temp[backwards_search_index]\n",
    "#             print('backwards')\n",
    "#             print(file_temp_item_backwards)\n",
    "            label_backwards = file_temp_item_backwards[1]  # Label is 2nd in tuple\n",
    "            if label_backwards[start_index_labels:] == label_type:\n",
    "#                 print('label found')\n",
    "                # Overwrite a possible content_found_index in the forward loop. \n",
    "                # We favor the backwards content_found_index over the forward loop.\n",
    "                # If we find a content, break immediately from while loop\n",
    "                label_found_index = backwards_search_index\n",
    "                token_backwards  = file_temp_item_backwards[0]  # Token is 1st in tuple\n",
    "                file_temp[label_found_index] = (token_backwards, label_backwards + '-' + str(label_nr))\n",
    "#                 print(file_temp[label_found_index])\n",
    "                file_temp = number_continuous_labels(file_temp, label_found_index, label_nr, label_type, reverse=True)\n",
    "                break\n",
    "\n",
    "        # Forwards loop\n",
    "        if forwards_search_index < file_temp_len:\n",
    "            file_temp_item_forwards = file_temp[forwards_search_index]\n",
    "#             print('forwards')\n",
    "#             print(file_temp_item_forwards)\n",
    "            label_forwards = file_temp_item_forwards[1]  # Label is 2nd in tuple\n",
    "            if label_forwards[start_index_labels:] == label_type: \n",
    "#                 print('label found')\n",
    "                label_found_index = forwards_search_index\n",
    "                token_forwards  = file_temp_item_forwards[0]  # Token is 1st in tuple\n",
    "                file_temp[label_found_index] = (token_forwards, label_forwards + '-' + str(label_nr))\n",
    "#                 print(file_temp[label_found_index])\n",
    "                file_temp = number_continuous_labels(file_temp, label_found_index, label_nr, label_type)\n",
    "                break\n",
    "\n",
    "        label_search_index += 1 \n",
    "        backwards_search_index = token_idx - label_search_index\n",
    "        forwards_search_index = token_idx + label_search_index\n",
    "\n",
    "        # Uncomment for debugging\n",
    "#         if label_search_index == 1:  # One is first sentence with labels\n",
    "#             break \n",
    "\n",
    "    return file_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "files_labels_numbered = []\n",
    "files_labels_max_label_nr = []\n",
    "for file_index, file in enumerate(files):\n",
    "#     file = file[0:30]  # Uncomment for debugging\n",
    "    # Copy of the tokens in the file\n",
    "    file_temp = file.copy()\n",
    "#     file_temp_len = len(file_temp)\n",
    "    label_nr = 1\n",
    "    max_label_nr = 0\n",
    "#     print('file')\n",
    "#     print(file)\n",
    "#     print(file_temp)\n",
    "#     print(file_temp_len)\n",
    "#     print(label_nr)\n",
    "    \n",
    "    # Loop over tokens to number labels\n",
    "    cue_found_index = -1\n",
    "    for token_idx, file_token in enumerate(file):\n",
    "        label = file_token[1]  # Label is 2nd in tuple\n",
    "        \n",
    "#         print('token')\n",
    "#         print(token_idx)\n",
    "#         print(label)\n",
    "        \n",
    "        # If the label is CUE without a number, number the CUE first\n",
    "        is_search_content_source = False\n",
    "        if label[start_index_labels:] == 'CUE':\n",
    "#             print('cue found')\n",
    "#             print(cue_found_index)\n",
    "#             print(token_idx > cue_found_index+1)\n",
    "\n",
    "            # We are only handling continuous CUEs. If CUE is not continuous, then the label nr goes up \n",
    "            if token_idx > cue_found_index + 1:\n",
    "                label_nr += 1\n",
    "                is_search_content_source = True\n",
    "                \n",
    "            token = file_token[0]  # Token is 1st in tuple\n",
    "            file_temp[token_idx] = (token, label + '-' + str(label_nr))\n",
    "            cue_found_index = token_idx\n",
    "#             print(file_temp[token_idx])\n",
    "            \n",
    "            if is_search_content_source:\n",
    "#                 print('label_nr')\n",
    "#                 print(label_nr)\n",
    "                \n",
    "#                 if label_nr == 3:\n",
    "                # Now search backwards and forwards in file for CONTENT and SOURCE labels\n",
    "                file_temp = number_label(token_idx, file_temp, label_nr, 'CONTENT')\n",
    "                file_temp = number_label(token_idx, file_temp, label_nr, 'SOURCE')\n",
    "                \n",
    "            max_label_nr = label_nr\n",
    "            \n",
    "    # Append highest label nr found to list\n",
    "    files_labels_numbered.append(file_temp)\n",
    "    files_labels_max_label_nr.append(max_label_nr)\n",
    "            \n",
    "#     if file_index == 0:\n",
    "#         break\n",
    "                \n",
    "print(len(files_labels_numbered))\n",
    "# files_labels_numbered[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CNN', '_ SOURCE-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"'s\", '_ SOURCE-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Jake', '_ SOURCE-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Tapper', '_ SOURCE-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('on', '_ CONTENT-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Donna', '_ CONTENT-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Brazile', '_ CONTENT-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('leak', '_ CONTENT-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('to', '_ CONTENT-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Clinton', '_ CONTENT-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Campaign', '_ CONTENT-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (':', '_ CUE-2 _ _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('`', 'O'),\n",
       " ('unethical', 'O'),\n",
       " (',', 'O'),\n",
       " (\"'\", 'O'),\n",
       " ('`', 'O'),\n",
       " ('horrifying', 'O'),\n",
       " ('.', 'O'),\n",
       " (\"'\", 'O'),\n",
       " ('a', '_ _ SOURCE-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('WikiLeaks', '_ _ SOURCE-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('document', '_ _ SOURCE-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('suggest', '_ _ CUE-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('former', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('adviser', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('to', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('President', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Bill', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Clinton', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('and', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('current', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Democrat', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('National', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Chairperson', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Donna', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Brazile', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('provide', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('a', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('question', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('to', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Clinton', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"'s\", '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('aide', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('during', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('the', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('recent', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Democrat', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('primary', '_ _ CONTENT-3 _ _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('.', 'O'),\n",
       " ('now', 'O'),\n",
       " ('Brazile', '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"'s\", '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('CNN', '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('colleague', '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (',', '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Jake', '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Tapper', '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (',', '_ _ _ SOURCE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('insist', '_ _ _ CUE-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Brazile', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"'s\", '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('action', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('``', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('unethical', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"''\", '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('and', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('``', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('horrifying', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('.', '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"''\", '_ _ _ CONTENT-4 _ _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('in', 'O'),\n",
       " ('a', 'O'),\n",
       " ('Thursday', 'O'),\n",
       " ('interview', 'O'),\n",
       " ('on', 'O'),\n",
       " ('Washington', 'O'),\n",
       " ('D.C.', 'O'),\n",
       " (\"'s\", 'O'),\n",
       " ('WMAL', 'O'),\n",
       " ('radio', 'O'),\n",
       " (',', 'O'),\n",
       " ('Tapper', '_ _ _ _ SOURCE-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('tell', '_ _ _ _ CUE-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('host', 'O'),\n",
       " ('Larry', 'O'),\n",
       " (\"O'Connor\", 'O'),\n",
       " ('that', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('the', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('evidence', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('that', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Brazile', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('leak', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('debate', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('question', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('to', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('Hillary', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"'s\", '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('campaign', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('a', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('journalistically', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('unethical', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('act', '_ _ _ _ CONTENT-5 _ _ _ _ _ _ _ _ _ _ _'),\n",
       " ('.', 'O'),\n",
       " ('Tapper', '_ _ _ _ _ SOURCE-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('tell', '_ _ _ _ _ CUE-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"O'Connor\", 'O'),\n",
       " ('that', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('WikiLeaks', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('news', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('``', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('very', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " (',', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('very', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('troubling', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " (',', '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " (\"''\", '_ _ _ _ _ CONTENT-6 _ _ _ _ _ _ _ _ _ _'),\n",
       " ('before', 'O'),\n",
       " ('note', '_ _ _ _ _ _ CUE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('that', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('Brazile', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('``', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('a', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('good', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('person', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('.', '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " (\"''\", '_ _ _ _ _ _ CONTENT-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('CNN', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('host', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('of', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('lead', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('with', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('Jake', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('Tapper', '_ _ _ _ _ _ SOURCE-7 _ _ _ _ _ _ _ _ _'),\n",
       " ('go', '_ _ _ _ _ _ _ CUE-8 _ _ _ _ _ _ _ _'),\n",
       " ('on', '_ _ _ _ _ _ _ CUE-8 _ _ _ _ _ _ _ _'),\n",
       " ('to', '_ _ _ _ _ _ _ CUE-8 _ _ _ _ _ _ _ _'),\n",
       " ('express', '_ _ _ _ _ _ _ CUE-8 _ _ _ _ _ _ _ _'),\n",
       " ('dismay', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('over', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('leak', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('purportedly', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('emanate', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('from', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('he', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('cable', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('network', '_ _ _ _ _ _ _ CONTENT-8 _ _ _ _ _ _ _ _'),\n",
       " ('.', 'O'),\n",
       " ('``', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('horrifying', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('.', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('journalistically', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('horrifying', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " (',', '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " (\"''\", '_ _ _ _ _ _ _ _ CONTENT-9 _ _ _ _ _ _ _'),\n",
       " ('Tapper', '_ _ _ _ _ _ _ SOURCE-8 _ _ _ _ _ _ _ _'),\n",
       " ('say', '_ _ _ _ _ _ _ _ CUE-9 _ _ _ _ _ _ _'),\n",
       " (',', 'O'),\n",
       " ('``', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('and', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('sure', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('will', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('have', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('a', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('impact', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('on', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('partner', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('with', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('this', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('organization', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('in', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('future', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('and', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('sure', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('will', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('have', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('a', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('effect', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('on', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('...', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('Donna', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('Brazile', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('no', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('longer', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('with', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('CNN', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('because', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('she', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('with', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('DNC', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('right', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('now', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " (',', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('but', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('sure', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('will', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('have', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('some', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('impact', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('on', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('Donna', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('Brazile', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('.', '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " (\"''\", '_ _ _ _ _ _ _ _ _ CONTENT-10 _ _ _ _ _ _'),\n",
       " ('Tapper', '_ _ _ _ _ _ _ _ SOURCE-9 _ _ _ _ _ _ _'),\n",
       " ('also', '_ _ _ _ _ _ _ _ _ CUE-10 _ _ _ _ _ _'),\n",
       " ('assure', '_ _ _ _ _ _ _ _ _ CUE-10 _ _ _ _ _ _'),\n",
       " ('WMAL', 'O'),\n",
       " ('that', 'O'),\n",
       " ('CNN', 'O'),\n",
       " ('take', 'O'),\n",
       " ('this', 'O'),\n",
       " ('``', 'O'),\n",
       " ('very', 'O'),\n",
       " (',', 'O'),\n",
       " ('very', 'O'),\n",
       " ('seriously', 'O'),\n",
       " ('.', 'O'),\n",
       " (\"''\", 'O'),\n",
       " ('in', 'O'),\n",
       " ('closing', 'O'),\n",
       " (',', 'O'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ SOURCE-10 _ _ _ _ _ _'),\n",
       " ('CNN', '_ _ _ _ _ _ _ _ _ SOURCE-10 _ _ _ _ _ _'),\n",
       " ('host', '_ _ _ _ _ _ _ _ _ SOURCE-10 _ _ _ _ _ _'),\n",
       " ('reiterate', '_ _ _ _ _ _ _ _ _ _ CUE-11 _ _ _ _ _'),\n",
       " (',', 'O'),\n",
       " ('``', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('horrifying', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('and', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('very', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " (',', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('very', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('upset', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('and', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('can', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('not', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('condemn', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('any', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('more', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('than', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('...', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('condemn', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('in', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('no', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('uncertain', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('term', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " (',', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('it', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('be', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('awful', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('.', '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " (\"''\", '_ _ _ _ _ _ _ _ _ _ CONTENT-11 _ _ _ _ _'),\n",
       " ('accord', '_ _ _ _ _ _ _ _ _ _ _ CUE-12 _ _ _ _'),\n",
       " ('to', '_ _ _ _ _ _ _ _ _ _ _ CUE-12 _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ SOURCE-11 _ _ _ _ _'),\n",
       " ('document', '_ _ _ _ _ _ _ _ _ _ SOURCE-11 _ _ _ _ _'),\n",
       " ('release', '_ _ _ _ _ _ _ _ _ _ SOURCE-11 _ _ _ _ _'),\n",
       " ('by', '_ _ _ _ _ _ _ _ _ _ SOURCE-11 _ _ _ _ _'),\n",
       " ('WikiLeaks', '_ _ _ _ _ _ _ _ _ _ SOURCE-11 _ _ _ _ _'),\n",
       " (',', 'O'),\n",
       " ('on', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('March', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('12', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('--', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('just', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('one', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('day', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('before', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('a', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('democratic', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('presidential', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('town', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('hall', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('debate', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('between', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Sen.', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Bernie', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Sanders', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('-lrb-', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('I-VT', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('-rrb-', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('and', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Hillary', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Clinton', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('--', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Brazile', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('allegedly', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('send', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('a', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('email', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('title', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('``', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('from', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('time', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('to', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('time', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('get', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('question', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('in', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('advance', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " (',', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " (\"''\", '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('to', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Jennifer', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Palmieri', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " (',', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('Clinton', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " (\"'s\", '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('campaign', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('communication', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('director', '_ _ _ _ _ _ _ _ _ _ _ CONTENT-12 _ _ _ _'),\n",
       " ('.', 'O'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ SOURCE-12 _ _ _ _'),\n",
       " ('email', '_ _ _ _ _ _ _ _ _ _ _ SOURCE-12 _ _ _ _'),\n",
       " ('provide', '_ _ _ _ _ _ _ _ _ _ _ _ CUE-13 _ _ _'),\n",
       " ('a', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('question', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('about', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('death', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('penalty', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('that', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('she', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('say', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('``', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('worry', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('about', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('HRC', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('.', '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " (\"''\", '_ _ _ _ _ _ _ _ _ _ _ _ CONTENT-13 _ _ _'),\n",
       " ('follow', 'O'),\n",
       " ('the', 'O'),\n",
       " ('original', 'O'),\n",
       " ('email', 'O'),\n",
       " ('be', 'O'),\n",
       " ('several', 'O'),\n",
       " ('more', 'O'),\n",
       " ('missive', 'O'),\n",
       " ('as', 'O'),\n",
       " ('Brazile', '_ _ _ _ _ _ _ _ _ _ _ _ SOURCE-13 _ _ _'),\n",
       " ('and', '_ _ _ _ _ _ _ _ _ _ _ _ SOURCE-13 _ _ _'),\n",
       " ('Hillary', '_ _ _ _ _ _ _ _ _ _ _ _ SOURCE-13 _ _ _'),\n",
       " (\"'s\", '_ _ _ _ _ _ _ _ _ _ _ _ SOURCE-13 _ _ _'),\n",
       " ('staffer', '_ _ _ _ _ _ _ _ _ _ _ _ SOURCE-13 _ _ _'),\n",
       " ('discuss', '_ _ _ _ _ _ _ _ _ _ _ _ _ CUE-14 _ _'),\n",
       " ('how', '_ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-14 _ _'),\n",
       " ('to', '_ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-14 _ _'),\n",
       " ('handle', '_ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-14 _ _'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-14 _ _'),\n",
       " ('expected', '_ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-14 _ _'),\n",
       " ('debate', '_ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-14 _ _'),\n",
       " ('question', '_ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-14 _ _'),\n",
       " ('.', 'O'),\n",
       " ('despite', 'O'),\n",
       " ('the', 'O'),\n",
       " ('evidence', 'O'),\n",
       " ('in', 'O'),\n",
       " ('the', 'O'),\n",
       " ('release', 'O'),\n",
       " ('document', 'O'),\n",
       " (',', 'O'),\n",
       " ('Brazile', '_ _ _ _ _ _ _ _ _ _ _ _ _ SOURCE-14 _ _'),\n",
       " ('later', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-15 _'),\n",
       " ('insist', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-15 _'),\n",
       " ('she', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('never', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('send', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('question', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('to', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('Hillary', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " (\"'s\", '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('campaign', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-15 _'),\n",
       " ('.', 'O'),\n",
       " ('``', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('never', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('have', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('access', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('to', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('question', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('and', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('would', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('never', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('have', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('share', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('they', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('with', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('candidate', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('if', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('I', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('do', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " (',', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " (\"''\", '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CONTENT-16'),\n",
       " ('Brazile', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ SOURCE-15 _'),\n",
       " ('say', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('in', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('a', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('statement', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('supply', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('to', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('press', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('after', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('the', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('WikiLeaks', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('release', '_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ CUE-16'),\n",
       " ('.', 'O'),\n",
       " ('follow', 'O'),\n",
       " ('Warner', 'O'),\n",
       " ('Todd', 'O'),\n",
       " ('Huston', 'O'),\n",
       " ('on', 'O'),\n",
       " ('Twitter', 'O'),\n",
       " ('@warnerthuston', 'O'),\n",
       " ('or', 'O'),\n",
       " ('email', 'O'),\n",
       " ('the', 'O'),\n",
       " ('author', 'O'),\n",
       " ('at', 'O'),\n",
       " ('igcolonel@hotmail.com', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_labels_numbered_padded = []\n",
    "for file_idx, file in enumerate(files_labels_numbered):\n",
    "    max_label_nr = files_labels_max_label_nr[file_idx]\n",
    "    file_temp = file.copy()\n",
    "    for token_idx, file_token in enumerate(file):\n",
    "        token = file_token[0]\n",
    "        label = file_token[1]\n",
    "        if label != 'O':\n",
    "            label_nr_str = label.replace('B','').replace('I','').replace('-','').replace('SOURCE','').replace('CONTENT','').replace('CUE','')\n",
    "            # Try except\n",
    "            if label_nr_str:\n",
    "                label_nr = int(label_nr_str)\n",
    "                lpad_string = \"_ \" * (max_label_nr - (max_label_nr - label_nr) - 1)\n",
    "                rpad_string = \" _\" * (max_label_nr - label_nr)\n",
    "                label = lpad_string + label + rpad_string\n",
    "            else:\n",
    "                # Replace quotation labels without a number by 'O'\n",
    "                label = 'O'\n",
    "        file_temp[token_idx] = (token, label)\n",
    "    files_labels_numbered_padded.append(file_temp)\n",
    "    \n",
    "    if file_idx == 1:\n",
    "        break\n",
    "    \n",
    "files_labels_numbered_padded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
