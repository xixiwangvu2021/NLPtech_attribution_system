{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. component_extraction_CRF\n",
    "1. To extract features and labels, sentences without quotations are not removed.\n",
    "2. CRF\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import csv\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To extract features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756666\n",
      "81574\n",
      "1109421\n",
      "34145\n",
      "115717\n",
      "1866085\n"
     ]
    }
   ],
   "source": [
    "# to prepare for using CRF.py, not removing unlabeled sentences\n",
    "preprocessed_data=[\"polnear_preprocessed_BIO_train.csv\", \"polnear_preprocessed_BIO_dev.csv\",\n",
    "                   \"parc3_preprocessed_BIO_train.csv\", \"parc3_preprocessed_BIO_dev.csv\",\n",
    "                   \"merged-parc3-polnear_preprocessed_BIO_dev.csv\", \"merged-parc3-polnear_preprocessed_BIO_train.csv\"]\n",
    "for i in preprocessed_data:\n",
    "    with open(f\"../Preprocessed_data/{i}\", \"r\") as source:\n",
    "        reader = csv.reader(source)\n",
    "        with open(f\"Preprocessed_data/{i.replace('.csv','_token_label.csv')}\", \"w\") as result:\n",
    "            writer = csv.writer(result, delimiter='\\t')\n",
    "            for r in reader:\n",
    "                writer.writerow((r[1], r[2], r[3], r[4], r[5], r[6], r[7], r[8], r[9], r[10], r[11]))\n",
    "            # to add an empty row at the end, otherwise CRF doesn't catch the last line. Because CRF is trained on sentences, and sentences are separated by ''.\n",
    "            footers=['', '', '', '', '', '', '', '', '', '', 'O']\n",
    "            footer_row='\\t'.join(footers)\n",
    "            writer.writerow(footer_row)\n",
    "    file1 = open(f\"../Preprocessed_data/{i.replace('.csv','_token_label.csv')}\")\n",
    "    reader1 = csv.reader(file1)\n",
    "    lines= len(list(reader1))\n",
    "\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to extract extra features, to prepare for using CRF.py, not removing unlabeled sentences\n",
    "# # to extract only token and labels, to prepare for using CRF.py, removing unlabeled sentences\n",
    "# preprocessed_data_token_label=[\"polnear_preprocessed_BIO_train_token_label.csv\",\n",
    "#                                \"parc3_preprocessed_BIO_train_token_label.csv\",\n",
    "#                                \"merged-parc3-polnear_preprocessed_BIO_train_token_label.csv\"\n",
    "#                               ]\n",
    "\n",
    "# def get_extra_features(filename):\n",
    "#     quote_opening = \"``\"\n",
    "#     quote_closing = \"''\"\n",
    "\n",
    "#     # Read file\n",
    "#     csvinput = open(f\"../Preprocessed_data/{filename}\",'r')\n",
    "#     csvreader = csv.reader(csvinput,delimiter='\\t')\n",
    "# #     print(csvreader)\n",
    "#     headers = next(csvreader)\n",
    "#     sents = []\n",
    "#     current_sent = []\n",
    "#     for row in csvreader:\n",
    "#         current_sent.append(list(row))\n",
    "#         #note that this is a simplification that works well for this particular data, in other situations, you may need to do more advanced preprocessing to identify sentence boundaries\n",
    "#         if row[6] == \".\":\n",
    "#             sents.append(current_sent)\n",
    "#             current_sent = []            \n",
    "\n",
    "#     # Loop over sentences in file\n",
    "#     in_quote = False\n",
    "#     sents_extra_features = []\n",
    "#     for sent in sents:\n",
    "#         print(type(sent))\n",
    "#         # Loop over tokens in sentence\n",
    "#         sent_extra_features = sent.copy()\n",
    "#         for token in sent:\n",
    "#             token_extra_features = token.copy()\n",
    "#             label = token[-1]\n",
    "#             sent_extra_features.append(token_extra_features)\n",
    "        \n",
    "#         # Append row with extra features to new list\n",
    "#         sents_extra_features.append(sent_extra_features)\n",
    "            \n",
    "#     # Write file\n",
    "#     with open(f\"../Preprocessed_data/{filename.replace('.csv','_extra_features.csv')}\", \"w\") as result:\n",
    "#         headers1 = [\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTO\", \"Word\", \"Lemma\", \"POS\", \"Dep_label\", \"Token_dep_head\", \n",
    "#                     \"AR_label\"]  # , \"In_quote\", ]\n",
    "#         writer = csv.writer(result, delimiter='\\t')\n",
    "#         writer.writerow(headers1)\n",
    "#         for f in sents_removed:\n",
    "#             writer.writerows(f)\n",
    "#     return\n",
    "\n",
    "# # For every filename, get extra features\n",
    "# for filename in preprocessed_data_token_label:\n",
    "#     get_extra_features(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract only token and labels, to prepare for using CRF.py, removing unlabeled sentences\n",
    "preprocessed_data_token_label=[\"polnear_preprocessed_BIO_train_token_label.csv\",\n",
    "                               \"parc3_preprocessed_BIO_train_token_label.csv\",\n",
    "                               \"merged-parc3-polnear_preprocessed_BIO_train_token_label.csv\"\n",
    "                              ]\n",
    "#inputfile=\"polnear_preprocessed_BIO_dev_token_label.csv\"\n",
    "#inputfile=polnear_outfile_path\n",
    "#for i in preprocessed_data_token_label:\n",
    "#i=\"polnear_preprocessed_BIO_dev_token_label.csv\"\n",
    "def get_removed_file(filename):\n",
    "#     print('something')\n",
    "    csvinput = open(f\"../Preprocessed_data/{filename}\",'r')\n",
    "    csvreader = csv.reader(csvinput,delimiter='\\t')\n",
    "#     print(csvreader)\n",
    "    headers=next(csvreader)\n",
    "    sents = []\n",
    "    current_sent = []\n",
    "    for row in csvreader:\n",
    "        current_sent.append(list(row))\n",
    "        #note that this is a simplification that works well for this particular data, in other situations, you may need to do more advanced preprocessing to identify sentence boundaries\n",
    "        if row[6] == \".\":\n",
    "            sents.append(current_sent)\n",
    "            current_sent = []\n",
    "    sents_removed=[]\n",
    "    for sent in sents:\n",
    "#         print(sents)\n",
    "        sent_label_list=[]\n",
    "        for f in sent:\n",
    "            label = f[-1]\n",
    "            sent_label_list.append(label)\n",
    "        if set(sent_label_list)!={'O'}:\n",
    "            #print(set(sent))\n",
    "            sents_removed.append(sent)\n",
    "    with open(f\"../Preprocessed_data/{filename.replace('.csv','_removed.csv')}\", \"w\") as result:\n",
    "#         headers1=['Word','AR_label']\n",
    "        headers1 = [\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTo\", \"Word\", \"Lemma\", \"POS\", \"Dep_label\", \"Token_dep_head\", \"AR_label\"]\n",
    "        #header_row=','.join(headers)+'\\n'    \n",
    "        writer = csv.writer(result, delimiter='\\t')\n",
    "        writer.writerow(headers1)\n",
    "        for f in sents_removed:\n",
    "            writer.writerows(f)\n",
    "    return\n",
    "\n",
    "for i in preprocessed_data_token_label:\n",
    "    get_removed_file(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run CRF.py\n",
    "\n",
    "2.1.1 The scripts for command line: train and dev data from the same source\n",
    "1) polnear_BIO: \n",
    "python3 CRF.py polnear_preprocessed_BIO_train_token_label.csv polnear_preprocessed_BIO_dev_token_label.csv CRF_out_BIO_polnear.csv\n",
    "2) parc3_BIO:\n",
    "python3 CRF.py parc3_preprocessed_BIO_train_token_label.csv parc3_preprocessed_BIO_dev_token_label.csv CRF_out_BIO_parc3.csv\n",
    "3) merged_parc3_polnear_BIO:\n",
    "python3 CRF.py merged-parc3-polnear_preprocessed_BIO_train_token_label.csv merged-parc3-polnear_preprocessed_BIO_dev_token_label.csv CRF_out_BIO_merged-parc3-polnear.csv\n",
    "\n",
    "2.1.2 The scripts for command line: train and dev data from the different source\n",
    "1) polnear_BIO: \n",
    "python3 CRF.py polnear_preprocessed_BIO_train_token_label.csv parc3_preprocessed_BIO_dev_token_label.csv CRF_out_BIO_polnear-parc3.csv\n",
    "2) parc3_BIO:\n",
    "python3 CRF.py parc3_preprocessed_BIO_train_token_label.csv polnear_preprocessed_BIO_dev_token_label.csv CRF_out_BIO_parc3-polnear.csv\n",
    "\n",
    "2.2.1 The scripts for command line: train and dev data from the same source, unlabeled sentences removed\n",
    "1) polnear_BIO: \n",
    "python3 CRF.py polnear_preprocessed_BIO_train_token_label_removed.csv polnear_preprocessed_BIO_dev_token_label_removed.csv CRF_out_BIO_polnear_removed.csv\n",
    "2) parc3_BIO:\n",
    "python3 CRF.py parc3_preprocessed_BIO_train_token_label_removed.csv parc3_preprocessed_BIO_dev_token_label_removed.csv CRF_out_BIO_parc3_removed.csv\n",
    "3) merged_parc3_polnear_BIO:\n",
    "python3 CRF.py merged-parc3-polnear_preprocessed_BIO_train_token_label_removed.csv merged-parc3-polnear_preprocessed_BIO_dev_token_label_removed.csv CRF_out_BIO_merged-parc3-polnear_removed.csv\n",
    "\n",
    "2.2.2 The scripts for command line: train and dev data from the different source, unlabeled sentences removed\n",
    "1) polnear_BIO: \n",
    "python3 CRF.py polnear_preprocessed_BIO_train_token_label_removed.csv parc3_preprocessed_BIO_dev_token_label_removed.csv CRF_out_BIO_polnear-parc3_removed.csv\n",
    "2) parc3_BIO:\n",
    "python3 CRF.py parc3_preprocessed_BIO_train_token_label_removed.csv polnear_preprocessed_BIO_dev_token_label_removed.csv CRF_out_BIO_parc3-polnear_removed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1 Evaluation: train and dev data from the same source. unlabeled sentences not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation polnear BIO with polnear's train and dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.59      0.40      0.48      2193\n",
      "       B-CUE       0.80      0.56      0.66      2189\n",
      "    B-SOURCE       0.72      0.50      0.59      1947\n",
      "   I-CONTENT       0.74      0.74      0.74     36877\n",
      "       I-CUE       0.42      0.23      0.30      1808\n",
      "    I-SOURCE       0.59      0.46      0.52      4070\n",
      "           O       0.66      0.73      0.70     32460\n",
      "\n",
      "    accuracy                           0.69     81544\n",
      "   macro avg       0.65      0.52      0.57     81544\n",
      "weighted avg       0.69      0.69      0.69     81544\n",
      "\n",
      "[[  883     1    63   531    75    21   619]\n",
      " [    1  1221     3   443    19    27   475]\n",
      " [   24     7   971   371     5    46   523]\n",
      " [  336   122    72 27216   237   315  8579]\n",
      " [   38    44     5   554   423    55   689]\n",
      " [   10     8    76   719    13  1867  1377]\n",
      " [  199   117   150  7099   224   817 23854]]\n",
      "----------------------------------------------------------\n",
      "\n",
      "Baseline evaluation parc3 BIO with parc3's train and dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.23      0.10      0.14       629\n",
      "       B-CUE       0.30      0.13      0.19       652\n",
      "    B-SOURCE       0.24      0.10      0.14       581\n",
      "   I-CONTENT       0.47      0.30      0.37      9816\n",
      "       I-CUE       0.32      0.05      0.09       275\n",
      "    I-SOURCE       0.22      0.11      0.15      1672\n",
      "           O       0.66      0.84      0.74     20437\n",
      "\n",
      "    accuracy                           0.60     34062\n",
      "   macro avg       0.35      0.24      0.26     34062\n",
      "weighted avg       0.56      0.60      0.57     34062\n",
      "\n",
      "[[   66     6     6   118     1    14   418]\n",
      " [    2    88     3   121     2    16   420]\n",
      " [    6     4    60   103     0    19   389]\n",
      " [   77    74    76  2964    11   283  6331]\n",
      " [    3     3     0    49    15     4   201]\n",
      " [   18    16    14   290     1   183  1150]\n",
      " [  121   103    94  2661    17   299 17142]]\n",
      "----------------------------------------------------------\n",
      "\n",
      "Baseline evaluation merged-parc3-polnear BIO with merged-parc3-polnear's train and dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.56      0.38      0.45      2822\n",
      "       B-CUE       0.74      0.48      0.58      2842\n",
      "    B-SOURCE       0.75      0.50      0.60      2529\n",
      "   I-CONTENT       0.77      0.66      0.71     46718\n",
      "       I-CUE       0.45      0.18      0.26      2083\n",
      "    I-SOURCE       0.67      0.48      0.56      5742\n",
      "           O       0.67      0.83      0.75     52898\n",
      "\n",
      "    accuracy                           0.71    115634\n",
      "   macro avg       0.66      0.50      0.56    115634\n",
      "weighted avg       0.71      0.71      0.70    115634\n",
      "\n",
      "[[ 1060     7    32   518    37    35  1133]\n",
      " [   11  1358    11   356    21    28  1057]\n",
      " [   28    10  1264   259     4    26   938]\n",
      " [  396   190   151 30662   195   435 14689]\n",
      " [   46    18     5   403   381    44  1186]\n",
      " [   20    22    19   565     7  2758  2351]\n",
      " [  328   230   199  6999   194   804 44144]]\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/python-read-csv-columns-into-list/\n",
    "# to write the annotations and predictions into lists\n",
    "\n",
    "# To get the predictions\n",
    "result_list=[\"CRF_out_BIO_polnear.csv\",\"CRF_out_BIO_parc3.csv\",\"CRF_out_BIO_merged-parc3-polnear.csv\"]\n",
    "for i in result_list:\n",
    "    filename = open(f'Result/{i}', 'r')\n",
    "    file = csv.DictReader(filename, delimiter='\\t')\n",
    "    four_class_system_output = []\n",
    "    tokens=[]\n",
    "    for col in file:\n",
    "        #print(col)\n",
    "        four_class_system_output.append(col['pred_AR_label'])\n",
    "        tokens.append(col['Word'])\n",
    "    #print(four_class_system_output[0:20])\n",
    "    # To get the annotations\n",
    "    data_source=i.replace('.csv','').split(\"_\")[-1]\n",
    "    filename1 = open(f'../Preprocessed_data/{data_source}_preprocessed_BIO_dev_token_label.csv', 'r')\n",
    "    file1 = csv.DictReader(filename1, delimiter='\\t')\n",
    "    four_class_human_annotation = []\n",
    "    for col in file1:\n",
    "        #print(col)\n",
    "        four_class_human_annotation.append(col['AR_label'])\n",
    "    # to check if the predicted file and the dev file are aligned.\n",
    "    #print(f\"{data_source} {len(four_class_human_annotation)} and {len(four_class_system_output)}\")\n",
    "    report = classification_report(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output,digits = 2)\n",
    "    print(f\"Baseline evaluation {data_source} BIO with {data_source}'s train and dev files\")\n",
    "    print(report)\n",
    "    print(sklearn.metrics.confusion_matrix(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output))\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2 Evaluation: train and dev data from the different source. unlabeled sentences not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation polnear-parc3 BIO with polnear's train and parc3's dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.08      0.05      0.06       627\n",
      "       B-CUE       0.12      0.07      0.09       650\n",
      "    B-SOURCE       0.07      0.03      0.04       579\n",
      "   I-CONTENT       0.36      0.36      0.36      9817\n",
      "       I-CUE       0.02      0.01      0.01       276\n",
      "    I-SOURCE       0.08      0.03      0.05      1671\n",
      "           O       0.64      0.71      0.68     20439\n",
      "\n",
      "    accuracy                           0.53     34059\n",
      "   macro avg       0.20      0.18      0.18     34059\n",
      "weighted avg       0.50      0.53      0.52     34059\n",
      "\n",
      "[[   31    15     4   196     5    12   364]\n",
      " [    5    46     3   203     3    16   374]\n",
      " [    8     7    19   179     1    10   355]\n",
      " [  128   115    99  3495    69   216  5695]\n",
      " [    5     4     1    97     3     1   165]\n",
      " [   22    17    16   498     8    57  1053]\n",
      " [  173   182   124  4931   105   378 14546]]\n",
      "----------------------------------------------------------\n",
      "\n",
      "Baseline evaluation parc3-polnear BIO with parc3's train and polnear's dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.56      0.20      0.30      2193\n",
      "       B-CUE       0.79      0.27      0.40      2190\n",
      "    B-SOURCE       0.76      0.22      0.35      1948\n",
      "   I-CONTENT       0.82      0.40      0.54     36902\n",
      "       I-CUE       0.30      0.02      0.04      1808\n",
      "    I-SOURCE       0.68      0.16      0.27      4070\n",
      "           O       0.50      0.92      0.64     32458\n",
      "\n",
      "    accuracy                           0.58     81569\n",
      "   macro avg       0.63      0.31      0.36     81569\n",
      "weighted avg       0.67      0.58      0.54     81569\n",
      "\n",
      "[[  440     1    34   207    12    10  1489]\n",
      " [    0   588     0   161     2    20  1419]\n",
      " [   14     2   437   160     0    14  1321]\n",
      " [  206   106    60 14933    52   110 21435]\n",
      " [   18    35     3   147    34    21  1550]\n",
      " [    1     2    21   167     0   671  3208]\n",
      " [  103    14    22  2343    12   135 29829]]\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/python-read-csv-columns-into-list/\n",
    "# to write the annotations and predictions into lists\n",
    "\n",
    "# To get the predictions\n",
    "result_list=[\"CRF_out_BIO_polnear-parc3.csv\",\"CRF_out_BIO_parc3-polnear.csv\"]\n",
    "for i in result_list:\n",
    "    filename = open(f'Result/{i}', 'r')\n",
    "    file = csv.DictReader(filename, delimiter='\\t')\n",
    "    four_class_system_output = []\n",
    "    tokens=[]\n",
    "    for col in file:\n",
    "        #print(col)\n",
    "        four_class_system_output.append(col['pred_AR_label'])\n",
    "        tokens.append(col['Word'])\n",
    "    #print(four_class_system_output[0:20])\n",
    "    # To get the annotations\n",
    "    data_source=i.replace('.csv','').split(\"_\")[-1]\n",
    "    train_data=data_source.split(\"-\")[0]\n",
    "    dev_data=data_source.split(\"-\")[1]  \n",
    "    filename1 = open(f'../Preprocessed_data/{dev_data}_preprocessed_BIO_dev_token_label.csv', 'r')\n",
    "    file1 = csv.DictReader(filename1, delimiter='\\t')\n",
    "    four_class_human_annotation = []\n",
    "    for col in file1:\n",
    "        #print(col)\n",
    "        four_class_human_annotation.append(col['AR_label'])\n",
    "    # to check if the predicted file and the dev file are aligned.\n",
    "    #print(f\"{data_source} {len(four_class_human_annotation)} and {len(four_class_system_output)}\")\n",
    "    report = classification_report(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output,digits = 2)\n",
    "    print(f\"Baseline evaluation {data_source} BIO with {train_data}'s train and {dev_data}'s dev files\")\n",
    "    print(report)\n",
    "    print(sklearn.metrics.confusion_matrix(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output))\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1 Evaluation: train and dev data from the same source. unlabeled sentences removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation polnear BIO with polnear's train and dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.59      0.44      0.51      2192\n",
      "       B-CUE       0.82      0.57      0.67      2189\n",
      "    B-SOURCE       0.73      0.50      0.59      1948\n",
      "   I-CONTENT       0.75      0.85      0.80     36895\n",
      "       I-CUE       0.44      0.27      0.33      1807\n",
      "    I-SOURCE       0.64      0.51      0.56      4070\n",
      "           O       0.60      0.54      0.57     16545\n",
      "\n",
      "    accuracy                           0.70     65646\n",
      "   macro avg       0.65      0.53      0.58     65646\n",
      "weighted avg       0.69      0.70      0.69     65646\n",
      "\n",
      "[[  970     3    59   746    85    33   296]\n",
      " [    0  1240     5   642    17    47   238]\n",
      " [   54     4   973   537    10    56   314]\n",
      " [  406   130    91 31403   320   362  4183]\n",
      " [   43    53     2   822   484    74   329]\n",
      " [   12    16    81  1147    26  2060   728]\n",
      " [  164    72   114  6470   150   595  8980]]\n",
      "----------------------------------------------------------\n",
      "\n",
      "Baseline evaluation parc3 BIO with parc3's train and dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.11      0.10      0.10       627\n",
      "       B-CUE       0.14      0.10      0.12       650\n",
      "    B-SOURCE       0.10      0.07      0.08       579\n",
      "   I-CONTENT       0.57      0.66      0.61      9817\n",
      "       I-CUE       0.09      0.03      0.04       276\n",
      "    I-SOURCE       0.23      0.17      0.20      1671\n",
      "           O       0.31      0.28      0.29      4498\n",
      "\n",
      "    accuracy                           0.45     18118\n",
      "   macro avg       0.22      0.20      0.21     18118\n",
      "weighted avg       0.42      0.45      0.43     18118\n",
      "\n",
      "[[  60   19   17  358    3   42  128]\n",
      " [  16   66   19  371    2   41  135]\n",
      " [  21   11   40  338    2   37  130]\n",
      " [ 264  235  215 6467   49  619 1968]\n",
      " [  11    7    3  183    8   18   46]\n",
      " [  42   36   31  860    8  291  403]\n",
      " [ 122   92   80 2691   21  235 1257]]\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/python-read-csv-columns-into-list/\n",
    "# to write the annotations and predictions into lists\n",
    "\n",
    "# To get the predictions\n",
    "result_list=[\"CRF_out_BIO_polnear_removed.csv\",\"CRF_out_BIO_parc3_removed.csv\"]\n",
    "for i in result_list:\n",
    "    filename = open(f'Result/{i}', 'r')\n",
    "    file = csv.DictReader(filename, delimiter='\\t')\n",
    "    four_class_system_output = []\n",
    "    tokens=[]\n",
    "    for col in file:\n",
    "        #print(col)\n",
    "        four_class_system_output.append(col['pred_AR_label'])\n",
    "        tokens.append(col['Word'])\n",
    "    #print(four_class_system_output[0:20])\n",
    "    # To get the annotations\n",
    "    data_source=i.replace('.csv','').split(\"_\")[-2]\n",
    "    #train_data=data_source.split(\"-\")[0]\n",
    "    #dev_data=data_source.split(\"-\")[1]  \n",
    "    filename1 = open(f'../Preprocessed_data/{data_source}_preprocessed_BIO_dev_token_label_removed.csv', 'r')\n",
    "    file1 = csv.DictReader(filename1, delimiter='\\t')\n",
    "    four_class_human_annotation = []\n",
    "    for col in file1:\n",
    "        #print(col)\n",
    "        four_class_human_annotation.append(col['AR_label'])\n",
    "    # to check if the predicted file and the dev file are aligned.\n",
    "    #print(f\"{data_source} {len(four_class_human_annotation)} and {len(four_class_system_output)}\")\n",
    "    report = classification_report(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output,digits = 2)\n",
    "    print(f\"Baseline evaluation {data_source} BIO with {data_source}'s train and dev files\")\n",
    "    print(report)\n",
    "    print(sklearn.metrics.confusion_matrix(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output))\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "3.2.2 Evaluation: train and dev data from the different source. unlabeled sentences removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation polnear-parc3 BIO with polnear's train and parc3's dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.10      0.06      0.08       627\n",
      "       B-CUE       0.13      0.07      0.09       650\n",
      "    B-SOURCE       0.09      0.05      0.06       579\n",
      "   I-CONTENT       0.57      0.61      0.59      9817\n",
      "       I-CUE       0.04      0.03      0.03       276\n",
      "    I-SOURCE       0.17      0.08      0.11      1671\n",
      "           O       0.30      0.37      0.33      4498\n",
      "\n",
      "    accuracy                           0.44     18118\n",
      "   macro avg       0.20      0.18      0.19     18118\n",
      "weighted avg       0.41      0.44      0.42     18118\n",
      "\n",
      "[[  40   16   16  353    4   20  178]\n",
      " [  12   48   11  362    4   22  191]\n",
      " [  20    9   27  317   11   19  176]\n",
      " [ 200  186  151 6026   87  415 2752]\n",
      " [   7    6    6  156    7    7   87]\n",
      " [  35   29   25  881   16  133  552]\n",
      " [  83   88   62 2386   50  156 1673]]\n",
      "----------------------------------------------------------\n",
      "\n",
      "Baseline evaluation parc3-polnear BIO with parc3's train and polnear's dev files\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-CONTENT       0.45      0.37      0.40      2192\n",
      "       B-CUE       0.75      0.47      0.57      2189\n",
      "    B-SOURCE       0.70      0.40      0.51      1948\n",
      "   I-CONTENT       0.73      0.81      0.77     36895\n",
      "       I-CUE       0.46      0.08      0.14      1807\n",
      "    I-SOURCE       0.63      0.41      0.50      4070\n",
      "           O       0.51      0.53      0.52     16545\n",
      "\n",
      "    accuracy                           0.66     65646\n",
      "   macro avg       0.60      0.44      0.49     65646\n",
      "weighted avg       0.65      0.66      0.65     65646\n",
      "\n",
      "[[  808     3    70   809    20    33   449]\n",
      " [    6  1019     2   708     7    59   388]\n",
      " [  111     7   777   570     2    41   440]\n",
      " [  488   213   141 30057   118   359  5519]\n",
      " [   66    59     2   897   149    61   573]\n",
      " [   11    12    38  1237     4  1685  1083]\n",
      " [  315    50    76  6870    27   448  8759]]\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/python-read-csv-columns-into-list/\n",
    "# to write the annotations and predictions into lists\n",
    "\n",
    "# To get the predictions\n",
    "result_list=[\"CRF_out_BIO_polnear-parc3_removed.csv\",\"CRF_out_BIO_parc3-polnear_removed.csv\"]\n",
    "for i in result_list:\n",
    "    filename = open(f'Result/{i}', 'r')\n",
    "    file = csv.DictReader(filename, delimiter='\\t')\n",
    "    four_class_system_output = []\n",
    "    tokens=[]\n",
    "    for col in file:\n",
    "        #print(col)\n",
    "        four_class_system_output.append(col['pred_AR_label'])\n",
    "        tokens.append(col['Word'])\n",
    "    #print(four_class_system_output[0:20])\n",
    "    # To get the annotations\n",
    "    data_source=i.replace('.csv','').split(\"_\")[-2]\n",
    "    train_data=data_source.split(\"-\")[0]\n",
    "    dev_data=data_source.split(\"-\")[1]  \n",
    "    filename1 = open(f'../Preprocessed_data/{dev_data}_preprocessed_BIO_dev_token_label_removed.csv', 'r')\n",
    "    file1 = csv.DictReader(filename1, delimiter='\\t')\n",
    "    four_class_human_annotation = []\n",
    "    for col in file1:\n",
    "        #print(col)\n",
    "        four_class_human_annotation.append(col['AR_label'])\n",
    "    # to check if the predicted file and the dev file are aligned.\n",
    "    #print(f\"{data_source} {len(four_class_human_annotation)} and {len(four_class_system_output)}\")\n",
    "    report = classification_report(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output,digits = 2)\n",
    "    print(f\"Baseline evaluation {data_source} BIO with {train_data}'s train and {dev_data}'s dev files\")\n",
    "    print(report)\n",
    "    print(sklearn.metrics.confusion_matrix(four_class_human_annotation[:len(four_class_system_output)],four_class_system_output))\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_py37_venv",
   "language": "python",
   "name": "jupyter_py37_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
