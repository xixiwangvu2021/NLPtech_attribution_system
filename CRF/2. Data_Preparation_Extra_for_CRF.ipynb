{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data_Preparation_Extra_for_CRF\n",
    "\n",
    "To extract extra features and labels for running the CRF models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_set = 'dev'\n",
    "preprocessed_data=[\n",
    "    f'polnear_with_BIO_{eval_data_set}.csv', \n",
    "    \"polnear_with_BIO_train.csv\",\n",
    "    f'parc3_with_BIO_{eval_data_set}.csv', \n",
    "    \"parc3_with_BIO_train.csv\",\n",
    "    f'merged_with_BIO_{eval_data_set}.csv', \n",
    "    \"merged_with_BIO_train.csv\",\n",
    "]\n",
    "\n",
    "# This line after is for FeaturesCRF\n",
    "filename_addition = '_extra_data.csv'\n",
    "# This line after is for Baseline CRF and EmbeddingCRF\n",
    "# filename_addition = '.csv'\n",
    "removed_filename_addition = '_removed' + filename_addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sentences = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda f: [(a, s, nf, ns, ft, w, l, p, d, td, al) \n",
    "                              for a, s, nf, ns, ft, w, l, p, d, td, al \n",
    "                              in zip(f[\"Article_Name\"].values.tolist(),\n",
    "                                     f[\"Sentence_nr\"].values.tolist(),\n",
    "                                     f[\"Nr_in_file\"].values.tolist(),\n",
    "                                     f[\"Nr_in_sentence\"].values.tolist(),\n",
    "                                     f[\"FromTo\"].values.tolist(),\n",
    "                                     f[\"Word\"].values.tolist(),\n",
    "                                     f[\"Lemma\"].values.tolist(),\n",
    "                                     f[\"POS\"].values.tolist(),\n",
    "                                     f[\"Dep_label\"].values.tolist(),\n",
    "                                     f[\"Token_dep_head\"].values.tolist(),\n",
    "                                     f[\"AR_label\"].values.tolist(),\n",
    "                                    )]\n",
    "        self.grouped = self.dataset.groupby([\"Article_Name\", \"Sentence_nr\"]).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentences: {}\".format(self.n_tokens)]\n",
    "            self.n_sentences += 1\n",
    "            return f\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceExtraGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sentences = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda f: [(a, s, nf, ns, ft, w, l, p, d, td, al, q, c, dd, dp) \n",
    "                              for a, s, nf, ns, ft, w, l, p, d, td, al, q, c, dd, dp\n",
    "                              in zip(f[\"Article_Name\"].values.tolist(),\n",
    "                                     f[\"Sentence_nr\"].values.tolist(),\n",
    "                                     f[\"Nr_in_file\"].values.tolist(),\n",
    "                                     f[\"Nr_in_sentence\"].values.tolist(),\n",
    "                                     f[\"FromTo\"].values.tolist(),\n",
    "                                     f[\"Word\"].values.tolist(),\n",
    "                                     f[\"Lemma\"].values.tolist(),\n",
    "                                     f[\"POS\"].values.tolist(),\n",
    "                                     f[\"Dep_label\"].values.tolist(),\n",
    "                                     f[\"Token_dep_head\"].values.tolist(),\n",
    "                                     f[\"AR_label\"].values.tolist(),\n",
    "                                     f[\"In_quote\"].values.tolist(),\n",
    "                                     f[\"After_colon\"].values.tolist(),\n",
    "                                     f[\"Dep_distance\"].values.tolist(),\n",
    "                                     f[\"Dep_path\"].values.tolist(),\n",
    "                                    )]\n",
    "        self.grouped = self.dataset.groupby([\"Article_Name\", \"Sentence_nr\"]).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentences: {}\".format(self.n_tokens)]\n",
    "            self.n_sentences += 1\n",
    "            return f\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def generate_graph(words):\n",
    "    edges = []\n",
    "    root_position = 0\n",
    "    for word in words:\n",
    "#         if word != 'Sent_end':\n",
    "        word_columns = list(word)\n",
    "        position = word_columns[3]\n",
    "        parent = word_columns[9]\n",
    "        dependency_label = word_columns[8]\n",
    "#         if dependency_label.lower() != 'root':\n",
    "        if parent != '0':\n",
    "            edges.append((position, parent))\n",
    "        else:\n",
    "            root_position = position\n",
    "            \n",
    "#     print(edges)\n",
    "    sentence_graph = nx.Graph(edges)\n",
    "    return sentence_graph, root_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_path(shortest_path, words):\n",
    "    dependencies_path = []\n",
    "    for path_word_position in shortest_path:\n",
    "#         print(f\"path_word_position is {path_word_position}.\")\n",
    "        for word in words:\n",
    "            word_columns = list(word)\n",
    "            word_position = word_columns[3]\n",
    "            if word_position == path_word_position:\n",
    "                dependency_label = word_columns[8]\n",
    "                dependencies_path.append(dependency_label)\n",
    "    return ';'.join(dependencies_path[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    if x:\n",
    "        x = str(x).split('.')[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_lemmas = ['say', 'be', 'to', 'have', 'tell', 'call', 'write', 'accord', 'add', 'ask', 'show', 'support', 'note', 'report', 'suggest', 'argue',\n",
    "             'expect', 'report', 'believe', 'agree', 'think', 'announce', 'cite', 'suggest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polnear_with_BIO_dev.csv\n",
      "[('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 1.0, 1.0, '0,7', 'Stealth', 'Stealth', 'NNP', 'compound', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 2.0, 2.0, '8,12', 'Over', 'Over', 'NNP', 'compound', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 3.0, 3.0, '13,19', 'Health', 'Health', 'NNP', 'root', 0.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 4.0, 4.0, '19,20', ':', ':', ':', 'punct', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 5.0, 5.0, '21,28', 'Hillary', 'Hillary', 'NNP', 'compound', 6.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 6.0, 6.0, '29,36', 'Clinton', 'Clinton', 'NNP', 'nsubjpass', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 7.0, 7.0, '37,40', 'Was', 'be', 'VBD', 'auxpass', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 8.0, 8.0, '41,47', 'Headed', 'head', 'VBN', 'dep', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 9.0, 9.0, '48,50', 'to', 'to', 'TO', 'case', 10.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 10.0, 10.0, '51,53', 'ER', 'er', 'NN', 'nmod', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 11.0, 11.0, '54,57', 'But', 'but', 'CC', 'cc', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 12.0, 12.0, '58,66', 'Diverted', 'divert', 'VBN', 'conj', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 13.0, 13.0, '67,69', 'to', 'to', 'TO', 'mark', 14.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 14.0, 14.0, '70,75', 'Avoid', 'avoid', 'VB', 'xcomp', 12.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 15.0, 15.0, '76,79', 'Bad', 'bad', 'JJ', 'amod', 16.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 16.0, 16.0, '80,86', 'Optics', 'optics', 'NNS', 'dobj', 14.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 17.0, 17.0, '86,87', '.', '.', '.', 'punct', 3.0, 'O')]\n",
      "[('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 1.0, 1.0, '0,7', 'Stealth', 'Stealth', 'NNP', 'compound', 3.0, 'O', False, False, 3, 'compound')]\n",
      "polnear_with_BIO_train.csv\n",
      "[('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 1.0, 1.0, '0,3', 'The', 'the', 'DT', 'det', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 2.0, 2.0, '4,9', 'Ninth', 'Ninth', 'NNP', 'compound', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 3.0, 3.0, '10,16', 'Circle', 'Circle', 'NNP', 'root', 0.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 4.0, 4.0, '16,17', ':', ':', ':', 'punct', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 5.0, 5.0, '18,21', 'The', 'the', 'DT', 'det', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 6.0, 6.0, '22,29', 'Hellish', 'hellish', 'JJ', 'amod', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 7.0, 7.0, '30,34', 'View', 'View', 'NNP', 'dep', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 8.0, 8.0, '35,39', 'from', 'from', 'IN', 'case', 11.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 9.0, 9.0, '40,46', 'Inside', 'inside', 'IN', 'case', 11.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 10.0, 10.0, '47,50', 'the', 'the', 'DT', 'det', 11.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 11.0, 11.0, '51,58', 'Beltway', 'Beltway', 'NNP', 'nmod', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 12.0, 12.0, '58,59', ',', ',', ',', 'punct', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 13.0, 13.0, '60,61', '#', '#', '#', 'dep', 14.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 14.0, 14.0, '61,62', '2', '2', 'CD', 'appos', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 15.0, 15.0, '62,63', '.', '.', '.', 'punct', 3.0, 'O')]\n",
      "[('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 1.0, 1.0, '0,3', 'The', 'the', 'DT', 'det', 3.0, 'O', False, False, 3, 'det')]\n",
      "parc3_with_BIO_dev.csv\n",
      "[('wsj_2400.xml', 1.0, 1.0, 1.0, '9,12', 'The', 'the', 'DT', 'det', 2.0, 'O'), ('wsj_2400.xml', 1.0, 2.0, 2.0, '13,20', 'economy', 'economy', 'NN', 'nmod:poss', 4.0, 'O'), ('wsj_2400.xml', 1.0, 3.0, 3.0, '20,22', \"'s\", \"'s\", 'POS', 'case', 2.0, 'O'), ('wsj_2400.xml', 1.0, 4.0, 4.0, '23,34', 'temperature', 'temperature', 'NN', 'nsubjpass', 7.0, 'O'), ('wsj_2400.xml', 1.0, 5.0, 5.0, '35,39', 'will', 'will', 'MD', 'aux', 7.0, 'O'), ('wsj_2400.xml', 1.0, 6.0, 6.0, '40,42', 'be', 'be', 'VB', 'auxpass', 7.0, 'O'), ('wsj_2400.xml', 1.0, 7.0, 7.0, '43,48', 'taken', 'take', 'VBN', 'root', 0.0, 'O'), ('wsj_2400.xml', 1.0, 8.0, 8.0, '49,53', 'from', 'from', 'IN', 'case', 11.0, 'O'), ('wsj_2400.xml', 1.0, 9.0, 9.0, '54,61', 'several', 'several', 'JJ', 'amod', 11.0, 'O'), ('wsj_2400.xml', 1.0, 10.0, 10.0, '62,69', 'vantage', 'vantage', 'NN', 'compound', 11.0, 'O'), ('wsj_2400.xml', 1.0, 11.0, 11.0, '70,76', 'points', 'point', 'NNS', 'nmod', 7.0, 'O'), ('wsj_2400.xml', 1.0, 12.0, 12.0, '77,81', 'this', 'this', 'DT', 'det', 13.0, 'O'), ('wsj_2400.xml', 1.0, 13.0, 13.0, '82,86', 'week', 'week', 'NN', 'nmod:tmod', 7.0, 'O'), ('wsj_2400.xml', 1.0, 14.0, 14.0, '86,87', ',', ',', ',', 'punct', 7.0, 'O'), ('wsj_2400.xml', 1.0, 15.0, 15.0, '88,92', 'with', 'with', 'IN', 'case', 16.0, 'O'), ('wsj_2400.xml', 1.0, 16.0, 16.0, '93,101', 'readings', 'reading', 'NNS', 'nmod', 7.0, 'O'), ('wsj_2400.xml', 1.0, 17.0, 17.0, '102,104', 'on', 'on', 'IN', 'case', 18.0, 'O'), ('wsj_2400.xml', 1.0, 18.0, 18.0, '105,110', 'trade', 'trade', 'NN', 'nmod', 16.0, 'O'), ('wsj_2400.xml', 1.0, 19.0, 19.0, '110,111', ',', ',', ',', 'punct', 18.0, 'O'), ('wsj_2400.xml', 1.0, 20.0, 20.0, '112,118', 'output', 'output', 'NN', 'conj', 18.0, 'O'), ('wsj_2400.xml', 1.0, 21.0, 21.0, '118,119', ',', ',', ',', 'punct', 18.0, 'O'), ('wsj_2400.xml', 1.0, 22.0, 22.0, '120,127', 'housing', 'housing', 'NN', 'conj', 18.0, 'O'), ('wsj_2400.xml', 1.0, 23.0, 23.0, '128,131', 'and', 'and', 'CC', 'cc', 18.0, 'O'), ('wsj_2400.xml', 1.0, 24.0, 24.0, '132,141', 'inflation', 'inflation', 'NN', 'conj', 18.0, 'O'), ('wsj_2400.xml', 1.0, 25.0, 25.0, '141,142', '.', '.', '.', 'punct', 7.0, 'O')]\n",
      "[('wsj_2400.xml', 1.0, 1.0, 1.0, '9,12', 'The', 'the', 'DT', 'det', 2.0, 'O', False, False, 5, 'det;nmod:poss;nsubjpass')]\n",
      "parc3_with_BIO_train.csv\n",
      "[('wsj_0001.xml', 1.0, 1.0, 1.0, '9,15', 'Pierre', 'Pierre', 'NNP', 'compound', 2.0, 'O'), ('wsj_0001.xml', 1.0, 2.0, 2.0, '16,22', 'Vinken', 'Vinken', 'NNP', 'nsubj', 9.0, 'O'), ('wsj_0001.xml', 1.0, 3.0, 3.0, '22,23', ',', ',', ',', 'punct', 2.0, 'O'), ('wsj_0001.xml', 1.0, 4.0, 4.0, '24,26', '61', '61', 'CD', 'nummod', 5.0, 'O'), ('wsj_0001.xml', 1.0, 5.0, 5.0, '27,32', 'years', 'year', 'NNS', 'nmod:npmod', 6.0, 'O'), ('wsj_0001.xml', 1.0, 6.0, 6.0, '33,36', 'old', 'old', 'JJ', 'amod', 2.0, 'O'), ('wsj_0001.xml', 1.0, 7.0, 7.0, '36,37', ',', ',', ',', 'punct', 2.0, 'O'), ('wsj_0001.xml', 1.0, 8.0, 8.0, '38,42', 'will', 'will', 'MD', 'aux', 9.0, 'O'), ('wsj_0001.xml', 1.0, 9.0, 9.0, '43,47', 'join', 'join', 'VB', 'root', 0.0, 'O'), ('wsj_0001.xml', 1.0, 10.0, 10.0, '48,51', 'the', 'the', 'DT', 'det', 11.0, 'O'), ('wsj_0001.xml', 1.0, 11.0, 11.0, '52,57', 'board', 'board', 'NN', 'dobj', 9.0, 'O'), ('wsj_0001.xml', 1.0, 12.0, 12.0, '58,60', 'as', 'as', 'IN', 'case', 15.0, 'O'), ('wsj_0001.xml', 1.0, 13.0, 13.0, '61,62', 'a', 'a', 'DT', 'det', 15.0, 'O'), ('wsj_0001.xml', 1.0, 14.0, 14.0, '63,75', 'nonexecutive', 'nonexecutive', 'JJ', 'amod', 15.0, 'O'), ('wsj_0001.xml', 1.0, 15.0, 15.0, '76,84', 'director', 'director', 'NN', 'nmod', 9.0, 'O'), ('wsj_0001.xml', 1.0, 16.0, 16.0, '85,89', 'Nov.', 'Nov.', 'NNP', 'nmod:tmod', 9.0, 'O'), ('wsj_0001.xml', 1.0, 17.0, 17.0, '90,92', '29', '29', 'CD', 'nummod', 16.0, 'O'), ('wsj_0001.xml', 1.0, 18.0, 18.0, '92,93', '.', '.', '.', 'punct', 9.0, 'O')]\n",
      "[('wsj_0001.xml', 1.0, 1.0, 1.0, '9,15', 'Pierre', 'Pierre', 'NNP', 'compound', 2.0, 'O', False, False, 4, 'compound;nsubj')]\n",
      "merged_with_BIO_dev.csv\n",
      "[('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 1.0, 1.0, '0,7', 'Stealth', 'Stealth', 'NNP', 'compound', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 2.0, 2.0, '8,12', 'Over', 'Over', 'NNP', 'compound', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 3.0, 3.0, '13,19', 'Health', 'Health', 'NNP', 'root', 0.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 4.0, 4.0, '19,20', ':', ':', ':', 'punct', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 5.0, 5.0, '21,28', 'Hillary', 'Hillary', 'NNP', 'compound', 6.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 6.0, 6.0, '29,36', 'Clinton', 'Clinton', 'NNP', 'nsubjpass', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 7.0, 7.0, '37,40', 'Was', 'be', 'VBD', 'auxpass', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 8.0, 8.0, '41,47', 'Headed', 'head', 'VBN', 'dep', 3.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 9.0, 9.0, '48,50', 'to', 'to', 'TO', 'case', 10.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 10.0, 10.0, '51,53', 'ER', 'er', 'NN', 'nmod', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 11.0, 11.0, '54,57', 'But', 'but', 'CC', 'cc', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 12.0, 12.0, '58,66', 'Diverted', 'divert', 'VBN', 'conj', 8.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 13.0, 13.0, '67,69', 'to', 'to', 'TO', 'mark', 14.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 14.0, 14.0, '70,75', 'Avoid', 'avoid', 'VB', 'xcomp', 12.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 15.0, 15.0, '76,79', 'Bad', 'bad', 'JJ', 'amod', 16.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 16.0, 16.0, '80,86', 'Optics', 'optics', 'NNS', 'dobj', 14.0, 'O'), ('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 17.0, 17.0, '86,87', '.', '.', '.', 'punct', 3.0, 'O')]\n",
      "[('breitbart_2016-09-12_stealth-over-health-hillary-clin.txt.xml', 1.0, 1.0, 1.0, '0,7', 'Stealth', 'Stealth', 'NNP', 'compound', 3.0, 'O', False, False, 3, 'compound')]\n",
      "merged_with_BIO_train.csv\n",
      "[('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 1.0, 1.0, '0,3', 'The', 'the', 'DT', 'det', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 2.0, 2.0, '4,9', 'Ninth', 'Ninth', 'NNP', 'compound', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 3.0, 3.0, '10,16', 'Circle', 'Circle', 'NNP', 'root', 0.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 4.0, 4.0, '16,17', ':', ':', ':', 'punct', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 5.0, 5.0, '18,21', 'The', 'the', 'DT', 'det', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 6.0, 6.0, '22,29', 'Hellish', 'hellish', 'JJ', 'amod', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 7.0, 7.0, '30,34', 'View', 'View', 'NNP', 'dep', 3.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 8.0, 8.0, '35,39', 'from', 'from', 'IN', 'case', 11.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 9.0, 9.0, '40,46', 'Inside', 'inside', 'IN', 'case', 11.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 10.0, 10.0, '47,50', 'the', 'the', 'DT', 'det', 11.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 11.0, 11.0, '51,58', 'Beltway', 'Beltway', 'NNP', 'nmod', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 12.0, 12.0, '58,59', ',', ',', ',', 'punct', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 13.0, 13.0, '60,61', '#', '#', '#', 'dep', 14.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 14.0, 14.0, '61,62', '2', '2', 'CD', 'appos', 7.0, 'O'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 15.0, 15.0, '62,63', '.', '.', '.', 'punct', 3.0, 'O')]\n",
      "[('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1.0, 1.0, 1.0, '0,3', 'The', 'the', 'DT', 'det', 3.0, 'O', False, False, 3, 'det')]\n"
     ]
    }
   ],
   "source": [
    "# To mark down this cell for baseline and embedding\n",
    "# to extract extra features, to prepare for using CRF.py, not removing unlabeled sentences\n",
    "\n",
    "def get_extra_features(filename):\n",
    "    quote_opening = \"``\"\n",
    "    quote_closing = \"''\"\n",
    "    colon_opening = ':'\n",
    "    colon_closing = '.'\n",
    "\n",
    "    ### Read file\n",
    "    input_file = f\"Preprocessed_data/{filename}\"\n",
    "#     csvinput = open(f\"Preprocessed_data/{filename}\", 'r', encoding=\"utf-8\")\n",
    "#     csvreader = csv.reader(csvinput, delimiter='\\t')\n",
    "# #     print(csvreader)\n",
    "#     headers = next(csvreader)\n",
    "#     sents = []\n",
    "#     current_sent = []\n",
    "    \n",
    "#     # Try except for nul byte error\n",
    "#     try:\n",
    "#         index = 0\n",
    "#         for i, row in enumerate(csvreader):\n",
    "#             index = i\n",
    "#             current_sent.append(list(row))\n",
    "#             if row[5] == \"Sent_end\":\n",
    "#                 sents.append(current_sent)\n",
    "#                 current_sent = []\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(index)\n",
    "#         pass\n",
    "    df_input = pd.read_csv(input_file, sep='|', encoding=\"utf-8\")\n",
    "#     print(df_input.head(2))\n",
    "#     print(df_input.tail(2))\n",
    "    getter = SentenceGetter(df_input)\n",
    "    sents = getter.sentences\n",
    "#     sents = df_input.to_records(index=False)\n",
    "#     sents = list(sents)\n",
    "    print(sents[0])\n",
    "        \n",
    "    ### Loop over sentences in file to get extra features for the CRF feature models\n",
    "    in_quote = False\n",
    "    after_colon = False\n",
    "    sents_extra_features = []\n",
    "    for sent in sents:\n",
    "#         print(type(sent))\n",
    "\n",
    "        # Generate graph\n",
    "        sentence_graph, root_position = generate_graph(sent)  \n",
    "\n",
    "        # Loop over tokens in sentence\n",
    "        sent_extra_features = []\n",
    "        for token_idx, token in enumerate(sent):\n",
    "            token_extra_features = token\n",
    "            word = token[5]\n",
    "            postag = token[7]\n",
    "            word_position = token[3]\n",
    "            \n",
    "            if word == quote_opening:\n",
    "                in_quote = True\n",
    "#             if postag == quote_opening:\n",
    "#                 in_quote = True\n",
    "            \n",
    "            if word == colon_opening:\n",
    "                after_colon = True\n",
    "            if word == colon_closing:\n",
    "                after_colon = False\n",
    "                \n",
    "            dependency_distance = 0\n",
    "            dependency_path = ''\n",
    "            if word != 'Sent_end':\n",
    "                try:\n",
    "                    shortest_path = nx.shortest_path(sentence_graph, source=word_position, target=root_position)\n",
    "#                     print(f\"Shortest path is {shortest_path}.\")\n",
    "                    dependency_distance = len(shortest_path)\n",
    "                    dependency_path = get_dependency_path(shortest_path, sent)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "#                     print(e)\n",
    "#                     print(sent)\n",
    "#                     print('root_position')   \n",
    "#                     print(root_position)\n",
    "#                     print('word_position')   \n",
    "#                     print(word_position)\n",
    "#                     print(word)\n",
    "            \n",
    "            token_extra_features = (token[0], token[1], token[2], token[3], token[4], token[5], \n",
    "                                    token[6], token[7], token[8], token[9], token[10], \n",
    "                                    in_quote, after_colon, dependency_distance, dependency_path)  # Tuple\n",
    "            sent_extra_features.append(token_extra_features)\n",
    "            \n",
    "            if word == quote_closing:\n",
    "                in_quote = False\n",
    "#             if postag == quote_closing:\n",
    "#                 in_quote = False\n",
    "            \n",
    "        # Append row with extra features to new list\n",
    "        sents_extra_features.append(sent_extra_features)\n",
    "            \n",
    "    ### Write file\n",
    "#     with open(f\"Preprocessed_data/{filename.replace('.csv', filename_addition)}\", \"w\", encoding=\"utf-8\") as result:\n",
    "#         headers1 = [\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTo\", \"Word\", \"Lemma\", \"POS\", \"Dep_label\", \"Token_dep_head\", \n",
    "#                     \"AR_label\", \"In_quote\", \"After_colon\", \"Dep_distance\", \"Dep_path\"]\n",
    "#         writer = csv.writer(result, delimiter='\\t')\n",
    "#         writer.writerow(headers1)\n",
    "#         for f in sents_extra_features:\n",
    "#             writer.writerows(f)\n",
    "    file_extra_features = [token for tokens in sents_extra_features for token in tokens]\n",
    "    print(file_extra_features[0:1])\n",
    "    df_file = pd.DataFrame(file_extra_features, columns=[\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTo\", \"Word\", \"Lemma\", \"POS\", \n",
    "                                                         \"Dep_label\", \"Token_dep_head\", \"AR_label\", \"In_quote\", \"After_colon\", \"Dep_distance\", \"Dep_path\"])\n",
    "    df_file['Sentence_nr'] = df_file['Sentence_nr'].apply(convert)\n",
    "    df_file['Nr_in_file'] = df_file['Sentence_nr'].apply(convert)\n",
    "    df_file['Nr_in_sentence'] = df_file['Sentence_nr'].apply(convert)\n",
    "    df_file['Token_dep_head'] = df_file['Token_dep_head'].apply(convert)\n",
    "    df_file.head(2)\n",
    "    df_file.tail(2)\n",
    "    output_file = f\"Preprocessed_data/{filename.replace('.csv', filename_addition)}\"\n",
    "    df_file.to_csv(output_file, sep='|', index=False, encoding=\"utf-8\")\n",
    "\n",
    "# For every filename, get extra features\n",
    "for filename in preprocessed_data:  # preprocessed_data_token_label:\n",
    "    print(filename)\n",
    "    get_extra_features(filename)  # .replace('.csv', filename_addition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '0,3', 'The', 'the', 'DT', 'det', 3.0, 'O', False, False, 3, 'det'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '4,9', 'Ninth', 'Ninth', 'NNP', 'compound', 3.0, 'O', False, False, 3, 'compound'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '10,16', 'Circle', 'Circle', 'NNP', 'root', 0.0, 'O', False, False, 2, nan), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '16,17', ':', ':', ':', 'punct', 3.0, 'O', False, True, 3, 'punct'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '18,21', 'The', 'the', 'DT', 'det', 7.0, 'O', False, True, 4, 'det;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '22,29', 'Hellish', 'hellish', 'JJ', 'amod', 7.0, 'O', False, True, 4, 'amod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '30,34', 'View', 'View', 'NNP', 'dep', 3.0, 'O', False, True, 3, 'dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '35,39', 'from', 'from', 'IN', 'case', 11.0, 'O', False, True, 5, 'case;nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '40,46', 'Inside', 'inside', 'IN', 'case', 11.0, 'O', False, True, 5, 'case;nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '47,50', 'the', 'the', 'DT', 'det', 11.0, 'O', False, True, 5, 'det;nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '51,58', 'Beltway', 'Beltway', 'NNP', 'nmod', 7.0, 'O', False, True, 4, 'nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '58,59', ',', ',', ',', 'punct', 7.0, 'O', False, True, 4, 'punct;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '60,61', '#', '#', '#', 'dep', 14.0, 'O', False, True, 5, 'dep;appos;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '61,62', '2', '2', 'CD', 'appos', 7.0, 'O', False, True, 4, 'appos;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '62,63', '.', '.', '.', 'punct', 3.0, 'O', False, False, 3, 'punct')]\n",
      "[('wsj_0001.xml', 1, 1, 1, '9,15', 'Pierre', 'Pierre', 'NNP', 'compound', 2.0, 'O', False, False, 4, 'compound;nsubj'), ('wsj_0001.xml', 1, 1, 1, '16,22', 'Vinken', 'Vinken', 'NNP', 'nsubj', 9.0, 'O', False, False, 3, 'nsubj'), ('wsj_0001.xml', 1, 1, 1, '22,23', ',', ',', ',', 'punct', 2.0, 'O', False, False, 4, 'punct;nsubj'), ('wsj_0001.xml', 1, 1, 1, '24,26', '61', '61', 'CD', 'nummod', 5.0, 'O', False, False, 6, 'nummod;nmod:npmod;amod;nsubj'), ('wsj_0001.xml', 1, 1, 1, '27,32', 'years', 'year', 'NNS', 'nmod:npmod', 6.0, 'O', False, False, 5, 'nmod:npmod;amod;nsubj'), ('wsj_0001.xml', 1, 1, 1, '33,36', 'old', 'old', 'JJ', 'amod', 2.0, 'O', False, False, 4, 'amod;nsubj'), ('wsj_0001.xml', 1, 1, 1, '36,37', ',', ',', ',', 'punct', 2.0, 'O', False, False, 4, 'punct;nsubj'), ('wsj_0001.xml', 1, 1, 1, '38,42', 'will', 'will', 'MD', 'aux', 9.0, 'O', False, False, 3, 'aux'), ('wsj_0001.xml', 1, 1, 1, '43,47', 'join', 'join', 'VB', 'root', 0.0, 'O', False, False, 2, nan), ('wsj_0001.xml', 1, 1, 1, '48,51', 'the', 'the', 'DT', 'det', 11.0, 'O', False, False, 4, 'det;dobj'), ('wsj_0001.xml', 1, 1, 1, '52,57', 'board', 'board', 'NN', 'dobj', 9.0, 'O', False, False, 3, 'dobj'), ('wsj_0001.xml', 1, 1, 1, '58,60', 'as', 'as', 'IN', 'case', 15.0, 'O', False, False, 4, 'case;nmod'), ('wsj_0001.xml', 1, 1, 1, '61,62', 'a', 'a', 'DT', 'det', 15.0, 'O', False, False, 4, 'det;nmod'), ('wsj_0001.xml', 1, 1, 1, '63,75', 'nonexecutive', 'nonexecutive', 'JJ', 'amod', 15.0, 'O', False, False, 4, 'amod;nmod'), ('wsj_0001.xml', 1, 1, 1, '76,84', 'director', 'director', 'NN', 'nmod', 9.0, 'O', False, False, 3, 'nmod'), ('wsj_0001.xml', 1, 1, 1, '85,89', 'Nov.', 'Nov.', 'NNP', 'nmod:tmod', 9.0, 'O', False, False, 3, 'nmod:tmod'), ('wsj_0001.xml', 1, 1, 1, '90,92', '29', '29', 'CD', 'nummod', 16.0, 'O', False, False, 4, 'nummod;nmod:tmod'), ('wsj_0001.xml', 1, 1, 1, '92,93', '.', '.', '.', 'punct', 9.0, 'O', False, False, 3, 'punct')]\n",
      "[('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '0,3', 'The', 'the', 'DT', 'det', 3.0, 'O', False, False, 3, 'det'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '4,9', 'Ninth', 'Ninth', 'NNP', 'compound', 3.0, 'O', False, False, 3, 'compound'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '10,16', 'Circle', 'Circle', 'NNP', 'root', 0.0, 'O', False, False, 2, nan), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '16,17', ':', ':', ':', 'punct', 3.0, 'O', False, True, 3, 'punct'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '18,21', 'The', 'the', 'DT', 'det', 7.0, 'O', False, True, 4, 'det;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '22,29', 'Hellish', 'hellish', 'JJ', 'amod', 7.0, 'O', False, True, 4, 'amod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '30,34', 'View', 'View', 'NNP', 'dep', 3.0, 'O', False, True, 3, 'dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '35,39', 'from', 'from', 'IN', 'case', 11.0, 'O', False, True, 5, 'case;nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '40,46', 'Inside', 'inside', 'IN', 'case', 11.0, 'O', False, True, 5, 'case;nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '47,50', 'the', 'the', 'DT', 'det', 11.0, 'O', False, True, 5, 'det;nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '51,58', 'Beltway', 'Beltway', 'NNP', 'nmod', 7.0, 'O', False, True, 4, 'nmod;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '58,59', ',', ',', ',', 'punct', 7.0, 'O', False, True, 4, 'punct;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '60,61', '#', '#', '#', 'dep', 14.0, 'O', False, True, 5, 'dep;appos;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '61,62', '2', '2', 'CD', 'appos', 7.0, 'O', False, True, 4, 'appos;dep'), ('breitbart_2015-11-11_the-ninth-circle-the-hellish-vie.txt.xml', 1, 1, 1, '62,63', '.', '.', '.', 'punct', 3.0, 'O', False, False, 3, 'punct')]\n"
     ]
    }
   ],
   "source": [
    "# to prepare for using CRF.py, removing unlabeled sentences\n",
    "\n",
    "def get_removed_file(filename):\n",
    "    ### Read file\n",
    "    \n",
    "    # This line after is for FeaturesCRF\n",
    "#     csvinput = open(f\"Preprocessed_data/{filename.replace('.csv', filename_addition)}\",'r', encoding=\"utf-8\")\n",
    "    # This line after is for Baseline CRF and EmbeddingCRF\n",
    "    csvinput = open(f\"Preprocessed_data/{filename}\",'r')\n",
    "\n",
    "#     csvreader = csv.reader(csvinput,delimiter='\\t')\n",
    "# #     print(csvreader)\n",
    "#     headers=next(csvreader)\n",
    "#     sents = []\n",
    "#     current_sent = []\n",
    "#     for row in csvreader:\n",
    "#         current_sent.append(list(row))\n",
    "#         if row[5] == \"Sent_end\":\n",
    "#             sents.append(current_sent)\n",
    "#             current_sent = []\n",
    "    df_input = pd.read_csv(csvinput, sep='|', encoding=\"utf-8\")\n",
    "    # This line after is for FeaturesCRF\n",
    "#     getter = SentenceExtraGetter(df_input)\n",
    "    # This line after is for Baseline CRF and EmbeddingCRF\n",
    "    getter = SentenceGetter(df_input)\n",
    "    sents = getter.sentences\n",
    "    print(sents[0])\n",
    "            \n",
    "    ### Create a new list with only the labeled sentences\n",
    "    sents_removed = []\n",
    "    for sent in sents:\n",
    "        sent_label_list = []\n",
    "        for f in sent:\n",
    "            label = f[10]\n",
    "            sent_label_list.append(label)\n",
    "        if set(sent_label_list) != {'O'}:\n",
    "            sents_removed.append(sent)\n",
    "            \n",
    "    ### Write file\n",
    "#     with open(f\"Preprocessed_data/{filename.replace('.csv', removed_filename_addition)}\", \"w\", encoding=\"utf-8\") as result:\n",
    "#         #this line after is for featuresCRF\n",
    "#         headers1 = [\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTo\", \"Word\", \"Lemma\", \"POS\", \"Dep_label\", \"Token_dep_head\", \n",
    "#                     \"AR_label\", \"In_quote\", \"After_colon\", \"Dep_distance\", \"Dep_path\"]\n",
    "#         #this line after is for baseline and embedding\n",
    "# #         headers1 = [\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTo\", \"Word\", \"Lemma\", \"POS\", \"Dep_label\", \"Token_dep_head\", \n",
    "# #                     \"AR_label\"]        \n",
    "#         writer = csv.writer(result, delimiter='\\t')\n",
    "#         writer.writerow(headers1)\n",
    "#         for f in sents_removed:\n",
    "#             writer.writerows(f)\n",
    "    # Transform the list of sentences back to a list of tokens again.\n",
    "    file_sents_removed = [token for tokens in sents_removed for token in tokens]\n",
    "    # This line after is for FeaturesCRF\n",
    "    df_file = pd.DataFrame(file_sents_removed, columns=[\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTo\", \"Word\", \"Lemma\", \"POS\", \n",
    "                                                         \"Dep_label\", \"Token_dep_head\", \"AR_label\", \"In_quote\", \"After_colon\", \"Dep_distance\", \"Dep_path\"])\n",
    "    # This line after is for Baseline CRF and EmbeddingCRF\n",
    "    df_file = pd.DataFrame(file_extra_features, columns=[\"Article_Name\", \"Sentence_nr\", \"Nr_in_file\", \"Nr_in_sentence\", \"FromTo\", \"Word\", \"Lemma\", \"POS\", \n",
    "                                                         \"Dep_label\", \"Token_dep_head\", \"AR_label\"])\n",
    "    df_file['Sentence_nr'] = df_file['Sentence_nr'].apply(convert)\n",
    "    df_file['Nr_in_file'] = df_file['Sentence_nr'].apply(convert)\n",
    "    df_file['Nr_in_sentence'] = df_file['Sentence_nr'].apply(convert)\n",
    "    df_file['Token_dep_head'] = df_file['Token_dep_head'].apply(convert)\n",
    "    output_file = f\"Preprocessed_data/{filename.replace('.csv', removed_filename_addition)}\"\n",
    "    df_file.to_csv(output_file, sep='|', index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "for filename in preprocessed_data:\n",
    "    if 'train' in filename:\n",
    "        get_removed_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB. In removal some Art_end markers are disappearing because the last sentence is an unlabeled sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
