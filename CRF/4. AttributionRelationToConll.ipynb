{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. AttributionRelationToConll\n",
    "\n",
    "This notebook links the different attribution labels, numbers them, formats the attribution labels to CONLL format and outputs the output data in a separate file per article, just like in the input data.\n",
    "\n",
    "#### Logic\n",
    "1. Loop over testfiles, loop over tokens, first look for CUE sequences and number them. \n",
    "2. Then find the closest SOURCE and CONTENTS sequences and give them the same number. For finding SOURCE and CONTENT do an additional loop traveling the dict backwards and forwards at the same time with loop_index and -loop_index.\n",
    "3. Then also number the continuous SOURCE and CONTENT labels for the label SOURCE or CONTENT label found.\n",
    "\n",
    "#### Possible additions\n",
    "Not just take the closest SOURCE and CONTENT, but also check if they are closer to another CUE.\n",
    "And at the end check and number the SOURCE and CONTENT labels without CUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bettyvandongen/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "start_index_labels = 2  # 2 if we use BIO labels, 0 if we don't use BIO labels\n",
    "\n",
    "preprocessed_data_files = [\n",
    "#     \"Result/CRF_out_BIO_features2-dev-removed_merged-merged.csv\",\n",
    "#     \"Result/CRF_out_BIO_embedding50-dev-removed_merged-merged.csv\",\n",
    "#     \"Result/CRF_out_BIO_embedding50-dev_merged-merged.csv\",\n",
    "#     \"Result/CRF_out_BIO_baseline-dev_merged-merged.csv\",\n",
    "    \"Result/CRF_out_BIO_baseline-dev_polnear-polnear.csv\",\n",
    "#     \"Result/CRF_out_BIO_features-dev_polnear-polnear.csv\",\n",
    "#     \"Result/CRF_out_BIO_features2-dev_polnear-polnear.csv\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilesGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_files = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda f: [(a, s, nf, ns, ft, w, l, p, d, td, al) \n",
    "                              for a, s, nf, ns, ft, w, l, p, d, td, al \n",
    "                              in zip(f[\"Article_Name\"].values.tolist(),\n",
    "                                     f[\"Sentence_nr\"].values.tolist(),\n",
    "                                     f[\"Nr_in_file\"].values.tolist(),\n",
    "                                     f[\"Nr_in_sentence\"].values.tolist(),\n",
    "                                     f[\"FromTo\"].values.tolist(),\n",
    "                                     f[\"Word\"].values.tolist(),\n",
    "                                     f[\"Lemma\"].values.tolist(),\n",
    "                                     f[\"POS\"].values.tolist(),\n",
    "                                     f[\"Dep_label\"].values.tolist(),\n",
    "                                     f[\"Token_dep_head\"].values.tolist(),\n",
    "                                     f[\"AR_label\"].values.tolist()\n",
    "                                    )]\n",
    "        self.grouped = self.dataset.groupby([\"Article_Name\"]).apply(agg_func)\n",
    "        self.files = [f for f in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            f = self.grouped[\"Files: {}\".format(self.n_tokens)]\n",
    "            self.n_files += 1\n",
    "            return f\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testlist = [1, 2, 3, 4, 5, 6]\n",
    "# start_index = 2\n",
    "# print(testlist[start_index])\n",
    "# testlist.reverse()\n",
    "# start_index = len(testlist) - start_index - 1\n",
    "# print(start_index)\n",
    "# print(testlist[start_index])\n",
    "\n",
    "# for file_index, file in enumerate(file_temp[start_index:]):\n",
    "#     print(file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_continuous_labels(file_temp, start_idx, label_nr, label_type, reverse=False):\n",
    "    if reverse:\n",
    "        file_temp.reverse()\n",
    "        start_idx = (len(file_temp) - 1) - start_idx\n",
    "        \n",
    "    for file_index, file_token in enumerate(file_temp):\n",
    "        if file_index > start_idx:\n",
    "            label = file_token[10]\n",
    "            if label[start_index_labels:] == label_type:\n",
    "                file_token_temp = (file_token[0], file_token[1], file_token[2], file_token[3], file_token[4], file_token[5], \n",
    "                                   file_token[6], file_token[7], file_token[8], file_token[9], label + '-' + str(label_nr))\n",
    "                file_temp[file_index] = file_token_temp\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Reverse back once we're done\n",
    "    if reverse:\n",
    "        file_temp.reverse()\n",
    "        \n",
    "    return file_temp\n",
    "\n",
    "\n",
    "# Find the CONTENT or SOURCE label for a CUE and add the number to the label\n",
    "def number_label(token_idx, file_temp, label_nr, label_type):\n",
    "    label_search_index = 1\n",
    "    label_found_index = None\n",
    "    \n",
    "    file_temp_len = len(file_temp)\n",
    "    backwards_search_index = token_idx - label_search_index\n",
    "    forwards_search_index = token_idx + label_search_index\n",
    "\n",
    "    while backwards_search_index >= 0 or forwards_search_index < file_temp_len:\n",
    "#         print(label_search_index)\n",
    "\n",
    "        # Backwards loop\n",
    "        if backwards_search_index >= 0:\n",
    "            file_temp_item_backwards = file_temp[backwards_search_index]\n",
    "            label_backwards = file_temp_item_backwards[10]\n",
    "            if label_backwards[start_index_labels:] == label_type:\n",
    "                # Overwrite a possible content_found_index in the forward loop. \n",
    "                # We favor the backwards content_found_index over the forward loop.\n",
    "                # If we find a content, break immediately from while loop\n",
    "                label_found_index = backwards_search_index\n",
    "                \n",
    "                file_token_temp = (file_temp_item_backwards[0], file_temp_item_backwards[1], file_temp_item_backwards[2], \n",
    "                                   file_temp_item_backwards[3], file_temp_item_backwards[4], file_temp_item_backwards[5], \n",
    "                                   file_temp_item_backwards[6], file_temp_item_backwards[7], file_temp_item_backwards[8], \n",
    "                                   file_temp_item_backwards[9], label_backwards + '-' + str(label_nr))\n",
    "                file_temp[label_found_index] = file_token_temp\n",
    "\n",
    "                file_temp = number_continuous_labels(file_temp, label_found_index, label_nr, label_type, reverse=True)\n",
    "                break\n",
    "\n",
    "        # Forwards loop\n",
    "        if forwards_search_index < file_temp_len:\n",
    "            file_temp_item_forwards = file_temp[forwards_search_index]\n",
    "            label_forwards = file_temp_item_forwards[10]\n",
    "            if label_forwards[start_index_labels:] == label_type: \n",
    "                label_found_index = forwards_search_index\n",
    "    \n",
    "                file_token_temp = (file_temp_item_forwards[0], file_temp_item_forwards[1], file_temp_item_forwards[2], \n",
    "                                   file_temp_item_forwards[3], file_temp_item_forwards[4], file_temp_item_forwards[5], \n",
    "                                   file_temp_item_forwards[6], file_temp_item_forwards[7], file_temp_item_forwards[8], \n",
    "                                   file_temp_item_forwards[9], label_forwards + '-' + str(label_nr))\n",
    "                file_temp[label_found_index] = file_token_temp\n",
    "            \n",
    "                file_temp = number_continuous_labels(file_temp, label_found_index, label_nr, label_type)\n",
    "                break\n",
    "\n",
    "        label_search_index += 1 \n",
    "        backwards_search_index = token_idx - label_search_index\n",
    "        forwards_search_index = token_idx + label_search_index\n",
    "\n",
    "        # Uncomment for debugging\n",
    "#         if label_search_index == 1:  # One is first sentence with labels\n",
    "#             break \n",
    "\n",
    "    return file_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tokens(files):\n",
    "    files_labels_numbered = []\n",
    "    files_labels_max_label_nr = []\n",
    "    for file_index, file in enumerate(files):\n",
    "    #     file = file[0:30]  # Uncomment for debugging\n",
    "        # Copy of the tokens in the file\n",
    "        file_temp = file.copy()\n",
    "        label_nr = 1\n",
    "        max_label_nr = 0\n",
    "\n",
    "        # Loop over tokens to number labels\n",
    "        cue_found_index = -1\n",
    "        for token_idx, file_token in enumerate(file):\n",
    "            file_token_temp = file_token\n",
    "            label = file_token[10] \n",
    "\n",
    "            # If the label is CUE without a number, number the CUE first\n",
    "            is_search_content_source = False\n",
    "            if label[start_index_labels:] == 'CUE':\n",
    "\n",
    "                # We are only handling continuous CUEs. If CUE is not continuous, then the label nr goes up \n",
    "                if token_idx > cue_found_index + 1:\n",
    "                    label_nr += 1\n",
    "                    is_search_content_source = True\n",
    "\n",
    "                file_token_temp = (file_token[0], file_token[1], file_token[2], file_token[3], file_token[4], file_token[5], \n",
    "                                   file_token[6], file_token[7], file_token[8], file_token[9], label + '-' + str(label_nr))\n",
    "                file_temp[token_idx] = file_token_temp\n",
    "\n",
    "                cue_found_index = token_idx\n",
    "\n",
    "                if is_search_content_source:\n",
    "                    # Now search backwards and forwards in file for CONTENT and SOURCE labels\n",
    "                    file_temp = number_label(token_idx, file_temp, label_nr, 'CONTENT')\n",
    "                    file_temp = number_label(token_idx, file_temp, label_nr, 'SOURCE')\n",
    "\n",
    "                max_label_nr = label_nr\n",
    "\n",
    "        # Append highest label nr found to list\n",
    "        files_labels_numbered.append(file_temp)\n",
    "        files_labels_max_label_nr.append(max_label_nr)\n",
    "\n",
    "    #     if file_index == 0:\n",
    "    #         break\n",
    "    \n",
    "    return files_labels_numbered, files_labels_max_label_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_labels_numbered[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_labels(files):\n",
    "    files_labels_numbered_padded = []\n",
    "    for file_idx, file in enumerate(files_labels_numbered):\n",
    "        max_label_nr = files_labels_max_label_nr[file_idx]\n",
    "        file_temp = file.copy()\n",
    "        for token_idx, file_token in enumerate(file):\n",
    "            label = file_token[10]\n",
    "            if label != 'O':\n",
    "                label_nr_str = label.replace('B','').replace('I','').replace('-','').replace('SOURCE','').replace('CONTENT','').replace('CUE','')\n",
    "                if label_nr_str:\n",
    "                    try:\n",
    "                        label_nr = int(label_nr_str)\n",
    "                        lpad_string = \"_ \" * (max_label_nr - (max_label_nr - label_nr) - 1)\n",
    "                        rpad_string = \" _\" * (max_label_nr - label_nr)\n",
    "                        label = lpad_string + label + rpad_string\n",
    "                    except:\n",
    "                        # 8 CONTENT labels didn't get a number\n",
    "                        print('error')\n",
    "            else:\n",
    "                if max_label_nr == 0:\n",
    "                    label = 'O'\n",
    "                else:\n",
    "                    # For 'O' label, create string of all underscores for the nr of labels in the file.\n",
    "                    label = (\"_ \" * (max_label_nr - 1)) + '_'\n",
    "\n",
    "#             file_temp[token_idx] = (token, label, filename, sentence_idx)\n",
    "            file_token_temp = (file_token[0], file_token[1], file_token[2], file_token[3], file_token[4], file_token[5], \n",
    "                               file_token[6], file_token[7], file_token[8], file_token[9], label)\n",
    "            file_temp[token_idx] = file_token_temp                    \n",
    "        files_labels_numbered_padded.append(file_temp)\n",
    "\n",
    "    #     if file_idx == 1:\n",
    "    #         break\n",
    "    \n",
    "    return files_labels_numbered_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_files_and_sentences(files): \n",
    "    files_list = []\n",
    "    for file in files:\n",
    "        previous_sentence_nr = 1\n",
    "        token_list = []\n",
    "        for token_idx, file_token in enumerate(file):\n",
    "            article_name = file_token[0]\n",
    "            sentence_nr = file_token[1]\n",
    "            nr_in_file = file_token[2]\n",
    "            nr_in_sentence = file_token[3]\n",
    "            token_dep_head = file_token[9]\n",
    "            if sentence_nr != previous_sentence_nr or article_name == 'Art_end':\n",
    "                # Add empty line between sentences and at the end of the file.\n",
    "                token_list.append({\"Article_Name\": '', \n",
    "                                   \"Sentence_nr\": '', \n",
    "                                   \"Nr_in_file\": '', \n",
    "                                   \"Nr_in_sentence\": '', \n",
    "                                   \"FromTo\": '', \n",
    "                                   \"Word\": '', \n",
    "                                   \"Lemma\": '', \n",
    "                                   \"POS\": '',\n",
    "                                   \"Dep_label\": '', \n",
    "                                   \"Token_dep_head\": '', \n",
    "                                   \"AR_label\": ''})\n",
    "            \n",
    "            if not np.isnan(sentence_nr):\n",
    "                sentence_nr = int(sentence_nr)\n",
    "            if not np.isnan(nr_in_file):\n",
    "                nr_in_file = int(nr_in_file)\n",
    "            if not np.isnan(nr_in_sentence):\n",
    "                nr_in_sentence = int(nr_in_sentence)\n",
    "            if not np.isnan(token_dep_head):\n",
    "                token_dep_head = int(token_dep_head)\n",
    "\n",
    "            token_list.append({\"Article_Name\": file_token[0], \n",
    "                               \"Sentence_nr\": sentence_nr, \n",
    "                               \"Nr_in_file\": nr_in_file, \n",
    "                               \"Nr_in_sentence\": nr_in_sentence, \n",
    "                               \"FromTo\": file_token[4], \n",
    "                               \"Word\": file_token[5], \n",
    "                               \"Lemma\": file_token[6], \n",
    "                               \"POS\": file_token[7],\n",
    "                               \"Dep_label\": file_token[8], \n",
    "                               \"Token_dep_head\": token_dep_head, \n",
    "                               \"AR_label\": file_token[10]})\n",
    "            previous_sentence_nr = sentence_nr\n",
    "        files_list.append(token_list)\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    if x:\n",
    "        x = str(x).split('.')[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_name):\n",
    "    try:\n",
    "        os.makedirs(dir_name)    \n",
    "#         print(\"Directory \" , dir_name ,  \" Created \")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "#         print(\"Directory \" , dir_name ,  \" already exists\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excel', 'excel-tab', 'unix']\n"
     ]
    }
   ],
   "source": [
    "print(csv.list_dialects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Article_Name  Sentence_nr  Nr_in_file  \\\n",
      "0  west-journal_2016-09-29_gold-star-mom-corners-...          1.0         1.0   \n",
      "1  west-journal_2016-09-29_gold-star-mom-corners-...          1.0         2.0   \n",
      "\n",
      "   Nr_in_sentence FromTo  Word Lemma  POS Dep_label  Token_dep_head   AR_label  \n",
      "0             1.0    0,4  Gold  Gold  NNP  compound             5.0  I-CONTENT  \n",
      "1             2.0    5,9  Star  Star  NNP  compound             5.0  I-CONTENT  \n",
      "85\n",
      "85\n",
      "85\n",
      "85\n",
      "{'Article_Name': 'Art_end', 'Sentence_nr': nan, 'Nr_in_file': nan, 'Nr_in_sentence': nan, 'FromTo': nan, 'Word': 'Sent_end', 'Lemma': nan, 'POS': nan, 'Dep_label': nan, 'Token_dep_head': nan, 'AR_label': 'O'}\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-12_stealth-over-health-hillary-clin.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-15_pat-caddell-democrat-voters-worr.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-20_poll-wealthy-voters-abandon-repu.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-21_only-percent-of-hillary-clinton-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-25_pence-gennifer-flowers-won-t-be-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-26_green-party-candidate-jill-stein.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-28_mary-j-blige-sings-to-hillary-cl.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-29_politifact-donald-trump-is-right.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-09-30_the-latest-trump-urges-supporter.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-10-02_washington-post-hit-piece-exploi.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-10-03_the-latest-trump-s-leaked-taxes-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/breitbart_2016-10-04_bill-clinton-calls-obamacare-cra.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-10_trump-s-east-european-achilles-h.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-16_black-lawmakers-rip-donald-trump.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-20_elizabeth-warren-hammers-wells-f.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-20_hillary-clinton-speaks-directly-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-22_trump-the-dealmaker-could-addres.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-26_one-tweet-sums-up-the-absurd-dou.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-27_hillary-clinton-goes-after-donal.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-27_latinos-share-their-most-honest-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-27_obama-power-plant-rules-face-key.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-09-28_clinton-supporting-super-pacs-ta.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-10-07_ava-duvernay-s-th-explores-the-e.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/huff-post_2016-10-07_frustrated-parents-rally-nationw.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-08_obama-on-climate-change-the-tren.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-12_visa-program-up-for-renewal-amid.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-16_clinton-plan-for-taxes-is-expans.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-19_obama-sees-personal-insult-if-bl.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-19_old-hands-help-russia-navigate-u.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-20_examining-the-u-n-s-record-on-ur.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-24_small-steps-toward-college-savin.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-25_obama-says-african-american-muse.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-28_what-mexico-could-do-when-up-aga.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-09-30_keep-the-returns-trump-just-give.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-10-02_a-curious-plan-to-fight-climate-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/nyt_2016-10-04_in-avoiding-tax-trump-is-merely-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-08_flynn-says-trump-absolutely-righ.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-12_former-dnc-chairman-calls-for-cl.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-15_trump-s-favorite-tv-doctor-has-a.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-16_sanders-clinton-needs-to-get-awa.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-17_de-blasio-says-chelsea-explosion.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-22_house-panel-votes-to-hold-clinto.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-22_judge-rejects-new-york-times-req.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-23_partisan-fireworks-over-clinton-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-25_poll-clinton-and-trump-in-dead-h.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-28_axelrod-calls-on-dean-to-apologi.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-09-30_flight-attendants-union-endorses.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/politico_2016-10-04_trump-not-walking-the-walk-on-hi.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-08_candidates-vie-for-vets-support-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-08_lawmaker-urges-retaliation-on-ru.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-08_laws-don-t-apply-equially-to-cli.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-08_with-two-months-to-go-trump-stil.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-16_every-minutes-a-veteran-commits-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-16_hold-melania-trump-to-her-husban.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-22_derelict-obama-and-clinton-trump.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-22_super-pac-haul-surges-past-b-sma.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-23_ready-for-trump-vs-clinton-his-p.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-23_trump-questions-the-fairness-of-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-09-29_can-you-imagine-if-trump-were-a-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/usa-today_2016-10-03_my-father-would-have-loved-trump.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-08_virginian-could-make-history-aga.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-10_outrage-but-few-options-on-the-t.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-14_a-lesson-from-loretta-lynch-be-t.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-16_burma-leader-s-visit-to-washingt.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-18_e-u-leaders-weigh-plans-for-grea.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-21_mixed-messages-on-trump-leave-pl.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-25_federal-court-rules-against-ohio.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-25_phone-video-is-first-public-look.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-26_ah-friends-once-more-ted-cruz-en.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-26_charlotte-struggles-to-return-to.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-09-29_the-real-trump-showed-up-at-deba.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/wash-post_2016-10-05_international-monetary-fund-slas.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-12_obama-seeks-more-muslim-refugees.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-14_clinton-hid-past-health-problems.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-15_retired-general-reveals-demorali.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-21_chris-christie-calls-hillary-cli.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-23_clinton-breaks-with-obama-on-law.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-26_popular-website-shows-bias-when-.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-26_rush-limbaugh-hillary-asking-mod.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-28_clinton-campaign-reportedly-scra.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-29_gold-star-mom-corners-obama-on-h.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-09-30_new-poll-shows-minor-post-debate.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-10-04_railing-against-trump-sanders-vo.xml.conll.features.foreval.out\n",
      "CONLL/CRF_out_BIO_baseline-dev_polnear-polnear/west-journal_2016-10-05_pence-gives-gop-a-boost-with-sol.xml.conll.features.foreval.out\n"
     ]
    }
   ],
   "source": [
    "for preprocessed_data_file in preprocessed_data_files:\n",
    "#     dataset = pd.read_csv(preprocessed_data_file, sep=\"\\t\", encoding=\"latin-1\")\n",
    "    dataset = pd.read_csv(preprocessed_data_file, sep=\"|\", encoding=\"utf-8\")\n",
    "    dataset['AR_label'] = dataset['AR_label'].apply(str)\n",
    "    print(dataset.head(2))\n",
    "    files_getter = FilesGetter(dataset)\n",
    "    files = files_getter.files\n",
    "    print(len(files))\n",
    "#     print(files[1])\n",
    "    \n",
    "    # Add numbers to labels\n",
    "    files_labels_numbered, files_labels_max_label_nr = label_tokens(files)\n",
    "    print(len(files_labels_numbered))\n",
    "#     print(files_labels_numbered[1])\n",
    "    \n",
    "    # Added _'s to labels\n",
    "    files_labels_numbered_padded = pad_labels(files)\n",
    "    print(len(files_labels_numbered_padded))\n",
    "#     print(files_labels_numbered_padded[1])\n",
    "\n",
    "    # Add empty lines between sentences\n",
    "    files_list = separate_files_and_sentences(files_labels_numbered_padded)\n",
    "    print(len(files_list))\n",
    "    print(files_list[0][1])\n",
    "    \n",
    "    # Write output to file\n",
    "    for file in files_list:\n",
    "#         print(file[0])\n",
    "        filename = file[0].get('Article_Name') \n",
    "#         print(filename)\n",
    "        dir_name = f\"CONLL/{preprocessed_data_file.replace('Result/','').replace('.csv','')}/\"\n",
    "        create_directory(dir_name)\n",
    "        if filename:\n",
    "            print(dir_name + filename.replace('.txt.xml', '.xml').replace('.xml', '.xml.conll.features.foreval.out'))\n",
    "            with open(dir_name + filename.replace('.txt.xml', '.xml').replace('.xml', '.xml.conll.features.foreval.out'), \"w\", encoding=\"utf-8\") as result:\n",
    "                writer = csv.writer(result, delimiter=\"\\t\", dialect='excel-tab',)\n",
    "                for row in file:\n",
    "#                     print(row)\n",
    "                    if row.get(\"Sentence_nr\") == '':\n",
    "                        # Not using df.to_csv to be able to write empty row between the sentences.\n",
    "                        writer.writerow(())\n",
    "#                         writer.writerow(('', '', '', '', '', '', '', '', '', '', '',))\n",
    "                    else:\n",
    "                        writer.writerow((row.get(\"Article_Name\"), \n",
    "                                         row.get(\"Sentence_nr\"), \n",
    "                                         row.get(\"Nr_in_file\"), \n",
    "                                         row.get(\"Nr_in_sentence\"), \n",
    "                                         row.get(\"FromTo\"), \n",
    "                                         row.get(\"Word\"), \n",
    "                                         row.get(\"Lemma\"), \n",
    "                                         row.get(\"POS\"), \n",
    "                                         row.get(\"Dep_label\"), \n",
    "                                         row.get(\"Token_dep_head\"), \n",
    "                                         row.get(\"AR_label\"),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files[0:1]\n",
    "# [[('Gold', 'I-CONTENT', 1.0, 1.0, '0,4'), ('Star', 'I-CONTENT', 1.0, 2.0, '5,9'), ('mom', 'I-CONTENT', 1.0, 3.0, '10,13'), \n",
    "# ('Corners', 'I-CONTENT', 1.0, 4.0, '14,21'), ('Obama', 'I-CONTENT', 1.0, 5.0, '22,27'), ('on', 'I-CONTENT', 1.0, 6.0, '28,30'), \n",
    "# ('he', 'I-CONTENT', 1.0, 7.0, '31,34'), ('refusal', 'I-CONTENT', 1.0, 8.0, '35,42'), ('to', 'I-CONTENT', 1.0, 9.0, '43,45'),\n",
    "# ('use', 'I-CONTENT', 1.0, 10.0, '46,49'), ('the', 'I-CONTENT', 1.0, 11.0, '50,53'), ('word', 'I-CONTENT', 1.0, 12.0, '54,59'), \n",
    "# ('`', 'I-CONTENT', 1.0, 13.0, '60,61'), etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_py37_venv",
   "language": "python",
   "name": "jupyter_py37_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
